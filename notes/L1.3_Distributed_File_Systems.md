# Lecture 1.3: Introduction to Distributed File Systems

## DS256 - Scalable Systems for Data Science
### Scale-out Data Storage using GFS/HDFS

---

## 1. Why Do We Need a File System?

A file system serves four fundamental purposes:

### 1.1 Persistence of Data (Reliability)
- Data survives beyond the lifetime of the process that created it
- Data remains intact even after system restarts or power failures
- Ensures long-term storage of information

### 1.2 Logical Organization and Access
- Provides a hierarchical structure (directories and files)
- Human-readable naming conventions
- Easy navigation and management of data

### 1.3 Process Access (Performance)
- Allows programs to read from and write to storage
- Provides efficient I/O operations
- Manages buffering and caching for better performance

### 1.4 Sharing Across Users (Access Control)
- Multiple users can access the same data
- Permission systems (read, write, execute)
- Concurrent access management

---

## 2. What is a File System?

A file system is an **abstraction layer** that hides the complexity of underlying storage media from users and applications.

### Key Abstractions:
- **File**: A logical unit of information with a name
- **Directory**: A container that organizes files hierarchically

### File Attributes:
- **Name**: Human-readable identifier
- **Type**: Extension or format (e.g., .txt, .pdf)
- **Size**: Amount of data stored
- **Access Control**: Permissions (rwx for owner/group/others)
- **Time**: Creation, modification, access timestamps

### Logical Operations:
- **Create**: Make a new file
- **Write**: Add or modify data
- **Read**: Retrieve data
- **Delete**: Remove the file

The beauty of a file system is that it presents the **same interface** regardless of whether the underlying storage is an HDD, SSD, tape drive, or network storage.

---

## 3. File System Modules

A file system is composed of several layered modules, each handling a specific responsibility:

```
┌─────────────────────────────┐
│     Directory Module        │  ← Maps file names to file IDs
├─────────────────────────────┤
│    Access Control Module    │  ← Checks permissions (read/write/execute)
├─────────────────────────────┤
│       Files Module          │  ← Manages file metadata (size, timestamps)
├─────────────────────────────┤
│   Block Allocation Module   │  ← Decides which disk blocks to use
├─────────────────────────────┤
│     Disk Access Module      │  ← Performs actual I/O with hardware
├─────────────────────────────┤
│       Disk Blocks           │  ← Physical storage on the disk
└─────────────────────────────┘
```

### 3.1 Directory Module
- Translates human-readable file names to internal file identifiers
- Maintains the hierarchical structure (parent-child relationships)
- Example: `/home/user/document.txt` → File ID 12345

### 3.2 Access Control Module
- Verifies if the requesting user/process has permission
- Enforces read, write, and execute permissions
- Prevents unauthorized access

### 3.3 Files Module
- Manages file metadata (not the actual data)
- Tracks file size, creation time, modification time
- Maintains pointers to where file data is stored

### 3.4 Block Allocation Module
- Decides which physical blocks on disk will store file data
- Handles allocation when files grow, deallocation when deleted
- Manages free space tracking

### 3.5 Disk Access Module
- Interfaces directly with the storage hardware
- Issues read/write commands to the disk controller
- Handles buffering and scheduling of I/O operations

---

## 4. Unix iNodes

An **iNode** (Index Node) is a data structure in Unix-based file systems that stores metadata about a file and maps the logical file to physical disk blocks.

### What an iNode Contains:
- File type (regular file, directory, symbolic link)
- Permissions (rwx for owner, group, others)
- Owner UID and Group GID
- File size in bytes
- Timestamps (access, modification, change)
- Link count (number of hard links)
- **Pointers to data blocks**

### How iNodes Map to Disk Blocks:

```
┌──────────────────────────────────────────┐
│              iNode Structure             │
├──────────────────────────────────────────┤
│  Metadata (permissions, size, times)     │
├──────────────────────────────────────────┤
│  Direct Block Pointers (12 pointers)     │──→ [Block 1][Block 2]...[Block 12]
├──────────────────────────────────────────┤
│  Single Indirect Pointer                 │──→ [Pointer Block] → [Data Blocks]
├──────────────────────────────────────────┤
│  Double Indirect Pointer                 │──→ [Ptr Block] → [Ptr Block] → [Data]
├──────────────────────────────────────────┤
│  Triple Indirect Pointer                 │──→ [Ptr] → [Ptr] → [Ptr] → [Data]
└──────────────────────────────────────────┘
```

### Why This Multi-Level Structure?
- **Small files** (most common): Use only direct pointers → Fast access
- **Medium files**: Use single indirect → Still reasonably fast
- **Large files**: Use double/triple indirect → Can store huge files

### Key Insight:
The iNode is the bridge between the **logical file system** (what users see) and the **physical storage** (actual disk blocks). When you access `/home/user/file.txt`:
1. Directory lookup gives you the iNode number
2. iNode gives you the block addresses
3. Disk access module reads those blocks

---

## 5. What is a Distributed File System (DFS)?

A **Distributed File System** is a file system where the storage location and the clients accessing the data can be on **different machines** connected over a network.

### Types of Distributed File Systems:

#### 5.1 Client-Server File System
- **Architecture**: Many clients, one server holding all data
- **Example**: NFS (Network File System)
- **Use case**: Small organizations, shared network drives

```
[Client 1] ──┐
[Client 2] ──┼──→ [Single Server with Data]
[Client 3] ──┘
```

#### 5.2 Cluster File System
- **Architecture**: Many clients, many servers in a single cluster
- **Example**: GFS (Google File System), HDFS (Hadoop Distributed File System)
- **Use case**: Big data processing, large-scale storage

```
[Client 1] ──┐      ┌─→ [Server 1]
[Client 2] ──┼──────┼─→ [Server 2]
[Client 3] ──┘      └─→ [Server N]
```

#### 5.3 Peer-to-Peer File System
- **Architecture**: Every node is both client and server
- **Example**: Chord, BitTorrent
- **Use case**: Decentralized storage, file sharing

```
[Node 1] ←──→ [Node 2]
   ↑↓           ↑↓
[Node 3] ←──→ [Node 4]
```

---

## 6. Why Do We Need a Distributed File System?

### 6.1 Remote Access
- Access data from anywhere on the network
- No need to physically be at the storage location

### 6.2 Performance
- **Bandwidth**: Aggregate bandwidth from multiple servers
- **Wide-area locality**: Place data closer to where it's needed
- **Parallel I/O**: Read from multiple servers simultaneously

### 6.3 Reliability
- Replication across multiple machines
- No single point of failure (for data)
- Automatic recovery from failures

### 6.4 Capacity
- Scale storage by adding more machines
- Petabytes of storage possible
- Beyond what any single machine can hold

### 6.5 Access Restrictions
- Physical restrictions (data center access)
- Network-level access control
- Geographical distribution

---

## 7. Amdahl's Laws (Rules of Thumb for System Design)

Gene Amdahl formulated several rules of thumb for designing balanced computer systems. These help us understand the relationships between different system components.

### 7.1 Amdahl's Law of Parallelism
(You already know this from L1.2)

If a computation has a serial part `s` and a parallel part `(1-s)`:
- **Maximum Speedup** = 1/s
- The serial portion limits how much parallelism helps

### 7.2 Amdahl's Balanced System Law

> **"A system needs 1 bit of disk I/O per second for every instruction per second"**

This means:
- If your CPU can execute 1 billion instructions/second (1 GIPS)
- You need 1 Gbit/s = 125 MB/s of disk I/O bandwidth
- Otherwise, the CPU will be waiting for data

### 7.3 Amdahl's Memory Law

> **"The RAM capacity (in MB) should equal the CPU speed (in MIPS)"**

The ratio α (alpha) = RAM MB / CPU MIPS should be approximately 1.

- If CPU = 1000 MIPS, you need ~1000 MB = 1 GB RAM
- Too little RAM → Constant disk swapping
- This ratio has been increasing to 4+ as RAM gets cheaper

### 7.4 Amdahl's I/O Law

> **"Programs do 1 I/O operation per 50,000 instructions"**

This helps estimate I/O requirements:
- 1 billion instructions → 20,000 I/O operations
- Helps size I/O subsystems appropriately

---

## 8. System Component Relationships: A Complete Example

Let's understand how different system metrics relate to each other with a concrete example.

### The Question: 
If you have certain performance requirements, how do you size your system?

### Key Metrics and Their Relationships:

| Component | Metric | Typical Modern Value |
|-----------|--------|---------------------|
| CPU | Instructions/second | 32 × 10⁹ (16 cores × 2 GHz) |
| Disk Controller | I/O bandwidth | 22.5 Gbit/s (SAS-4) |
| RAM | Capacity | 32 GB |
| Single HDD | Transfer rate | ~100-150 MB/s |
| Single SSD | Transfer rate | ~500 MB/s |

### Example: Designing a Balanced System

**Given Requirements:**
- Need to process 1 TB of data
- Must complete in 1 hour
- Want a balanced system

**Step 1: Calculate Required I/O Bandwidth**
```
Data size = 1 TB = 1,000,000 MB
Time = 1 hour = 3,600 seconds
Required bandwidth = 1,000,000 / 3,600 ≈ 278 MB/s
```

**Step 2: How Many Disks for Bandwidth?**
```
If using HDDs (100 MB/s each):
    Disks needed = 278 / 100 ≈ 3 HDDs

If using SSDs (500 MB/s each):
    Disks needed = 278 / 500 ≈ 1 SSD
```

**Step 3: How Many Disks for Capacity?**
```
If each disk is 1 TB:
    Disks needed = 1 TB / 1 TB = 1 disk

If each disk is 500 GB:
    Disks needed = 1 TB / 500 GB = 2 disks
```

**Step 4: Choose the Larger Number**
```
For bandwidth: 3 HDDs
For capacity: 1 HDD
→ Need at least 3 HDDs (bandwidth is the bottleneck)
```

**Step 5: Match CPU and RAM (Amdahl's Laws)**
```
If disk I/O = 278 MB/s ≈ 2.2 Gbit/s
CPU should handle ≈ 2.2 billion instructions/sec
RAM should be ≈ 2-8 GB (with α = 1 to 4)
```

### The Key Insight:

**You're only as fast as your slowest component!**

```
         ┌─────────────┐
         │    CPU      │ ← Can process 32 billion ops/sec
         └──────┬──────┘
                │
         ┌──────▼──────┐
         │    RAM      │ ← Can supply 25 GB/s
         └──────┬──────┘
                │
         ┌──────▼──────┐
         │ Disk I/O    │ ← Can only do 278 MB/s  ← BOTTLENECK!
         └──────┬──────┘
                │
         ┌──────▼──────┐
         │   Network   │ ← Depends on configuration
         └─────────────┘
```

In Big Data systems, **disk I/O is almost always the bottleneck**, which is why:
- We use many disks in parallel (HDFS/GFS approach)
- We try to move computation to data (not data to computation)
- We use replication which also helps read parallelism

---

## 9. DFS Characteristics

When designing or evaluating a Distributed File System, consider these key characteristics:

### 9.1 Transparency
- **Access Transparency**: Same API regardless of where data is stored
- **Location Transparency**: Don't need to know physical location
- **Performance Transparency**: Consistent performance expectations
- **Scaling Transparency**: System can grow without API changes

### 9.2 Concurrency & Consistency
- Multiple clients can access same file simultaneously
- Need to define what happens with concurrent writes
- Trade-off between strong consistency and performance

### 9.3 Replication
- Multiple copies of data on different machines
- Improves fault tolerance
- Can improve read performance (read from any replica)
- Challenge: Keeping replicas consistent

### 9.4 Fault Tolerance
- System continues operating despite failures
- Automatic detection of failed components
- Automatic recovery and data repair
- No single point of failure (ideally)

### 9.5 Efficiency
- High throughput for large transfers
- Low latency for small operations
- Efficient use of network bandwidth
- Minimal overhead from distribution

### 9.6 Heterogeneity
- Work with different hardware (CPUs, disks, networks)
- Support different operating systems
- Handle varying network speeds

### 9.7 Security
- Authentication (who is accessing?)
- Authorization (what are they allowed to do?)
- Encryption (protect data in transit and at rest)
- Audit logging

---

## 10. Storage Systems Comparison

Different storage systems make different trade-offs. Here's a comprehensive comparison:

| Storage System | Sharing | Persistence | Distributed Cache/Replicas | Consistency | Example |
|----------------|---------|-------------|---------------------------|-------------|---------|
| **Main Memory** | Within process | No (volatile) | No | N/A (single location) | RAM, CPU cache |
| **File System** | Within machine | Yes | No | Strong (single machine) | ext4, NTFS |
| **Distributed File System** | Across network | Yes | Yes (replicas) | Varies (often relaxed) | GFS, HDFS, NFS |
| **Web** | Global (read-mostly) | Yes | Yes (CDN caches) | Weak (eventual) | HTTP/HTTPS |
| **Distributed Shared Memory** | Across cluster | No | Yes (cached copies) | Varies | Treadmarks |
| **Remote Objects** | Across network | Varies | Optional | Strong or eventual | Java RMI, CORBA |
| **Persistent Object Store** | Across network | Yes | Optional | Strong | Object databases |
| **Peer-to-Peer** | Across internet | Yes | Yes (distributed) | Eventual | BitTorrent, IPFS |

### Detailed Breakdown:

#### Main Memory
- **Sharing**: Only within a single process
- **Persistence**: Lost when power off
- **Use case**: Running programs, temporary data

#### Traditional File System
- **Sharing**: Multiple processes on same machine
- **Persistence**: Survives restarts
- **Use case**: Local storage on laptops, servers

#### Distributed File System
- **Sharing**: Any machine on the network
- **Persistence**: Yes, with replication for reliability
- **Replicas**: Multiple copies for fault tolerance
- **Use case**: Big data storage, shared enterprise storage

#### Web
- **Sharing**: Global, primarily read access
- **Persistence**: Yes (at origin servers)
- **Caching**: Extensive (browsers, CDNs, proxies)
- **Consistency**: Weak - you might see stale data
- **Use case**: Websites, APIs

#### Distributed Shared Memory (DSM)
- **Sharing**: Makes distributed memory look like shared memory
- **Persistence**: No - it's still RAM
- **Consistency**: Must be carefully managed
- **Use case**: Parallel computing applications

#### Peer-to-Peer
- **Sharing**: No central authority
- **Persistence**: As long as some peer has the data
- **Replicas**: Data spread across many peers
- **Consistency**: Eventual (no central coordinator)
- **Use case**: File sharing, blockchain

---

## 11. Big Data Sizes in the Era of Deep Learning

Modern machine learning success is largely due to **massive datasets**. Neural networks aren't new (50+ years old), but we only recently have enough data and compute to train them effectively.

### Historical Milestones:

#### Netflix Prize (2006)
- **Task**: Movie recommendation system
- **Data**: 100 million ratings
- **Scale**: 17,000 movies, 500,000 users
- **Impact**: Sparked interest in recommendation systems

#### ImageNet (2009)
- **Task**: Image classification
- **Data**: 14 million images
- **Scale**: 20,000+ categories
- **Impact**: Enabled deep learning breakthrough (AlexNet, 2012)

#### IBM Watson Beats Jeopardy (2011)
- **Task**: Question answering
- **Compute**: 5,500 independent experiments
- **Scale**: 2,000 CPU hours each, generating 10 GB error-analysis data
- **Architecture**: Massively parallel processing

#### Google Brain Recognizes Cats (2012)
- **Task**: Unsupervised feature learning
- **Data**: 10 million 200×200 pixel images
- **Scale**: 20,000 classes
- **Compute**: 1,000 machines with 16,000 cores
- **Impact**: Showed deep learning can discover concepts without labels

#### Facebook DeepFace (2014)
- **Task**: Face recognition
- **Data**: 4.4 million labeled faces
- **Scale**: 4,000 people, 1,000 samples each
- **Result**: Near human-level face verification

#### GPT-3 (2020)
- **Task**: Text generation
- **Data**: 
  - 400 billion tokens from Common Crawl
  - 19 billion tokens from WebText2
  - 67 billion tokens from books
  - 3 billion tokens from Wikipedia
- **Model**: 175 billion parameters
- **Impact**: Showed emergent abilities in large language models

#### ChatGPT (2022)
- **Base**: GPT-3 with 175 billion parameters
- **Fine-tuning**: Reinforcement Learning from Human Feedback (RLHF)
- **Human Data**: 33,000 human prompts for instruction tuning
- **Impact**: Made AI accessible to general public

### The Trend:
```
Dataset Size Over Time (Log Scale)

10 PB   │                                    ●  LLMs (2023)
        │                               ●  GPT-3
1 PB    │
        │
100 TB  │                          ●  Google Brain
        │
10 TB   │                     ●  ImageNet
        │
1 TB    │                ●  Netflix
        │           
100 GB  │      ●  Earlier ML
        │
        └────────────────────────────────────────────→
              2005    2010    2015    2020    2025
```

---

## 12. Training the ChatGPT Model

### The Infrastructure Challenge

Training models like ChatGPT requires massive distributed systems. Here's how it works:

### Microsoft Azure "Singularity" - AI Cloud Infrastructure

ChatGPT was trained on Microsoft Azure's specialized AI infrastructure called **Singularity**.

#### Key Features:

##### 1. Incremental Memory Checkpointing
```
┌─────────────┐     ┌─────────────┐
│  Worker 1   │────▶│   Storage   │
│  (GPU/CPU)  │     │  Checkpoint │
└─────────────┘     └─────────────┘
       │
       ▼
   Training continues while checkpoint saves
```

- Periodically saves the state of all workers (CPU and GPU memory)
- Used for **recovery** if a machine fails
- Used for **elastic resizing** (adding/removing machines)

##### 2. De-duplication for Efficiency
- Uses checksums to identify duplicate memory content
- Train and loader processes across workers often have similar data
- A worker's memory over time has similar content
- De-duplication reduces checkpoint size significantly

##### 3. Transparent Scaling
> "To scale up or scale down a job, we simply change the number of devices the workers are mapped to. This is completely transparent to the user..."

This means:
- Can add more GPUs when available
- Can reduce GPUs if higher-priority job needs them
- Training continues without restarting

##### 4. Storage Bottleneck
Even with all optimizations:
- **50% of checkpoint latency** goes to Azure Blob Storage
- Storage I/O is still the bottleneck, even for AI workloads

### The Investment Scale

> "Microsoft invested $1 billion in OpenAI in 2019, and in return OpenAI has built its AI models on Microsoft's Azure AI supercomputing technologies..."

This investment paid for:
- Thousands of high-end GPUs
- Specialized networking (high bandwidth, low latency)
- Massive storage infrastructure
- Custom software stack for distributed training

### Why This Matters for This Course

The ChatGPT training example shows why we need:
1. **Distributed File Systems** - Store petabytes of training data
2. **Fault Tolerance** - Training runs for weeks; machines will fail
3. **High Bandwidth** - Move data fast enough to keep GPUs busy
4. **Scalability** - Use 1000s of machines efficiently

---

## Summary

### Key Takeaways from Lecture 1.3:

1. **File systems abstract storage** - Same interface regardless of underlying media

2. **DFS extends this across networks** - Enabling scale, reliability, and remote access

3. **Amdahl's Laws guide system design** - Balance CPU, memory, and I/O

4. **The bottleneck is usually I/O** - This is why we parallelize storage

5. **Different storage systems for different needs** - Trade-offs between consistency, performance, and availability

6. **Big Data drives AI progress** - Modern ML needs massive datasets that only DFS can handle

7. **Training large models requires distributed systems** - Everything we learn in this course applies to real AI infrastructure

---

## References

1. Coulouris, Dollimore, Kindberg and Blair, "Distributed Systems: Concepts and Design", 5th Edition, Chapter 12
2. Silberschatz, Galvin, Gagne, "Operating System Concepts", 9th Edition, Chapter 12
3. Jim Gray, Prashant Shenoy, "Rules of Thumb in Data Engineering", ICDE 2000
4. Bell, Gray and Szalay, "Petascale Computational Systems", IEEE Computer, 2006
5. OpenAI, "Language Models are Few-Shot Learners" (GPT-3 paper)
6. Microsoft Research, "Singularity: Planet-Scale, Preemptive and Elastic Scheduling of AI Workloads"
