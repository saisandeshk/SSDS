# Lecture 1.4: The Google File System (GFS)

## DS256 — Scalable Systems for Data Science
### Module 1: Introduction to Big Data & Distributed Storage

---

## 1. Motivation: Why Was GFS Built?

### 1.1 The Google Search Problem (Circa 2000)

Google's core mission was to **crawl, index, and search the entire World Wide Web**. This required a pipeline of massive data operations:

```
┌──────────────┐     ┌──────────────────┐     ┌──────────────────┐
│   Web Crawl  │────▶│  Pre-processing  │────▶│  Search & Rank   │
│  (millions   │     │  (Index, Graph,  │     │  (Query-time     │
│   of pages)  │     │   PageRank)      │     │   lookups)       │
└──────────────┘     └──────────────────┘     └──────────────────┘
```

#### Pre-processing Steps:
1. **Web Crawl**: Crawlers fetch millions of web pages (HTML text) from the internet
2. **Build Inverted Index**: Map each keyword to the set of URLs (web pages) containing it
3. **Build Web Graph**: Extract hyperlinks between pages to form a directed graph
4. **Compute PageRank**: Run the PageRank algorithm over the web graph to score and rank pages by importance

#### At Search Time:
- Look up the query keywords in the inverted index to find matching URLs
- Use PageRank scores to rank the results
- Return the top results to the user

### 1.2 Inverted Index — How Search Works

An inverted index reverses the mapping from "document → words" to "word → documents":

**Step 1: URL to HTML Text Mapping (Forward Index)**
```
URL     Content
u1      "We the People of India, having solemnly…"
u2      "It was the best of times, it was the…"
u3      "Call me Ishmael. Some years ago…"
u4      "Here's my number, call me maybe…"
u5      "People call me the best…"
u6      "Number of people in India is…"
u7      "Best years of my life…"
```

**Step 2: Parse, Tokenize, Remove Stop Words**

Each document is broken into tokens (words). Common stop words (the, of, it, was, is, in, etc.) and contractions are removed. The remaining keywords form the document's keyword set.

**Step 3: Invert the Index**
```
Keyword     URLs containing it
──────────  ──────────────────
People      u1, u5, u6
India       u1, u6
Best        u2, u5, u7
Call        u3, u4, u5
Ishmael     u3
Some        u3
Years       u3, u7
Here        u4
Number      u4, u6
Life        u7
```

Now a search query like `"People India"` can quickly find `u1` and `u6` by intersecting the posting lists.

### 1.3 Web Graph and PageRank

**Web Graph Construction:**
- Extract all hyperlinks from each crawled page
- Build a directed graph: each page is a node, each hyperlink is a directed edge

**PageRank Algorithm:**
- Models a "random surfer" who randomly clicks links
- Pages that are linked to by many important pages get a higher rank
- Iteratively computed over the entire web graph until convergence

```
URL     PageRank
u1      0.02
u2      0.30
u3      0.08
u4      0.10
u5      0.20
u6      0.25
u7      0.05
```

### 1.4 Modern Relevance: LLM Training Data

The same web crawl infrastructure is now used for training Large Language Models:
- **Common Crawl** provides the raw text (WET files) from billions of web pages
- **LLaMA** (Meta) and **Falcon** (TII) both use Common Crawl as a primary training data source
- The pipeline involves extensive filtering, deduplication, and quality scoring before the text is used for training

---

## 2. Design Goals of GFS

GFS was designed with very specific workload assumptions that differ from traditional file systems:

### 2.1 Failure as the Norm
- The system is built from **hundreds or thousands of inexpensive commodity machines**
- Component failures (disks, memory, network, power) are **routine, not exceptional**
- The system must **constantly monitor itself**, detect failures, tolerate them, and **recover automatically**

### 2.2 Large Files, Not Small Files
- A **modest number of large files**: a few million files, each typically **100 MB or larger**
- Multi-GB files are the **common case**
- Small files are supported but **not optimized for**

### 2.3 Read-Heavy, Append-Mostly Workload
- **Large streaming reads**: each operation reads ≥1 MB; clients read contiguous regions of a file sequentially
- **Small random reads**: a few KB at arbitrary offsets; performance-conscious applications batch and sort these to advance sequentially
- **Writes are mostly large, sequential appends**: files are written once (append-only) and then read many times
- **Random writes** are supported but need not be efficient
- Files are **seldom modified after creation**

### 2.4 Concurrent Appends by Many Producers
- Hundreds of producer clients may **concurrently append** to the same file (e.g., a log file, a merged-results file)
- **Atomicity with minimal synchronization** is essential
- A consumer may be **reading the file while writers are still appending**

### 2.5 Bandwidth over Latency
- **High sustained throughput** (bulk data processing) is more important than **low latency** (interactive response)
- Applications are batch-oriented, not interactive

---

## 3. GFS Interface (API)

GFS provides a **familiar file system interface** but does **not implement the full POSIX standard**.

### 3.1 Standard Operations
Files are organized **hierarchically in directories** and identified by **pathnames**.

| Operation | Description |
|-----------|-------------|
| `create`  | Create a new file |
| `delete`  | Remove a file |
| `open`    | Open a file for reading/writing |
| `close`   | Close an open file |
| `read`    | Read data from a file |
| `write`   | Write data to a file at a specified offset |

### 3.2 New Operations Unique to GFS

| Operation | Description |
|-----------|-------------|
| **Snapshot** | Creates a copy of a file or directory tree almost instantaneously (copy-on-write) |
| **Record Append** | Appends a record atomically at least once, even with concurrent writers. GFS chooses the offset. |

**Why not full POSIX?**
- GFS is accessed by Google's own applications, not arbitrary Unix programs
- Relaxing the interface allows simpler design and better performance
- No need to hook into the Linux vnode layer

---

## 4. GFS Architecture

### 4.1 Components Overview

A GFS cluster consists of three types of entities:

```
┌─────────────────────────────────────────────────────────────────────┐
│                          GFS Cluster                                │
│                                                                     │
│  ┌────────────────┐                                                 │
│  │   Master       │  (Single)  — Stores all metadata                │
│  │  (NameNode)    │  — Namespace, file→chunk mapping                │
│  │                │  — Chunk→chunkserver mapping                    │
│  │                │  — Lease management, GC, rebalancing            │
│  └───────┬────────┘                                                 │
│          │ Heartbeats, Instructions                                 │
│          │                                                          │
│  ┌───────▼────────┐  ┌────────────────┐       ┌────────────────┐   │
│  │ ChunkServer 1  │  │ ChunkServer 2  │  ...  │ ChunkServer N  │   │
│  │  (DataNode)    │  │  (DataNode)    │       │  (DataNode)    │   │
│  │ ┌────┐ ┌────┐  │  │ ┌────┐ ┌────┐ │       │ ┌────┐ ┌────┐  │   │
│  │ │Blk1│ │Blk3│  │  │ │Blk1│ │Blk2│ │       │ │Blk2│ │Blk3│  │   │
│  │ └────┘ └────┘  │  │ └────┘ └────┘ │       │ └────┘ └────┘  │   │
│  │  Linux FS      │  │  Linux FS     │       │  Linux FS      │   │
│  └────────────────┘  └───────────────┘       └────────────────┘   │
│                                                                     │
│  ┌────────────────┐                                                 │
│  │  GFS Clients   │  — Application-linked library                   │
│  │                │  — Metadata ops → Master                        │
│  │                │  — Data ops → ChunkServers directly              │
│  └────────────────┘                                                 │
└─────────────────────────────────────────────────────────────────────┘
```

### 4.2 The Single Master

**Design choice:** A single master **vastly simplifies the design** and enables globally optimal decisions for chunk placement, replication, and load balancing.

**Critical design constraint:** The master must **never become a bottleneck**.

How this is achieved:
- Clients **never read or write file data through the master**
- Clients only contact the master for **metadata operations** (which chunkservers to talk to)
- Clients **cache** the metadata (chunk locations) and interact **directly with chunkservers** for data

#### Read Flow (Step by Step):

```
Client                          Master                    ChunkServer
  │                               │                            │
  │ 1. (filename, chunk_index)    │                            │
  │──────────────────────────────▶│                            │
  │                               │                            │
  │ 2. (chunk_handle, locations)  │                            │
  │◀──────────────────────────────│                            │
  │                               │                            │
  │        [Client caches this info]                           │
  │                               │                            │
  │ 3. (chunk_handle, byte_range) │                            │
  │────────────────────────────────────────────────────────────▶│
  │                               │                            │
  │ 4. chunk_data                 │                            │
  │◀───────────────────────────────────────────────────────────│
```

1. Client translates `(filename, byte_offset)` into `(filename, chunk_index)` using the fixed chunk size (64 MB). Sends request to master.
2. Master replies with the **chunk handle** (unique 64-bit ID) and the **locations** of all replicas. Client caches this using `(filename, chunk_index)` as the key.
3. Client sends a read request to the **closest** chunkserver replica, specifying `(chunk_handle, byte_range)`.
4. ChunkServer returns the requested data.

**Optimization:** Client typically requests **multiple chunks** in one master request; master may proactively return info for subsequent chunks.

### 4.3 ChunkServers (Data Nodes / Workers)

- Each chunkserver stores chunks as **ordinary Linux files** on its local disk
- Data is read/written by specifying `(chunk_handle, byte_offset)`
- ChunkServers are **unaware of GFS file semantics** — they just store opaque chunks
- Linux's buffer cache handles caching automatically — no additional caching layer needed

### 4.4 Clients

- GFS client code is a **library linked into applications**
- It implements the GFS API and handles communication with master and chunkservers
- **Clients do NOT cache file data** — why?
  - Most applications **stream through huge files** sequentially
  - Working sets are **too large to be cached** effectively
  - Eliminating client caches removes **cache coherence complexity**
- Clients **do cache metadata** (chunk locations) for a limited time

---

## 5. Chunk Size: 64 MB

### 5.1 Why So Large?

The chunk size of **64 MB** (128 MB in HDFS v3) is **vastly larger** than typical filesystem block sizes (~4 KB). This is a deliberate design choice.

Each chunk replica is stored as a **plain Linux file** on the chunkserver and is extended only as needed (**lazy space allocation** — no wasted space from internal fragmentation).

### 5.2 Advantages of Large Chunk Size

| Advantage | Explanation |
|-----------|-------------|
| **Fewer client-master interactions** | Reads/writes on the same chunk only need one initial metadata request. Critical for large sequential reads. |
| **Reduced network overhead** | Client can maintain a single **persistent TCP connection** to a chunkserver for many operations on the same chunk. |
| **Smaller metadata on Master** | Fewer chunks means fewer entries in the master's in-memory metadata tables. Allows all metadata to fit in RAM. |

### 5.3 Disadvantage: Hot Spots

- A small file consists of perhaps **only one chunk**
- If many clients access the same small file, the chunkserver(s) storing that chunk become **hot spots**
- **In practice**, this was rare because applications mostly read large multi-chunk files sequentially

**Real-world hot spot incident:** When GFS was first used by a batch-queue system, an executable (single chunk) was started on hundreds of machines simultaneously, overloading the chunkservers. **Fix:** Store such files with a **higher replication factor** and stagger start times.

---

## 6. Master Metadata

The master stores three types of metadata, **all kept in memory** for fast access:

### 6.1 Three Types of Metadata

| Metadata | Persistent? | How Maintained |
|----------|-------------|----------------|
| **File & chunk namespaces** | Yes (operation log) | Mutations logged to disk |
| **File → chunk mapping** | Yes (operation log) | Mutations logged to disk |
| **Chunk → chunkserver mapping** | **No** | Built on-demand at startup from chunkserver reports |

### 6.2 Why In-Memory?

- Master operations become **extremely fast**
- Enables efficient **background scanning** for:
  - **Garbage collection** of orphaned chunks
  - **Re-replication** when replicas are lost
  - **Chunk migration** for load balancing
- **Memory usage is compact**: ~64 bytes per chunk, ~64 bytes per file (filenames stored with prefix compression)
- For a cluster with millions of files, this is only tens of MB — easily fits in RAM

### 6.3 Why Is Chunk Location NOT Persisted?

The master does **not** store chunk-to-chunkserver mapping persistently. Instead, it asks each chunkserver at startup what chunks it has (via **Block Reports**).

**Rationale:**
- In a cluster of hundreds of servers, **machines constantly join, leave, fail, restart, change names**
- Keeping persistent mappings in sync would be complex and error-prone
- The **chunkserver is the final authority** on what chunks it actually has on disk — data may "spontaneously vanish" (e.g., disk failure)
- Simpler to just **poll at startup** and maintain via heartbeats thereafter

### 6.4 The Operation Log

The operation log is the **most critical piece of GFS**:

- It is the **only persistent record** of metadata
- It serves as a **logical timeline** defining the order of all operations
- Files, chunks, and their versions are identified by the logical times at which they were created

**Persistence and Replication:**
- The operation log is **replicated on multiple remote master machines**
- A metadata change is **not visible to clients** until the log record has been **flushed to disk both locally and on all replicas**
- Log records are **batched** before flushing to reduce I/O overhead

**Recovery via Checkpoints:**
- The master periodically **checkpoints** its entire in-memory state to disk
- Checkpoint format: a **compact B-tree** that can be directly **memory-mapped** without parsing
- Recovery process: load the latest checkpoint + replay only the log records after it
- Checkpointing is done in a **separate thread** without blocking incoming mutations
- Takes about **1 minute** for a cluster with a few million files

```
┌──────────┐     ┌──────────────────┐     ┌──────────────────┐
│ Ops Log  │────▶│   Checkpoint     │────▶│   Recovery       │
│ (append) │     │ (periodic B-tree │     │ (load checkpoint │
│          │     │  snapshot)       │     │  + replay log)   │
└──────────┘     └──────────────────┘     └──────────────────┘
```

---

## 7. Chunk Replication

### 7.1 Default Replication Factor: 3

Every chunk is stored on **3 different chunkservers** by default.

**Why 3 replicas?**
- **Fault tolerance**: Survive the simultaneous failure of 2 chunkservers (or 2 disks)
- **Read performance**: Clients can read from the **closest** replica, distributing read load
- **Availability**: Even during replication/recovery, data remains accessible

### 7.2 Replica Placement Strategy

Replicas are spread across **both machines and racks**:

```
        Rack 1                 Rack 2                 Rack 3
┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐
│  ChunkServer A  │   │  ChunkServer C  │   │  ChunkServer E  │
│  [Chunk X - R1] │   │  [Chunk X - R2] │   │                 │
│                 │   │                 │   │                 │
│  ChunkServer B  │   │  ChunkServer D  │   │  ChunkServer F  │
│                 │   │                 │   │  [Chunk X - R3] │
└─────────────────┘   └─────────────────┘   └─────────────────┘
```

**Benefits of cross-rack placement:**
- Survives even if an **entire rack goes offline** (switch failure, power failure)
- **Read traffic** can exploit the aggregate bandwidth of multiple racks

**Trade-off:**
- **Write traffic** must flow across racks (higher latency), but this is acceptable because GFS prioritizes write throughput over write latency

---

## 8. Master's Housekeeping Responsibilities

### 8.1 Garbage Collection

GFS uses **lazy deletion** — a simpler and more reliable approach than immediate deletion:

1. When a file is deleted, the master **logs the deletion** but does not immediately reclaim storage
2. The file is **renamed to a hidden name** (with a deletion timestamp)
3. During the master's regular namespace scan, hidden files **older than 3 days** (configurable) are removed
4. Orphaned chunks (not referenced by any file) are identified and their metadata erased
5. ChunkServers learn about orphaned chunks via heartbeats and **delete their local replicas**

**Advantages of lazy garbage collection:**
- **Simple and reliable** in a large distributed system where failures are common
- **Merges with existing background scans** — amortized cost
- Provides a **safety net** against accidental deletion (files can be undeleted within 3 days)

### 8.2 Stale Replica Detection

- Each chunk has a **version number** maintained by the master
- When a lease is granted, the master **increments the version number** and informs all up-to-date replicas
- If a chunkserver was down during a mutation, its chunk version **will not advance** → it becomes **stale**
- On restart, the chunkserver reports its chunks with version numbers; master detects staleness
- Stale replicas are **never served to clients** and are garbage collected

### 8.3 Re-Replication

When the number of replicas falls below the target (e.g., a chunkserver dies):
- The master **re-replicates** the chunk by instructing a chunkserver to copy from an existing valid replica
- **Priority**: chunks with fewer replicas get higher priority (1 replica > 2 replicas)
- Chunks blocking client progress are boosted in priority
- **Throttled**: limits on concurrent cloning per chunkserver and per cluster to avoid overwhelming network

### 8.4 Rebalancing

- Master periodically examines replica distribution
- Moves replicas for better **disk space** and **load balancing**
- Gradually fills new chunkservers rather than swamping them instantly

---

## 9. Heartbeat Mechanism

The master communicates with chunkservers via **periodic HeartBeat messages**:

```
                    Master
                      │
          ┌───────────┼───────────┐
          │           │           │
          ▼           ▼           ▼
     ChunkServer  ChunkServer  ChunkServer
          1           2           N
```

### 9.1 What the Master Sends (Piggybacked Instructions):
- Replicate specific chunks to other chunkservers
- Remove local replicas that are orphaned or stale
- Shut down the node
- Send an immediate block report

### 9.2 What the ChunkServer Reports Back:
- Confirmation that it is alive
- Storage capacity and fraction in use
- Number of data transfers in progress
- List of chunks it holds (Block Report — full report sent hourly, first report sent immediately at startup)

### 9.3 Failure Detection:
- Default heartbeat interval: **3 seconds**
- If no heartbeat received for **10 minutes**, the master considers the chunkserver **dead** and schedules re-replication of its chunks

---

## 10. Fault Tolerance

### 10.1 High Availability

#### Fast Recovery
- Both master and chunkservers can **restore state and restart in seconds**
- No distinction between normal and abnormal shutdown — servers are routinely killed and restarted
- Clients experience only a brief hiccup while reconnecting

#### Chunk Replication
- Default **3 replicas** on different racks
- Master continuously monitors and clones to maintain the target replication level
- Checksums protect against silent data corruption

#### Master Replication
- Operation log and checkpoints are **replicated on multiple machines**
- If the master machine fails, monitoring infrastructure starts a **new master process** on another machine using the replicated state
- Clients access the master via a **DNS alias** (e.g., `gfs-test`) that can be redirected

#### Shadow Masters
- **Read-only** shadow masters provide file system access even when the primary is down
- They are **shadows, not mirrors** — may lag the primary by fractions of a second
- They replay the operation log and poll chunkservers independently
- Useful for applications that can tolerate slightly stale metadata

### 10.2 Data Integrity: Checksums

Each chunkserver independently verifies data integrity:

```
┌──────────────────────────────────────────────────────┐
│                    One Chunk (64 MB)                  │
├──────┬──────┬──────┬──────┬──────┬──────┬────────────┤
│ 64KB │ 64KB │ 64KB │ 64KB │ 64KB │ 64KB │    ...     │
│ blk  │ blk  │ blk  │ blk  │ blk  │ blk  │            │
├──────┼──────┼──────┼──────┼──────┼──────┼────────────┤
│ CRC  │ CRC  │ CRC  │ CRC  │ CRC  │ CRC  │    ...     │
│  32b │  32b │  32b │  32b │  32b │  32b │            │
└──────┴──────┴──────┴──────┴──────┴──────┴────────────┘
```

- Every **64 KB block** within a chunk has a **32-bit checksum**
- Checksums are stored **in memory** and **persisted with logging**, separate from user data
- **On read**: chunkserver verifies checksums of overlapping blocks **before** returning data. If mismatch → return error, report to master, master clones from another replica.
- **On append**: incrementally update checksum for the last partial block; compute new checksums for new full blocks
- **On overwrite**: must read and verify the first and last blocks being partially overwritten, then write, then recompute checksums
- **During idle periods**: chunkservers scan inactive chunks to detect latent corruption

---

## 11. Performance Results (From the GFS Paper)

### 11.1 Micro-Benchmarks

Test setup: 1 master, 2 master replicas, 16 chunkservers, 16 clients, 100 Mbps Ethernet, 1 Gbps inter-switch link.

| Metric | 1 Client | 16 Clients | Theoretical Limit |
|--------|----------|------------|-------------------|
| **Read** | 10 MB/s (80% of limit) | 94 MB/s (75% of 125 MB/s link limit) | 125 MB/s |
| **Write** | 6.3 MB/s (50% of limit) | 35 MB/s (52% of 67 MB/s limit) | 67 MB/s |
| **Record Append** | 6.0 MB/s | 4.8 MB/s | Limited by last-chunk chunkserver |

**Write is slower than read** because each write must go to 3 replicas (3x network cost).

### 11.2 Real-World Cluster Characteristics

| Metric | Cluster A (R&D) | Cluster B (Production) |
|--------|-----------------|----------------------|
| Chunkservers | 342 | 227 |
| Available disk | 72 TB | 180 TB |
| Used disk | 55 TB | 155 TB |
| Number of files | 735K | 737K |
| Number of chunks | 992K | 1,550K |
| Metadata at chunkservers | 13 GB | 21 GB |
| Metadata at master | 48 MB | 60 MB |
| Read rate (sustained) | 580 MB/s | 380 MB/s |
| Write rate (sustained) | 1-25 MB/s | 13-117 MB/s |

**Key observation:** Master metadata is only **48–60 MB** — confirming that in-memory metadata is practical. Chunkserver metadata (mostly checksums) is much larger at 13–21 GB.

### 11.3 Deployment Scale

By the time of the paper:
- **50+ GFS clusters** deployed at Google
- Each with **thousands of storage nodes**
- Managing **petabytes of data**
- GFS underpins higher-level systems like **BigTable**

---

## 12. Summary of Key Design Decisions

| Design Decision | Rationale |
|----------------|-----------|
| **Single master** | Simplifies design; global knowledge for placement. Bottleneck avoided by separating metadata from data path. |
| **Large chunks (64 MB)** | Reduces metadata, reduces master interactions, enables persistent TCP connections. |
| **No client data caching** | Files too large for caching; eliminates cache coherence complexity. |
| **In-memory metadata** | Fast operations; compact (64 bytes/chunk); enables efficient background scanning. |
| **Chunk locations not persisted** | Chunkservers are the source of truth; eliminates sync issues in a dynamic cluster. |
| **Operation log + checkpoints** | Reliable persistence with fast recovery; logical timeline for ordering. |
| **Lazy garbage collection** | Simpler, more reliable, provides undelete safety net. |
| **Relaxed consistency model** | Simpler design; applications handle relaxed semantics with checksums and dedup. |
| **Append-optimized** | Matches the write-once-read-many workload; atomic record append for concurrent producers. |
| **Cross-rack replication** | Survives rack failures; increases aggregate read bandwidth. |

---

## References

1. Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung, "The Google File System", SOSP 2003
2. DS256 Lecture 1.4 Slides — Yogesh Simmhan, IISc Bangalore
3. LLaMA: Open and Efficient Foundation Language Models, Meta AI
4. The RefinedWeb Dataset for Falcon LLM
