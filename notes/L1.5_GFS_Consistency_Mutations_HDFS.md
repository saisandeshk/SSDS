# Lecture 1.5: GFS Consistency, Mutations & HDFS

## DS256 — Scalable Systems for Data Science
### Module 1: Introduction to Big Data & Distributed Storage

---

## 1. Data Mutations in GFS

GFS supports three types of data mutations (operations that modify chunk contents):

### 1.1 Write

A **write** places data at a **client-specified file offset**.

```
write(fileId, offset, bytes[])
```

- The client knows exactly where in the file the data should go
- Overwrites any existing data at that offset
- Used when the client controls the file layout

### 1.2 Append

An **append** writes data at the client's perception of the end-of-file:

```
s = getFileSize(fileId)
write(fileId, s, bytes[])
```

- The client first queries the file size, then writes at that offset
- This is a **"regular" append** — essentially a write at the current EOF
- **Problem**: In a concurrent setting, multiple clients may compute the same EOF and overwrite each other's data — this is **not safe for concurrent use**

### 1.3 Record Append (GFS-Specific)

A **record append** causes a record to be **appended atomically at least once**, at an **offset chosen by GFS** (not the client):

```
offset = recordAppend(fileId, bytes[])
```

- GFS guarantees the record is written **as a contiguous, atomic unit**
- The system chooses the offset — the client gets back the offset where the record was placed
- **Critical for concurrent producers**: hundreds of clients can safely append to the same file without external synchronization
- Used for **producer-consumer queues** and **merged result files**
- Record append is restricted to be at most **1/4 of the chunk size** (16 MB) to limit worst-case fragmentation

---

## 2. GFS Consistency Model

GFS uses a **relaxed consistency model** — simpler to implement but requires applications to handle some complexity.

### 2.1 Region States

Consistency is defined for **regions** within a chunk — specific byte ranges that are written to by clients.

A region can be in one of three states:

| State | Definition |
|-------|-----------|
| **Defined** | All replicas have the **same content** for that region, AND the region reflects the **complete, unmingled update** from a single write operation. (Implies consistent.) |
| **Consistent** | All replicas have the **same byte contents** for that region, but the content may contain **mingled fragments** from multiple concurrent writers. |
| **Inconsistent** | Different replicas have **different byte contents** for that region. Different clients may see different data at different times. |

### 2.2 When Does Each State Occur?

| Scenario | Write Result | Record Append Result |
|----------|-------------|---------------------|
| **Serial success** (one writer, no concurrency) | **Defined** | **Defined** (interspersed with inconsistent padding/duplicates) |
| **Concurrent successes** (multiple writers, all succeed) | **Consistent but undefined** | **Defined** (interspersed with inconsistent) |
| **Any failure** | **Inconsistent** | **Inconsistent** |

### 2.3 Visual Example of Consistency States

```
                 Chunk Replica 1         Chunk Replica 2         Chunk Replica 3
                ┌────────────────┐      ┌────────────────┐      ┌────────────────┐
                │                │      │                │      │                │
  Single        │   AAAAAAAAAA   │      │   AAAAAAAAAA   │      │   AAAAAAAAAA   │
  Writer (C1)   │   (defined)    │      │   (defined)    │      │   (defined)    │
  succeeds      │                │      │                │      │                │
                ├────────────────┤      ├────────────────┤      ├────────────────┤
                │                │      │                │      │                │
  Concurrent    │   AABBCCAABB   │      │   AABBCCAABB   │      │   AABBCCAABB   │
  Writers       │   (consistent  │      │   (consistent  │      │   (consistent  │
  C1,C2,C3      │    but undef.) │      │    but undef.) │      │    but undef.) │
  all succeed   │                │      │                │      │                │
                ├────────────────┤      ├────────────────┤      ├────────────────┤
                │                │      │                │      │                │
  Failed        │   AABB????     │      │   AABBCCDD     │      │   AABB??CC     │
  mutation      │   (inconsist.) │      │   (inconsist.) │      │   (inconsist.) │
                │                │      │                │      │                │
                └────────────────┘      └────────────────┘      └────────────────┘
```

### 2.4 Key Insight: Consistent but Undefined

When multiple clients write concurrently and all succeed:
- All replicas end up with **identical content** (consistent) because all mutations are applied in the same serial order
- But the content is a **mixture of fragments** from different clients (undefined) — no single client's complete write is guaranteed to be intact
- **Data flow** from different clients may arrive in different orders and be buffered in memory
- **Control flow** (commit to disk) is serialized — FCFS (First Come, First Served)
- **No rollback** — once a partial write succeeds, it stays

---

## 3. Leases and Mutation Order

The core mechanism for maintaining **consistent mutation order across replicas** is the **lease system**.

### 3.1 How Leases Work

1. The master grants a **chunk lease** to one of the chunk's replicas → this replica becomes the **primary**
2. The primary **assigns serial numbers** to all mutations it receives
3. All replicas (primary + secondaries) apply mutations in **serial number order**
4. The **global mutation order** is defined by:
   - The **lease grant order** (which primary is chosen, in what order)
   - Within a lease, the **serial numbers** assigned by the primary

**Lease properties:**
- Initial timeout: **60 seconds**
- Primary can request **extensions** (piggybacked on heartbeats) indefinitely while the chunk is being mutated
- Master can **revoke** a lease early (e.g., before a rename/snapshot)
- If master loses contact with primary, it **waits for the lease to expire** before granting a new one (safety)

---

## 4. Write Data Flow — The Full Protocol

This is the most important protocol in GFS. It shows how data and control flow are **decoupled** for maximum efficiency.

### 4.1 The Seven Steps

```
                    Master
                      │
            ┌─────────┤
            │ Step 1   │ Step 2
            │ Request  │ Reply (primary ID,
            │ lease    │ secondary locations)
            ▼ info     │
          Client ◄─────┘
            │
            │ Step 3: Push data to ALL replicas (any order)
            │         Each chunkserver stores in LRU buffer
            │
            ├──────────────▶ Primary         (data)
            ├──────────────▶ Secondary A     (data)
            └──────────────▶ Secondary B     (data)
            
            │ Step 4: All replicas ACK data received
            │         Client sends WRITE REQUEST to Primary
            │
            ▼
          Primary
            │ Step 5: Assigns serial # to mutation
            │         Applies mutation locally
            │         Forwards write request to all secondaries
            │
            ├──────────────▶ Secondary A  (apply in serial # order)
            └──────────────▶ Secondary B  (apply in serial # order)
            
            │ Step 6: Secondaries ACK to Primary
            │
            ▼
          Primary
            │ Step 7: Primary ACKs to Client
            ▼
          Client (write complete)
```

### 4.2 Detailed Step-by-Step

| Step | Action | Details |
|------|--------|---------|
| **1** | Client → Master | Client asks which chunkserver holds the current lease (primary) and the locations of all replicas |
| **2** | Master → Client | Master replies with primary identity + secondary locations. Client **caches** this. Only re-contacts master if primary becomes unreachable. |
| **3** | Client → All Replicas | Client pushes data to **all replicas** (primary + secondaries), in **any order**. Each chunkserver stores the data in an internal **LRU buffer cache** (not yet written to disk). |
| **4** | Client → Primary | Once **all** replicas ACK receiving the data, client sends a **write request** to the primary (identifying the data pushed in step 3). |
| **5** | Primary → Secondaries | Primary **assigns a serial number** to the mutation. Applies it **locally** first. Then forwards the write request (with serial #) to all secondaries. |
| **6** | Secondaries → Primary | Each secondary applies the mutation in **serial number order** and ACKs to primary. |
| **7** | Primary → Client | Primary ACKs to client after receiving ACKs from **all secondaries**. Errors at any replica are reported. |

### 4.3 Failure Handling

| Failure Scenario | What Happens |
|-----------------|-------------|
| **Primary fails before writing** | No writes were performed. Client retries from the beginning. |
| **Primary succeeds, some secondaries fail** | Write is reported as **failed** to the client. The affected region is left **inconsistent**. Client retries (steps 3–7), potentially multiple times before falling back to a full retry. |
| **Large write straddles chunk boundary** | GFS client code breaks it into **multiple write operations**, each following the above flow. May be interleaved with concurrent operations from other clients. |

### 4.4 Why Decouple Data Flow from Control Flow?

This is a key architectural insight:

```
Control Flow:  Client ──▶ Primary ──▶ Secondaries
               (Ensures serial order, consistency)

Data Flow:     Client ──▶ Nearest chunkserver ──▶ Next nearest ──▶ ...
               (Pipelined, topology-aware, maximizes bandwidth)
```

**Control flow** (which writes happen in which order) flows from client → primary → secondaries. This ensures a single serial order.

**Data flow** (the actual bytes) is pushed through a **pipeline** along a chain of chunkservers, optimized for network topology:

- Each machine sends data to the **nearest machine** that hasn't received it yet
- "Distance" is estimated from **IP addresses** (same rack < different rack < different datacenter)
- **Pipelined**: once a chunkserver receives some data, it **starts forwarding immediately** (doesn't wait for the full transfer)
- Fully utilizes each machine's **outbound bandwidth** without splitting it among multiple recipients

**Performance:** Transferring B bytes to R replicas takes approximately:
```
Time ≈ B/T + R × L
```
Where `T` = network throughput, `L` = latency between two machines. With 100 Mbps links and <1 ms latency, **1 MB can be distributed in ~80 ms**.

---

## 5. Atomic Record Append — Detailed Protocol

### 5.1 How It Works

1. Client pushes data to all replicas of the **last chunk** of the file
2. Client sends append request to the **primary**
3. Primary checks: **will this record fit in the current chunk?**
   - **If NO** (would exceed 64 MB): Primary **pads** the chunk to the end of its capacity, tells secondaries to do the same, tells client to **retry on the next chunk**
   - **If YES**: Primary appends the record to its replica at a chosen offset, tells secondaries to write at the **exact same offset**, ACKs to client with the offset

### 5.2 Failure and Retry Behavior

- If the append **fails at any replica**, the client **retries**
- This means replicas may contain **duplicate records** (same record written multiple times)
- GFS guarantees: the record is written **at least once** as an atomic unit at the **same offset on all replicas of some chunk**, if the operation reports success
- Regions with successful appends are **defined** (consistent)
- Intervening regions (padding, failed partial appends) are **inconsistent**

### 5.3 The "At Least Once" Semantics

```
Chunk on Replica 1          Chunk on Replica 2          Chunk on Replica 3
┌──────────────────┐        ┌──────────────────┐        ┌──────────────────┐
│ Record A  ✓      │        │ Record A  ✓      │        │ Record A  ✓      │
│ Record B  ✓      │        │ Record B  ✓      │        │ Record B  ✓      │
│ Record C (failed)│        │ Record C  ✓      │        │ Record C (failed)│
│ Padding          │        │ Padding          │        │ Padding          │
│ Record C  ✓      │ ◄──    │ Record C  ✓      │ ◄──    │ Record C  ✓      │ ◄── Retry
│ Record D  ✓      │        │ Record D  ✓      │        │ Record D  ✓      │
└──────────────────┘        └──────────────────┘        └──────────────────┘
                                   ▲
                                   │
                              All replicas have Record C at the SAME offset (the retry)
                              But some have a partial/failed copy earlier too
```

---

## 6. Snapshot

Snapshot creates a **copy of a file or directory tree** almost **instantaneously** using **copy-on-write (CoW)**.

### 6.1 Snapshot Process

1. Master **revokes** all outstanding leases on chunks of the file (or waits for them to expire)
   - This ensures any subsequent writes must contact the master first
2. Master **logs** the snapshot operation to the operation log
3. Master **duplicates the metadata** of the source file → creates a new snapshot file pointing to the **same chunks**
4. Master **increments the reference counter** on each shared chunk

### 6.2 Copy-on-Write Behavior

When a client later tries to **write to a shared chunk** (reference count > 1):

```
Before Write:
  Source File ────┐
                  ├──▶ Chunk C (refcount = 2)
  Snapshot File ──┘

After Write:
  Source File ─────────▶ Chunk C' (new copy, refcount = 1) ← writes go here
  Snapshot File ───────▶ Chunk C  (original, refcount = 1) ← unchanged
```

1. Client sends write request to master
2. Master notices `refcount > 1` for chunk C
3. Master picks a **new chunk handle C'**
4. Master asks chunkservers holding C to **locally copy** C to C' (local copy = fast, no network transfer)
5. Master grants a lease on C' and replies to client
6. Client writes to C' normally

**Key benefit:** Snapshots are **O(metadata)** — only metadata is duplicated, not data. Data is only copied when actually modified.

---

## 7. Implications for Applications

The relaxed consistency model places some burden on applications. GFS applications use these techniques:

### 7.1 Append Rather Than Overwrite

- Virtually all GFS applications **append** to files rather than overwriting
- Appending is more efficient and more resilient to application failures

### 7.2 Checkpointing

**Pattern 1: Single writer**
- A writer generates a file from **beginning to end**
- After writing all data, it **atomically renames** the file to a permanent name, OR
- It **periodically checkpoints** how much has been successfully written (with application-level checksums)
- Readers **only process data up to the last checkpoint** (which is in the defined state)
- This lets writers **restart incrementally** on failure and keeps readers from processing incomplete data

**Pattern 2: Multiple writers (producer-consumer)**
- Multiple producers concurrently **record-append** to the same file
- Record append preserves each writer's output
- Readers handle padding and duplicates:
  - Each record contains a **checksum** so readers can verify validity and detect fragments
  - Each record contains a **UUID** so readers can identify and filter duplicates
  - These checks are implemented in a **shared library** used by all GFS applications

---

## 8. Master: Namespace Management and Locking

### 8.1 Namespace Representation

- GFS does **not** have a per-directory data structure listing all files
- Instead, the namespace is a **lookup table** mapping **full pathnames to metadata**
- Stored compactly using **prefix compression**
- No hard links or symbolic links

### 8.2 Locking Mechanism

Each node in the namespace tree has an associated **read-write lock**.

For an operation on `/d1/d2/.../dn/leaf`:
- Acquire **read locks** on: `/d1`, `/d1/d2`, ..., `/d1/d2/.../dn`
- Acquire a **read lock** or **write lock** on: `/d1/d2/.../dn/leaf`

**Example: Preventing conflicts between snapshot and file creation**

| Operation | Locks Acquired |
|-----------|---------------|
| Snapshot `/home/user` → `/save/user` | Read: `/home`, `/save` — Write: `/home/user`, `/save/user` |
| Create `/home/user/foo` | Read: `/home`, `/home/user` — Write: `/home/user/foo` |

These serialize correctly because they **conflict on `/home/user`** (write lock vs read lock).

**Key property:** Multiple file creations in the **same directory** can proceed **concurrently**:
- Each acquires a **read lock** on the directory (prevents deletion/rename)
- Each acquires a **write lock** on its own filename (serializes attempts to create the same filename)

**Deadlock prevention:** Locks are acquired in a **consistent total order** — by level in the namespace tree, then lexicographically within the same level.

---

## 9. Master: Replica Placement Strategy

### 9.1 Goals
- **Maximize data reliability and availability**
- **Maximize network bandwidth utilization**

### 9.2 Placement Rules

| Rule | Details |
|------|---------|
| Spread across **machines** | Guards against individual disk/machine failure |
| Spread across **racks** | Guards against rack-level failures (switch, power) |
| No DataNode has more than **one replica** of a block | Ensures machine failure loses at most one replica |
| No rack has more than **two replicas** of a block | Ensures rack failure loses at most two replicas |

### 9.3 Placement for New Chunks
The master considers:
1. Place on chunkservers with **below-average disk utilization** (equalize over time)
2. **Limit recent creations** per chunkserver (avoids imminent write traffic overload)
3. **Spread across racks**

### 9.4 Read Optimization
When returning chunk locations to a reader, the master returns replicas **sorted by closeness** to the reader (same rack > same datacenter > remote).

### 9.5 Distance Metric

```
Distance = sum of distances to common ancestor

Same machine:       distance = 0
Same rack:          distance = 2  (node→rack→node)
Different rack:     distance = 4  (node→rack→switch→rack→node)
Different datacenter: distance = 6
```

---

## 10. Hadoop Distributed File System (HDFS)

HDFS is the **open-source implementation** inspired by GFS, forming the storage layer of the Apache Hadoop ecosystem.

### 10.1 GFS vs HDFS Terminology

| GFS Term | HDFS Term |
|----------|-----------|
| Master | **NameNode** |
| ChunkServer | **DataNode** |
| Chunk | **Block** |
| Chunk Size: 64 MB | Block Size: **128 MB** (HDFS v3) |
| Operation Log | **Journal / EditLog** |
| Checkpoint | **FSImage** |
| Shadow Master | **Checkpoint Node / Backup Node** |

### 10.2 Architecture (Same as GFS)

```
┌─────────────────────────────────────────────────────────┐
│                     HDFS Cluster                        │
│                                                         │
│  ┌───────────────┐     ┌───────────────────────────┐    │
│  │   NameNode    │     │  Namespace (FSImage)       │    │
│  │               │────▶│  File → Block mapping      │    │
│  │               │     │  Block → DataNode mapping   │    │
│  │               │     │  EditLog (Journal)          │    │
│  └───────┬───────┘     └───────────────────────────┘    │
│          │                                               │
│          │ Heartbeats (every 3 sec)                      │
│          │ Block Reports (every 1 hour)                  │
│          │                                               │
│  ┌───────▼───────┐  ┌───────────────┐  ┌─────────────┐  │
│  │  DataNode 1   │  │  DataNode 2   │  │  DataNode N  │  │
│  │  ┌────┐┌────┐ │  │  ┌────┐┌────┐ │  │  ┌────┐     │  │
│  │  │Blk1││Blk3│ │  │  │Blk1││Blk2│ │  │  │Blk2│     │  │
│  │  └────┘└────┘ │  │  └────┘└────┘ │  │  └────┘     │  │
│  │  Linux FS     │  │  Linux FS     │  │  Linux FS   │  │
│  └───────────────┘  └───────────────┘  └─────────────┘  │
└─────────────────────────────────────────────────────────┘
```

---

## 11. HDFS Block Reports

### 11.1 Purpose
A DataNode identifies all block replicas in its possession by sending a **block report** to the NameNode.

### 11.2 Contents
Each block report entry contains:
- **Block ID**: unique identifier
- **Generation stamp**: version number (analogous to GFS chunk version number)
- **Block length**: how many bytes are stored

### 11.3 Timing
- **First block report**: sent **immediately** after DataNode registration (at startup)
- **Subsequent reports**: sent **every hour**
- NameNode can request an **immediate block report** via heartbeat response

---

## 12. HDFS Heartbeat Mechanism

### 12.1 DataNode → NameNode Heartbeat

| Property | Value |
|----------|-------|
| Default interval | **3 seconds** |
| Dead threshold | **No heartbeat for 10 minutes** → considered out of service |
| Action on death | NameNode schedules **re-replication** of all blocks on dead DataNode |

### 12.2 Heartbeat Payload (DataNode → NameNode)
- Storage capacity (total disk)
- Fraction of storage in use
- Number of data transfers currently in progress

### 12.3 Heartbeat Response (NameNode → DataNode)
- **Replicate** specific blocks to other DataNodes
- **Remove** local replicas (orphaned/stale)
- **Shut down** the node
- **Send immediate block report**

---

## 13. HDFS Checkpoint and Backup Nodes

### 13.1 The Problem with Journal Replay

- Recreating NameNode state from a week's worth of journal (edit log) entries can take **hours**
- The journal grows continuously as operations are performed
- Need a mechanism to periodically compact the state

### 13.2 Checkpoint Node

```
                    NameNode
                      │
          ┌───────────┤
          │ Download   │ Upload
          │ FSImage +  │ new FSImage
          │ Journal    │
          ▼            │
     Checkpoint Node ──┘
     (merges locally)
```

1. Downloads the current **FSImage (checkpoint)** and **Journal (edit log)** from the NameNode
2. **Merges** them locally: applies all journal entries to the FSImage
3. Uploads the **new FSImage** back to the NameNode
4. NameNode can now **truncate the journal** (discard entries already merged into the checkpoint)

### 13.3 Backup Node

The Backup Node is a **more capable** version of the Checkpoint Node:

| Feature | Checkpoint Node | Backup Node |
|---------|----------------|-------------|
| Creates periodic checkpoints | ✓ | ✓ |
| Maintains in-memory namespace | ✗ | ✓ (synced with NameNode) |
| Receives live journal stream | ✗ | ✓ (applies in real-time) |
| Can serve read-only queries | ✗ | ✓ (**Read-only NameNode**) |
| Checkpoint creation | Requires download | Done **locally** (faster) |

The Backup Node accepts the journal stream of namespace transactions from the active NameNode, saves them to its local store, and applies them to its own namespace image in memory. This is analogous to GFS's shadow master.

---

## 14. HDFS Block Creation Pipeline

### 14.1 How Data is Written

When a client writes a block, DataNodes form a **pipeline**:

```
Client ──▶ DataNode 1 ──▶ DataNode 2 ──▶ DataNode 3
           (closest)                      (furthest)
```

The pipeline order **minimizes total network distance** from the client to the last DataNode.

### 14.2 Write Protocol Details

| Property | Details |
|----------|---------|
| Packet size | Data pushed as **64 KB packet buffers** |
| ACK handling | **Asynchronous** — client doesn't wait for each packet to be ACKed before sending the next |
| Outstanding ACKs | Maximum number of unacknowledged packets in flight |
| Checksums | Client **generates checksums** for each block; DataNode **stores checksums** alongside the block |
| Read verification | Client verifies checksums during reads to **detect corruption** |

---

## 15. HDFS Block Placement Policy

### 15.1 Replica Placement Rules

| Replica | Placement |
|---------|-----------|
| **1st replica** | On the **writer's node** (same machine as the client, if the client is on a DataNode) |
| **2nd replica** | On a **different node** in a **different rack** |
| **3rd replica** | On another **different node** in the **same rack** as the 2nd replica |

**Constraints:**
- No DataNode has more than **one replica** of a block
- No rack has more than **two replicas** of a block

### 15.2 Trade-off
- Writing: 2nd replica requires **cross-rack transfer** (higher latency)
- Reading: NameNode returns replicas **sorted by closeness** to the reader → fast reads from nearby replicas
- Reliability: Survives the failure of any single rack

---

## 16. HDFS Replication Management

### 16.1 Detecting Under/Over-Replication

The NameNode detects replication anomalies from **block reports**:

**Under-replication:**
- Placed in a **priority queue** (block with only 1 replica has **highest priority**)
- A background thread scans the queue and decides where to place new replicas
- Factors: disk space utilization, rack diversity, bandwidth limits

**Over-replication:**
- Remove replicas without reducing the number of **racks** hosting the block
- Prefer removing from the DataNode with the **least available disk space**

### 16.2 Real-World Recovery Example (from GFS paper)

| Scenario | Recovery Time | Details |
|----------|---------------|---------|
| **1 chunkserver killed** (15,000 chunks, 600 GB) | **23.2 minutes** | 91 concurrent clones, 6.25 MB/s per clone, effective rate = 440 MB/s |
| **2 chunkservers killed** (32,000 chunks, 1.32 TB) | **2 minutes** for critical chunks | 266 chunks reduced to 1 replica → cloned at highest priority to 2x replication |

---

## 17. HDFS Balancer

The Balancer is a tool that **equalizes disk space utilization** across DataNodes in the cluster.

### 17.1 Goal
The **utilization** of any node (percentage of disk used) should differ from the **cluster average utilization** by no more than a configurable **threshold**.

### 17.2 How It Works
- **Iteratively moves replicas** from nodes with higher utilization to nodes with lower utilization
- Maintains **data availability** during moves (ensures replication factor is never violated)
- **Minimizes inter-rack copying** (prefers intra-rack moves when possible)
- **Limits bandwidth consumed** to avoid impacting application performance

---

## 18. Disk Failures in the Real World

### 18.1 The Reality of MTTF

Disk manufacturers quote Mean Time To Failure (MTTF) of **1,000,000 hours** (~114 years). But this is misleading:

- With **1000 disks** in a cluster, expected failure rate = 1,000,000 / 1,000 = **1,000 hours** = **~42 days per failure**
- With **10,000 disks**, expect a disk failure roughly **every 4 days**
- Research (Schroeder & Gibson, USENIX FAST 2007) showed **real-world failure rates are significantly higher** than manufacturer specifications

### 18.2 Annual Failure Rates (Observed)

Real-world studies show:
- Annual replacement rates of **2-4%** for enterprise disks
- Up to **8-10%** for consumer-grade disks
- Failure rates **increase with disk age** (not constant as MTTF assumes)

**This is precisely why GFS/HDFS design assumes failures are routine.**

---

## 19. Erasure Coding: An Alternative to Full Replication

### 19.1 The Problem with 3x Replication

3x replication provides excellent fault tolerance but at **200% storage overhead** — for every 1 TB of data, you need 3 TB of disk space.

### 19.2 Erasure Coding Concept

Erasure coding tolerates failures **without full replication** using parity information:

```
  Data Units (k=4)           Parity Units (m=2)
┌──────┬──────┬──────┬──────┬──────┬──────┐
│  D1  │  D2  │  D3  │  D4  │  P1  │  P2  │
└──────┴──────┴──────┴──────┴──────┴──────┘
```

- **k data blocks** + **m parity blocks** = total (k+m) blocks
- Can tolerate the loss of **any m blocks** (data or parity)
- Missing blocks are **reconstructed** from the remaining blocks using algebraic operations (Reed-Solomon encoding)

### 19.3 Storage Efficiency Comparison

| Method | Storage Overhead | Fault Tolerance |
|--------|-----------------|-----------------|
| **3x Replication** | 200% (3x storage) | Survives loss of any 2 copies |
| **RS(6,3)** — 6 data + 3 parity | 50% (1.5x storage) | Survives loss of any 3 blocks |
| **RS(4,2)** — 4 data + 2 parity | 50% (1.5x storage) | Survives loss of any 2 blocks |

### 19.4 Trade-offs

| Aspect | Replication | Erasure Coding |
|--------|-------------|----------------|
| **Storage efficiency** | Poor (3x) | Good (1.5x) |
| **Read performance** | Excellent (direct access) | Good (direct access if block available) |
| **Recovery cost** | Low (just copy) | High (compute from multiple blocks) |
| **Write overhead** | Low (just replicate) | Higher (compute parity) |
| **Best for** | Hot data, frequent access | Cold/warm data, archival |

HDFS supports erasure coding natively (since Hadoop 3.0) as an alternative to replication for infrequently accessed data.

---

## 20. Cloud Storage Categories

Modern cloud platforms offer multiple storage tiers, each serving different use cases:

### 20.1 IaaS Storage Categories

| Category | AWS | Azure | Use Case |
|----------|-----|-------|----------|
| **Object Storage** | S3 | Blob Storage | Unstructured data, media, backups, data lakes |
| **Block Storage** | Elastic Block Storage (EBS) | Azure Disks | VM disks, databases, low-latency I/O |
| **Network File System** | Elastic File System (NFS), Lustre | Azure Files (NFS, SMB), HPC Cache | Shared file access, HPC workloads |
| **Backup** | AWS Backup | Azure Backup | Data protection, compliance |
| **Sync & Transfer** | DataSync, Snow/Import-Export | FileSync, Bulk Transfer Disks | Data migration, hybrid cloud |

### 20.2 Where GFS/HDFS Fits

GFS/HDFS is a **cluster file system** — it maps most closely to the **Network File System** category in cloud storage, but at a much larger scale. Cloud object storage (S3, Azure Blob) has largely replaced HDFS for many use cases, but HDFS remains important for Hadoop-ecosystem workloads.

---

## 21. Other Distributed Storage Systems

| System | Year | Key Innovation |
|--------|------|---------------|
| **Ceph** | 2006 (OSDI) | Scalable, high-performance DFS with CRUSH algorithm for decentralized placement |
| **Kademlia** | 2002 | Peer-to-peer DHT using XOR distance metric for routing |
| **HopsFS** | 2017 | Scaled HDFS to **1 million+ operations per second** by distributing NameNode metadata across multiple nodes |

---

## 22. Summary: Key Takeaways

### From GFS Consistency & Mutations:

1. **Three mutation types** — write (client offset), append (client EOF), record append (GFS-chosen offset, atomic, at-least-once)

2. **Relaxed consistency** — defined > consistent > inconsistent. Applications use checksums, UUIDs, and checkpointing to handle the relaxed model.

3. **Leases ensure mutation order** — one primary per chunk assigns serial numbers; all replicas apply in the same order

4. **Data flow decoupled from control flow** — control ensures ordering (client → primary → secondaries); data is pipelined along the network topology for maximum throughput

5. **Snapshots use copy-on-write** — O(metadata) cost; data only copied on modification

### From HDFS:

6. **HDFS is the open-source GFS** — same architecture with different terminology (NameNode, DataNode, Block)

7. **Checkpoint/Backup Nodes** solve journal replay latency — periodic merging of FSImage + EditLog

8. **Pipeline writes** with 64 KB packets, async ACKs, and client-generated checksums

9. **Erasure coding** offers 50% overhead (vs 200% for 3x replication) for cold data

10. **Disk failures are routine** — real-world MTTF is much worse than manufacturer specs; design must assume failure

---

## References

1. Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung, "The Google File System", SOSP 2003
2. Konstantin Shvachko, Hairong Kuang, Sanjay Radia, Robert Chansler, "The Hadoop Distributed File System", IEEE MSST 2010
3. HDFS Architecture Guide, D. Borthakur, 2008
4. Konstantin V. Shvachko, "HDFS Scalability: The Limits to Growth", ;login: April 2010
5. Bianca Schroeder, Garth A. Gibson, "Disk Failures in the Real World", USENIX FAST 2007
6. Ismail, Mahmoud, et al., "Scaling HDFS to more than 1 million operations per second with HopsFS", IEEE/ACM CCGrid 2017
7. Eltabakh et al., "CoHadoop: Flexible Data Placement and Its Exploitation in Hadoop", VLDB 2011
8. Weil, Sage A., et al., "Ceph: A Scalable, High-Performance Distributed File System", OSDI 2006
9. DS256 Lecture 1.5 Slides — Yogesh Simmhan, IISc Bangalore
