# Lecture 2.1: Big Data Processing with Apache Spark (Part 1)

## DS256 - Scalable Systems for Data Science
### Module 2: Processing Large Volumes of Big Data

---

## 1. Role of Big Data Processing System for Large Data Volumes

When dealing with Big Data (especially Volume), we need a layered architecture where each layer builds upon the previous one, providing increasingly sophisticated capabilities.

### 1.1 The Big Data Processing Stack

```
┌─────────────────────────────────────────────────────┐
│         Specialized Processing System               │  E.g., Spark ML, GraphX
│   (Domain-specific algorithms: ML, Graph, etc.)     │
├─────────────────────────────────────────────────────┤
│          Generic Processing System                  │  E.g., Spark RDD, DataFrames
│   (General-purpose distributed computation)         │
├─────────────────────────────────────────────────────┤
│            Management System                        │  E.g., NoSQL (HBase, Cassandra)
│   (Structured access, queries, indexing)            │
├─────────────────────────────────────────────────────┤
│             Storage System                          │  E.g., GFS/HDFS, Ceph
│   (Reliable, distributed, fault-tolerant storage)   │
└─────────────────────────────────────────────────────┘
                        ▲
                        │
                  Data Arrives
```

### 1.2 Overview of Each Layer

#### Storage System (GFS/HDFS, Ceph)
- **Purpose**: Store massive amounts of data reliably across commodity hardware
- **Key Features**:
  - Distributed across hundreds/thousands of machines
  - Fault-tolerant through replication (typically 3 copies)
  - Optimized for large sequential reads/writes
  - Handles machine failures transparently
- **Examples**: Google File System (GFS), Hadoop Distributed File System (HDFS), Ceph
- **Trade-off**: High throughput for large files, but poor latency for small random accesses

#### Management System (NoSQL)
- **Purpose**: Provide structured access to data with indexing and query capabilities
- **Key Features**:
  - Schema-flexible or schema-less data models
  - Horizontal scalability (add more machines)
  - Support for CRUD operations (Create, Read, Update, Delete)
  - Various consistency models (eventual to strong)
- **Examples**: HBase (column-family), Cassandra (wide-column), MongoDB (document)
- **Trade-off**: Flexibility and scalability over ACID transactions

#### Generic Processing System (Spark RDD, DataFrames)
- **Purpose**: Execute arbitrary distributed computations on large datasets
- **Key Features**:
  - Data-parallel programming model
  - Fault-tolerant execution
  - In-memory computation for iterative algorithms
  - Rich set of transformations and actions
- **Examples**: Apache Spark (RDDs, DataFrames), Apache Flink
- **Trade-off**: General-purpose means not optimized for specific workloads

#### Specialized Processing System (Spark ML, GraphX)
- **Purpose**: Optimized libraries for specific domains
- **Key Features**:
  - Machine Learning: MLlib (classification, regression, clustering)
  - Graph Processing: GraphX (PageRank, connected components)
  - Stream Processing: Spark Streaming, Structured Streaming
- **Examples**: Spark MLlib, GraphX, Spark Streaming
- **Trade-off**: Highly optimized but limited to specific use cases

---

## 2. Storage System Fundamentals: Memory Hierarchy

Understanding the memory hierarchy is **critical** for designing efficient Big Data systems. The key insight is that different storage components have vastly different latencies and bandwidths.

### 2.1 The Memory Hierarchy

```
                    ┌───────────────┐
                    │  CPU Registers │  ← Fastest (< 1 ns)
                    │    (bytes)     │
                    └───────┬───────┘
                            │
                    ┌───────▼───────┐
                    │   L1 Cache    │  ← 0.5 ns
                    │   (32-64 KB)  │
                    └───────┬───────┘
                            │
                    ┌───────▼───────┐
                    │   L2 Cache    │  ← 7 ns
                    │  (256 KB-1MB) │
                    └───────┬───────┘
                            │
                    ┌───────▼───────┐
                    │   L3 Cache    │  ← 20-40 ns
                    │   (8-64 MB)   │
                    └───────┬───────┘
                            │
                    ┌───────▼───────┐
                    │  Main Memory  │  ← 100 ns
                    │   (4-256 GB)  │
                    └───────┬───────┘
                            │
                    ┌───────▼───────┐
                    │   SSD/NVMe    │  ← 10-150 μs
                    │  (256GB-8TB)  │
                    └───────┬───────┘
                            │
                    ┌───────▼───────┐
                    │    HDD        │  ← 10-20 ms
                    │  (1TB-20TB)   │
                    └───────┬───────┘
                            │
                    ┌───────▼───────┐
                    │   Network     │  ← 0.5-150 ms
                    │  (Unlimited)  │
                    └───────────────┘

        Speed ▲                          ▼ Capacity
              │                          │
        Cost  │                          ▼ Persistence
```

### 2.2 Key Differences Between Components

| Component | Capacity | Latency | Bandwidth | Volatile? | Cost/GB |
|-----------|----------|---------|-----------|-----------|---------|
| **L1 Cache** | 32-64 KB | 0.5 ns | ~1 TB/s | Yes | Highest |
| **L2 Cache** | 256 KB-1 MB | 7 ns | ~500 GB/s | Yes | Very High |
| **Main Memory (RAM)** | 4-256 GB | 100 ns | 25-100 GB/s | Yes | ~$3-5/GB |
| **SSD** | 256 GB-8 TB | 10-150 μs | 500 MB-7 GB/s | No | ~$0.10/GB |
| **HDD** | 1-20 TB | 10-20 ms | 100-250 MB/s | No | ~$0.02/GB |
| **Network (Local)** | Unlimited | 0.5 ms | 1-100 Gbps | N/A | - |
| **Network (WAN)** | Unlimited | 50-150 ms | Variable | N/A | - |

### 2.3 Why This Matters for Big Data

1. **Memory is fast but limited and volatile** → Can't store everything in RAM
2. **Disk is large but slow** → Need parallelism to achieve throughput
3. **Network is the biggest bottleneck** → Move computation to data, not data to computation

---

## 3. MapReduce

MapReduce is a programming model and runtime system introduced by Google in 2004. It provides a simple abstraction for processing large datasets in parallel across a cluster of commodity machines.

### 3.1 The MapReduce Vision

> *"A simple and powerful interface that enables automatic parallelization and distribution of large-scale computations, combined with an implementation of this interface that achieves high performance on large clusters of commodity PCs."*
> — Dean and Ghemawat, OSDI 2004

### 3.2 Google's Cluster Model (2004)

The MapReduce paper describes the computing environment at Google in 2004:

| Component | Specification (2004) | Modern Equivalent |
|-----------|---------------------|-------------------|
| **CPU** | 2 x86 processors | 16-64 cores |
| **Memory** | 2-4 GB RAM | 64-256 GB RAM |
| **Network** | 100 Mbps - 1 Gbps per machine | 10-100 Gbps |
| **Cluster Size** | 100s - 1000s of machines | 1000s - 10000s |
| **Storage** | IDE disks (local) | SSDs + HDDs |
| **File System** | GFS | HDFS, Cloud Storage |

**Key Characteristics:**
- **Commodity hardware**: Not specialized supercomputers, but regular PCs
- **Failures are common**: With 1000s of machines, something is always failing
- **Low bisection bandwidth**: Network is the bottleneck (not enough bandwidth for all-to-all communication)
- **Storage is local**: Each machine has its own disks (managed by GFS)
- **Jobs submitted to scheduler**: Users don't directly control which machines run their code

### 3.3 MapReduce Design Pattern

MapReduce provides:

1. **Clean Abstraction for Programmers**
   - Users write just two functions: `map` and `reduce`
   - No need to worry about parallelization, distribution, or fault tolerance

2. **Automatic Parallelization & Distribution**
   - Framework handles splitting data and distributing tasks
   - Runs on hundreds/thousands of machines automatically

3. **Fault Tolerance**
   - Automatically re-executes failed tasks
   - No data loss even when machines fail

4. **Batch Data Processing**
   - Designed for large input sizes (TB/PB scale)
   - Not for real-time or interactive queries

### 3.4 Example Applications

| Application | Map Function | Reduce Function |
|-------------|--------------|-----------------|
| **Distributed Grep** | Emit line if matches pattern | Identity (copy to output) |
| **URL Access Frequency** | `<URL, 1>` for each access | Sum counts for each URL |
| **Reverse Web-Link Graph** | `<target, source>` for each link | Concatenate all sources per target |
| **Term-Vector per Host** | `<hostname, term_vector>` | Add term vectors for same host |
| **Inverted Index** | `<word, docID>` for each word | Collect all docIDs per word |
| **Distributed Sort** | `<key, record>` | Emit unchanged (partitioning does the work) |

---

## 4. MapReduce: Data-Parallel Programming Model

### 4.1 The Core Model

MapReduce processes data using two user-defined functions:

```
Input Data (K1, V1 pairs)
         │
         ▼
┌─────────────────────────────────────────────────────┐
│  MAP: (K1, V1) → List<K2, V2>                       │
│  - Called once on every input item                  │
│  - Emits zero or more intermediate key/value pairs  │
└─────────────────────────────────────────────────────┘
         │
         ▼
    Intermediate Data (K2, V2 pairs)
         │
         ▼
┌─────────────────────────────────────────────────────┐
│  SHUFFLE & SORT (Internal to Framework)             │
│  - Groups all values with the same key together     │
│  - Sorts keys within each group                     │
└─────────────────────────────────────────────────────┘
         │
         ▼
    Grouped Data (K2, List<V2>)
         │
         ▼
┌─────────────────────────────────────────────────────┐
│  REDUCE: (K2, List<V2>) → List<K3, V3>              │
│  - Called once on every unique key                  │
│  - Combines all values for a key into output        │
└─────────────────────────────────────────────────────┘
         │
         ▼
Output Data (K3, V3 pairs)
```

### 4.2 Type Signature

```
map:    (K1, V1)        → List<K2, V2>
reduce: (K2, List<V2>)  → List<K3, V3>
```

**Important**: Input key/value types (K1, V1) can be different from intermediate types (K2, V2), which can be different from output types (K3, V3).

### 4.3 Word Count Example

The classic MapReduce example - counting word frequencies in documents:

```python
# Pseudo-code

def map(document_name, document_contents):
    # document_name: e.g., "file1.txt"
    # document_contents: the text content of the file
    for word in document_contents.split():
        emit(word, 1)  # Emit <word, 1> for each occurrence

def reduce(word, counts):
    # word: a unique word
    # counts: list of all 1s emitted for this word [1, 1, 1, ...]
    total = sum(counts)
    emit(word, total)  # Emit <word, total_count>
```

**Execution Flow:**
```
Input: "hello world hello"

MAP:
  emit("hello", 1)
  emit("world", 1)
  emit("hello", 1)

SHUFFLE & SORT:
  "hello" → [1, 1]
  "world" → [1]

REDUCE:
  reduce("hello", [1, 1]) → emit("hello", 2)
  reduce("world", [1])    → emit("world", 1)

Output:
  hello 2
  world 1
```

---

## 5. Inverted Index using MapReduce

An **Inverted Index** is a fundamental data structure for search engines. It maps each word to the list of documents containing that word.

### 5.1 The Task

**Goal**: Given a collection of web pages/documents, build an index that answers:
> "Which documents contain the word X?"

**Example Input:**
```
doc1: "the quick brown fox"
doc2: "the lazy dog"
doc3: "the quick dog jumps"
```

**Desired Output (Inverted Index):**
```
the    → [doc1, doc2, doc3]
quick  → [doc1, doc3]
brown  → [doc1]
fox    → [doc1]
lazy   → [doc2]
dog    → [doc2, doc3]
jumps  → [doc3]
```

### 5.2 MapReduce Solution

```
                    MAP                     SHUFFLE                 REDUCE
                     
doc1: "the quick"   ──→  (the, doc1)    ┐
                         (quick, doc1)  ─┼─→ (the, [doc1,doc2,doc3]) ──→ (the, [doc1,doc2,doc3])
                                        │
doc2: "the lazy"    ──→  (the, doc2)    ┘    (quick, [doc1,doc3])   ──→ (quick, [doc1,doc3])
                         (lazy, doc2)   ───→ (lazy, [doc2])         ──→ (lazy, [doc2])
                                        
doc3: "the quick"   ──→  (the, doc3)         (dog, [doc2,doc3])     ──→ (dog, [doc2,doc3])
                         (quick, doc3)
```

### 5.3 Implementation

```python
def map(url, page_content):
    """
    Input:  url = document identifier
            page_content = text content of the page
    Output: Emits (word, url) for each word in the page
    """
    for word in page_content.split():
        emit(word, url)

def reduce(word, url_list):
    """
    Input:  word = a unique word
            url_list = list of all URLs where this word appears
    Output: Emits (word, distinct_urls) - the inverted index entry
    """
    distinct_urls = remove_duplicates(url_list)
    emit(word, distinct_urls)
```

### 5.4 Why MapReduce is Perfect for This

1. **Embarrassingly Parallel in Map Phase**: Each document can be processed independently
2. **Natural Grouping**: Shuffle automatically groups all occurrences of the same word
3. **Scalable**: Can process billions of web pages
4. **Fault-Tolerant**: If a machine fails, just re-process those documents

---

## 6. Map-Shuffle-Sort-Reduce: The Complete Pipeline

### 6.1 Detailed Execution Flow

```
        ┌───────────────────────────────────────────────────────────────────────────┐
        │                              MAPREDUCE JOB                                │
        └───────────────────────────────────────────────────────────────────────────┘

                    MAP PHASE                                    REDUCE PHASE
        ┌─────────────────────────────┐               ┌─────────────────────────────┐
        │                             │               │                             │
        │   Worker A      Worker B    │               │   Worker X      Worker Y    │
        │  ┌───────┐     ┌───────┐    │               │  ┌───────┐     ┌───────┐    │
        │  │ Map 1 │     │ Map 3 │    │               │  │Reduce1│     │Reduce2│    │
        │  └───┬───┘     └───┬───┘    │               │  └───┬───┘     └───┬───┘    │
        │      │             │        │               │      │             │        │
        │  ┌───┴───┐     ┌───┴───┐    │               │      │             │        │
        │  │ Map 2 │     │ Map 4 │    │               │      │             │        │
        │  └───┬───┘     └───┬───┘    │               │      │             │        │
        └──────┼─────────────┼────────┘               └──────┼─────────────┼────────┘
               │             │                               ▲             ▲
               ▼             ▼                               │             │
        ┌──────────────────────────────────────────────────────────────────────────┐
        │                                                                          │
        │                    SHUFFLE & SORT (Network Transfer)                     │
        │                                                                          │
        │   Each Map task writes intermediate data partitioned by reduce task      │
        │   Each Reduce task fetches its partition from ALL map tasks              │
        │                                                                          │
        └──────────────────────────────────────────────────────────────────────────┘
```

### 6.2 Phase-by-Phase Breakdown

#### Phase 1: MAP
- **What happens**: 
  - Input data is split into M chunks (typically 16-64 MB each)
  - Each map task processes one chunk
  - User's map function is called on each key-value pair
  - Intermediate key-value pairs are buffered in memory

- **Where it runs**: On machines that have the input data (locality optimization)

- **Output**: Intermediate key-value pairs stored locally on the mapper's disk

#### Phase 2: SHUFFLE (The Expensive Part!)
- **What happens**:
  - Intermediate data is partitioned by key (using `hash(key) mod R`)
  - Each reducer fetches its partition from ALL mappers
  - This requires network transfer across the cluster

- **Why it's expensive**:
  - Every mapper must send data to every reducer
  - Network bandwidth becomes the bottleneck
  - If there are M mappers and R reducers, there are M × R data transfers

```
Mappers                          Reducers
┌─────┐                         ┌─────────┐
│ M1  │────────┬───────────────▶│   R1    │
└─────┘        │       ┌───────▶│         │
               │       │        └─────────┘
┌─────┐        │       │        
│ M2  │────────┼───────┤        ┌─────────┐
└─────┘        │       └───────▶│   R2    │
               │       ┌───────▶│         │
┌─────┐        │       │        └─────────┘
│ M3  │────────┴───────┘        
└─────┘                         
                   ▲
                   │
         All-to-All Communication!
         (This is the bottleneck)
```

#### Phase 3: SORT
- **What happens**:
  - Each reducer sorts its received data by key
  - Groups all values with the same key together
  - Prepares data for the reduce function

- **Why it's needed**:
  - Reduce function expects all values for a key together
  - Sorting enables efficient grouping

- **Cost**: External sort may be needed if data doesn't fit in memory

#### Phase 4: REDUCE
- **What happens**:
  - For each unique key, reduce function is called with all its values
  - Output is written to the distributed file system (GFS/HDFS)
  
- **Output**: R output files (one per reducer)

### 6.3 Why Shuffle and Sort Should Be Avoided/Minimized

The shuffle and sort phases are the **most expensive** parts of MapReduce:

| Reason | Impact |
|--------|--------|
| **Network Transfer** | All intermediate data crosses the network |
| **Disk I/O** | Data is written to disk by mappers, read by reducers |
| **Sorting Cost** | O(n log n) for sorting, plus external sort overhead |
| **Synchronization** | Reducers must wait for ALL mappers to complete |
| **Data Amplification** | Same data may be shuffled multiple times in multi-stage jobs |

**Strategies to Minimize Shuffle:**

1. **Combiners**: Pre-aggregate data at the mapper before shuffle
   ```
   Without combiner: Map emits [<a,1>, <a,1>, <a,1>] → 3 items shuffled
   With combiner:    Map emits [<a,3>]              → 1 item shuffled
   ```

2. **Smart Partitioning**: Ensure related data goes to the same reducer

3. **Filter Early**: Reduce data volume before shuffle using map-side filtering

4. **Avoid Unnecessary Shuffle**: Some operations don't need reduce at all

---

## 7. Histogram using MapReduce

### 7.1 The Task

Build a histogram of values, where each bucket contains the count of values falling within a range.

**Example**: Given values [0-11], create buckets of width 4:
- Bucket 0: values 0-3
- Bucket 1: values 4-7
- Bucket 2: values 8-11

### 7.2 Input Data

```
7    2    11   2
2    1    11   4
9    10   6    6
6    3    2    8
0    5    1    10
2    4    8    11
5    0    1    0
```

### 7.3 MapReduce Solution

```python
bucketWidth = 4  # Configurable parameter

def map(key, value):
    """
    Input:  value = a number
    Output: Emit (bucketID, 1)
    """
    bucketID = floor(value / bucketWidth)
    emit(bucketID, 1)

def reduce(bucketID, counts):
    """
    Input:  bucketID = bucket identifier
            counts = list of 1s for each value in this bucket
    Output: Emit (bucketID, frequency)
    """
    frequency = sum(counts)
    emit(bucketID, frequency)
```

### 7.4 Execution Trace

```
Input Values (4 partitions):
    [7,2,9,6,0,2,5]  [2,1,10,3,5,4,0]  [11,11,6,2,1,8,1]  [2,4,6,8,10,11,0]

MAP Phase (floor(value/4)):
    Partition 1:     Partition 2:      Partition 3:       Partition 4:
    (1,1) ← 7        (0,1) ← 2         (2,1) ← 11         (0,1) ← 2
    (0,1) ← 2        (0,1) ← 1         (2,1) ← 11         (1,1) ← 4
    (2,1) ← 9        (2,1) ← 10        (1,1) ← 6          (1,1) ← 6
    (1,1) ← 6        (0,1) ← 3         (0,1) ← 2          (2,1) ← 8
    (0,1) ← 0        (1,1) ← 5         (0,1) ← 1          (2,1) ← 10
    (0,1) ← 2        (1,1) ← 4         (2,1) ← 8          (2,1) ← 11
    (1,1) ← 5        (0,1) ← 0         (0,1) ← 1          (0,1) ← 0

SHUFFLE Phase (28 items transferred across network):
    Key 0 → [(0,1), (0,1), (0,1), (0,1), (0,1), (0,1), (0,1), (0,1), (0,1), (0,1), (0,1), (0,1)]
    Key 1 → [(1,1), (1,1), (1,1), (1,1), (1,1), (1,1), (1,1), (1,1)]
    Key 2 → [(2,1), (2,1), (2,1), (2,1), (2,1), (2,1), (2,1), (2,1)]

REDUCE Phase:
    Bucket 0: 12 values (0,1,2,3)
    Bucket 1: 8 values  (4,5,6,7)
    Bucket 2: 8 values  (8,9,10,11)

Output:
    (0, 12)
    (1, 8)
    (2, 8)
```

### 7.5 Using a Combiner for Optimization

Without combiner: 28 items shuffled
With combiner: Pre-aggregate at each mapper

```python
def combiner(bucketID, counts):
    """Same as reduce - can be used because sum is associative"""
    emit(bucketID, sum(counts))
```

```
After Combiner (before shuffle):
    Partition 1: (0,2), (1,3), (2,1)   → 3 items
    Partition 2: (0,4), (1,2), (2,1)   → 3 items
    Partition 3: (0,3), (1,1), (2,3)   → 3 items
    Partition 4: (0,2), (1,2), (2,3)   → 3 items
    
Total items shuffled: 12 (vs 28 without combiner) → 57% reduction!
```

---

## 8. Limitations of MapReduce

### 8.1 Expressivity Limitations

#### Problem 1: Multi-Stage Computing is Complex

Real analytics pipelines often require multiple stages, but MapReduce only provides a single Map→Reduce step.

```
                                      ┌────────────┐
                                      │Analytics 1 │
┌───────────┐    ┌───────────┐    ┌───┴────────────┴───┐    ┌───────────┐
│PreProcess │───▶│ Transform │───▶│       Join         │───▶│ Visualize │
└───────────┘    └───────────┘    └───┬────────────┬───┘    └───────────┘
                                      │Analytics 2 │
                                      └────────────┘
```

**In MapReduce**:
- Each box = separate MapReduce job
- Each job reads from / writes to HDFS
- Must manage job dependencies manually
- Intermediate data written to disk between stages
- **Result**: Complex code, poor performance for iterative algorithms

#### Problem 2: Complex Code for Simple Transformations

Even simple operations require writing full map/reduce functions:

```python
# In a modern system (SQL or DataFrame):
result = data.filter(x > 5).groupBy(key).sum()

# In MapReduce: Need to write multiple classes, configure jobs, etc.
# Repetitive boilerplate code obscures the actual logic
```

#### Problem 3: Limited Support for Non-Text, Non-Static Data

- Originally designed for web crawl data (text files)
- Poor support for:
  - Streaming data
  - Graph data
  - Complex nested structures
  - Real-time updates

### 8.2 Performance Limitations

#### Problem 1: Iterative Algorithms

Many ML algorithms are iterative (repeat until convergence):

```
┌─────────────────────────────────────────────────────────────────┐
│                      Iterative Algorithm                        │
│                                                                 │
│  ┌──────┐     ┌──────┐     ┌──────┐     ┌──────┐              │
│  │ MR 1 │────▶│ MR 2 │────▶│ MR 3 │────▶│ MR n │────▶ Done    │
│  └──────┘     └──────┘     └──────┘     └──────┘              │
│      │            │            │            │                   │
│      ▼            ▼            ▼            ▼                   │
│    HDFS         HDFS         HDFS         HDFS                  │
│  (write)      (read/write) (read/write) (read/write)            │
│                                                                 │
│  Each iteration: Read from disk → Process → Write to disk      │
│  No data reuse between iterations!                              │
└─────────────────────────────────────────────────────────────────┘
```

**Example: PageRank**
- Requires 10-50 iterations
- Each iteration reads/writes the entire graph
- Same data re-read from disk every iteration
- **Result**: Hours instead of minutes

#### Problem 2: Interactive Queries

- MapReduce has high job startup overhead
- Each query = new MapReduce job
- Not suitable for exploratory data analysis
- **Result**: Minutes per query instead of seconds

---

## 9. Latency and Bandwidth: Understanding the Numbers

This is **critical** for understanding why certain design decisions are made in distributed systems.

### 9.1 The Latency Numbers Every Programmer Should Know

| Operation | Time | Scaled Comparison |
|-----------|------|-------------------|
| **L1 cache reference** | 0.5 ns | 1 second |
| **L2 cache reference** | 7 ns | 14 seconds |
| **Main memory reference** | 100 ns | 3.3 minutes |
| **Send 1KB over 1Gbps network** | 10,000 ns (10 μs) | 5.5 hours |
| **Read 4KB randomly from SSD** | 150,000 ns (150 μs) | 3.5 days |
| **Read 1MB sequentially from memory** | 250,000 ns (250 μs) | 5.8 days |
| **Round trip within datacenter** | 500,000 ns (500 μs) | 11.6 days |
| **Read 1MB sequentially from SSD** | 1,000,000 ns (1 ms) | 23 days |
| **Send 1MB over 1Gbps network** | 8,250,000 ns (8.25 ms) | 190 days |
| **HDD disk seek** | 10,000,000 ns (10 ms) | 231 days |
| **Read 1MB sequentially from HDD** | 20,000,000 ns (20 ms) | 1.3 years |
| **Send packet CA → Netherlands → CA** | 150,000,000 ns (150 ms) | 9.5 years |

### 9.2 Visualizing the Scale

```
Latency Scale (Log Scale)

    0.5ns │█ L1 cache
          │
      7ns │██ L2 cache
          │
    100ns │███ Main memory
          │
          │                         14x slower
          │
    10μs  │████████████ Network (1KB)
          │
   150μs  │██████████████████████ SSD random read
          │
   250μs  │████████████████████████ Memory sequential (1MB)
          │
   500μs  │██████████████████████████████ Datacenter RTT
          │
     1ms  │████████████████████████████████ SSD sequential (1MB)
          │
   8.3ms  │████████████████████████████████████████ Network (1MB)
          │
    10ms  │██████████████████████████████████████████ HDD seek
          │
    20ms  │████████████████████████████████████████████ HDD seq (1MB)
          │
   150ms  │████████████████████████████████████████████████████ WAN RTT
          │
          └─────────────────────────────────────────────────────────────▶
               Nanoseconds        Microseconds        Milliseconds
```

### 9.3 Key Insights

#### Insight 1: Memory is MUCH faster than everything else

```
Reading 1MB:
  From Memory:  250 μs
  From SSD:     1 ms      (4x slower than memory)
  From HDD:     20 ms     (80x slower than memory)
  From Network: 8.25 ms   (33x slower than memory)
```

**Implication**: Keep frequently accessed data in memory!

#### Insight 2: Sequential access >> Random access

```
SSD:
  Random 4KB read:  150 μs
  Sequential 1MB:   1 ms (1,000 μs)
  
  Sequential reads 256 x 4KB = 1MB
  But random 256 reads × 150 μs = 38.4 ms
  
  Sequential is 38x faster!

HDD:
  Seek time:        10 ms
  Sequential 1MB:   20 ms
  
  Random read (seek + read 4KB): ~10 ms
  Sequential 1MB: 20 ms = 256 × 4KB
  
  Sequential is 128x faster!
```

**Implication**: Design for sequential access patterns!

#### Insight 3: Network is expensive

```
Sending data:
  1KB over network:  10 μs
  1MB over network:  8.25 ms
  
Compare to:
  1MB from memory:   250 μs
  
Network is 33x slower than memory for 1MB!
```

**Implication**: Minimize data movement across the network. This is why:
- MapReduce moves computation to data (not data to computation)
- Shuffle phase is the bottleneck
- Data locality is critical

#### Insight 4: Bandwidth of Memory >> Network >> Disk

```
Bandwidth comparison (approximate):

Memory:    ~25-100 GB/s
SSD:       ~0.5-7 GB/s
Network:   ~1-12.5 GB/s (10-100 Gbps)
HDD:       ~0.1-0.25 GB/s

Memory : Network : HDD = 100 : 10 : 1 (order of magnitude)
```

### 9.4 Why This Matters for MapReduce

The shuffle phase in MapReduce involves:
1. **Disk write** (mapper writes intermediate data): ~20 ms/MB
2. **Network transfer** (mapper → reducer): ~8.25 ms/MB
3. **Disk read** (reducer reads intermediate data): ~20 ms/MB
4. **Sort** (in memory or external): additional cost

**Total for 1MB of shuffle**: ~50+ ms

Compare to just processing 1MB in memory: ~250 μs

**Shuffle is 200x slower than in-memory processing!**

This is the fundamental reason why:
- Iterative algorithms are slow in MapReduce
- Spark's in-memory processing is revolutionary
- Combiners are essential for reducing shuffle

---

## 10. Why MapReduce Fails for Modern Workloads

### 10.1 The Core Problem: Disk-Based Processing

```
┌─────────────────────────────────────────────────────────────────────────┐
│                     MapReduce Execution Model                           │
│                                                                         │
│   HDFS ──Read──▶ Map ──Write──▶ Local Disk ──Read──▶ Reduce ──Write──▶ HDFS  │
│    │                                │                               │   │
│    ▼                                ▼                               ▼   ▼
│  Disk I/O                       Disk I/O                         Disk I/O
│                                                                         │
│   Every stage involves disk!                                            │
│   No data reuse between stages!                                         │
│   High latency for all operations!                                      │
└─────────────────────────────────────────────────────────────────────────┘
```

### 10.2 Summary of MapReduce Limitations

| Category | Limitation | Impact |
|----------|------------|--------|
| **Expressivity** | Only Map→Reduce pattern | Complex pipelines need multiple jobs |
| **Expressivity** | Verbose code | Simple operations require boilerplate |
| **Expressivity** | Text-centric | Poor support for graphs, streams, complex types |
| **Performance** | Disk-based | Every stage reads/writes disk |
| **Performance** | No data reuse | Iterative algorithms re-read data |
| **Performance** | High startup overhead | Not suitable for interactive queries |
| **Performance** | Shuffle bottleneck | Network I/O dominates for many workloads |

### 10.3 The Need for Something Better

This sets the stage for Apache Spark:
- In-memory processing (avoid disk between stages)
- Rich API (beyond Map→Reduce)
- Data reuse across operations (RDD caching)
- Low latency (interactive queries)
- Unified engine (batch, streaming, ML, graphs)

---

## Summary

### Key Takeaways from Lecture 2.1 Part 1:

1. **Big Data Processing Stack**: Storage → Management → Generic Processing → Specialized Processing

2. **Memory Hierarchy**: Understand the massive latency differences between cache, memory, SSD, HDD, and network

3. **MapReduce Model**: map(k,v) → list(k',v') followed by reduce(k', list(v')) → list(k'',v'')

4. **Shuffle is Expensive**: Network + disk I/O makes shuffle the bottleneck

5. **Latency Numbers Matter**: 
   - Memory: 100 ns
   - Network (1MB): 8 ms
   - Disk (1MB): 20 ms
   - Memory is 100-200x faster!

6. **MapReduce Limitations**: 
   - Disk-based (no in-memory reuse)
   - Single stage (complex for pipelines)
   - High latency (bad for iterative/interactive)

7. **This Sets the Stage for Spark**: In-memory, lazy evaluation, rich API

---

## References

1. Dean, J. and Ghemawat, S., "MapReduce: Simplified Data Processing on Large Clusters", USENIX OSDI, 2004
2. Zaharia, M., et al., "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing", USENIX NSDI, 2012
3. Latency Numbers Every Programmer Should Know: https://gist.github.com/jboner/2841832
4. Lin, J. and Dyer, C., "Data-Intensive Text Processing with MapReduce", Morgan & Claypool, 2010

# Lecture 2.1: Big Data Processing with Apache Spark (Part 2)

## DS256 - Scalable Systems for Data Science
### Module 2: Processing Large Volumes of Big Data

---

## 1. Understanding Latency and Bandwidth

Before diving into Spark, let's clarify two fundamental concepts that determine system performance: **Latency** and **Bandwidth**.

### 1.1 What is Latency?

**Latency** is the **time delay** between initiating a request and receiving the first response. Think of it as "how long do I have to wait before something starts happening?"

```
┌──────────────┐                              ┌──────────────┐
│   Request    │ ─────── Latency ──────────▶  │   Response   │
│   Initiated  │        (waiting time)        │   Starts     │
└──────────────┘                              └──────────────┘
                    
     t = 0                                    t = latency
```

**Analogy**: When you order food at a restaurant, latency is how long you wait before the first dish arrives at your table.

**Examples**:
- L1 cache access latency: 0.5 ns
- Main memory access latency: 100 ns
- SSD random read latency: 150 μs
- HDD seek latency: 10 ms
- Network round-trip (same datacenter): 0.5 ms
- Network round-trip (cross-continent): 150 ms

### 1.2 What is Bandwidth?

**Bandwidth** is the **rate** at which data can be transferred once the transfer has started. Think of it as "how much data can flow per second?"

```
┌──────────────────────────────────────────────────────────┐
│                                                          │
│  ═══════════════════════════════════════════════════▶   │
│                                                          │
│  Bandwidth = Amount of Data / Time = MB/s or GB/s        │
│                                                          │
└──────────────────────────────────────────────────────────┘
```

**Analogy**: If latency is how long you wait for the food to arrive, bandwidth is how many dishes per minute the kitchen can produce once they start cooking.

**Examples**:
- Main memory bandwidth: ~25-100 GB/s
- SSD sequential bandwidth: ~0.5-7 GB/s
- Network bandwidth (1 Gbps): ~125 MB/s
- Network bandwidth (10 Gbps): ~1.25 GB/s
- HDD sequential bandwidth: ~100-250 MB/s

### 1.3 Latency vs Bandwidth: The Key Difference

| Aspect | Latency | Bandwidth |
|--------|---------|-----------|
| **Measures** | Time to start | Rate of transfer |
| **Unit** | Time (ns, μs, ms) | Data/Time (MB/s, GB/s) |
| **Affected by** | Distance, processing overhead | Physical capacity of channel |
| **Analogy** | Time to first byte | Bytes per second |

### 1.4 Why Both Matter

For a complete data transfer:
```
Total Time = Latency + (Data Size / Bandwidth)
```

**Small transfers** are dominated by **latency**:
```
Transfer 1 KB over network:
  Latency: 500 μs (datacenter round-trip)
  Transfer: 1KB / 125 MB/s = 8 μs
  Total: ~508 μs  ← Latency dominates!
```

**Large transfers** are dominated by **bandwidth**:
```
Transfer 1 GB over network:
  Latency: 500 μs
  Transfer: 1GB / 125 MB/s = 8 seconds
  Total: ~8 seconds ← Bandwidth dominates!
```

### 1.5 The Critical Numbers (Revisited)

| Operation | Time | Category |
|-----------|------|----------|
| L1 cache reference | 0.5 ns | Memory |
| L2 cache reference | 7 ns | Memory |
| Main memory reference | 100 ns | Memory |
| Read 1MB from memory | 250 μs | Memory |
| Send 1KB over 1Gbps network | 10 μs | Network |
| SSD random read (4KB) | 150 μs | Storage |
| Datacenter round-trip | 500 μs | Network |
| Read 1MB from SSD | 1 ms | Storage |
| Send 1MB over 1Gbps network | 8.25 ms | Network |
| HDD seek | 10 ms | Storage |
| Read 1MB from HDD | 20 ms | Storage |
| Cross-continent round-trip | 150 ms | Network |

---

## 2. Bandwidth Comparison: Memory >> Network >> Disk

### 2.1 The Critical Insight

```
Bandwidth Comparison (Reading 1 MB):

Memory:     ████████████████████████████████████████ 4,000 MB/s (250 μs)
SSD:        ██████████                               1,000 MB/s (1 ms)
Network:    █████                                      121 MB/s (8.25 ms)
HDD:        ██                                          50 MB/s (20 ms)

Memory is 4x faster than SSD
Memory is 33x faster than Network
Memory is 80x faster than HDD
```

### 2.2 What This Means for System Design

```
┌─────────────────────────────────────────────────────────────────────┐
│                         THE GOLDEN RULE                              │
│                                                                      │
│   Bandwidth of Memory  ≫  Network  ≫  Disk                          │
│                                                                      │
│   Therefore:                                                         │
│   1. Keep data in memory as much as possible                        │
│   2. Minimize network transfers                                      │
│   3. When disk is needed, use sequential access                     │
│   4. Move computation to data, not data to computation              │
└─────────────────────────────────────────────────────────────────────┘
```

### 2.3 Why MapReduce is Slow

MapReduce violates this principle at every step:

```
MapReduce Flow:
                                                   
HDFS (Disk) ──Read──▶ Map ──Write──▶ Local Disk ──Read──▶ Shuffle ──Network──▶ 
                                                                              │
                      Write ◀──Disk── Reduce ◀──Read──Disk──────────────────┘

Every arrow = expensive I/O operation!
```

### 2.4 Why Spark is Faster

Spark keeps data in memory:

```
Spark Flow (with caching):

HDFS ──Read (once)──▶ Memory ──Transform──▶ Memory ──Transform──▶ Memory ──Action──▶ Result
                        ▲                                              │
                        └──────────────── Reuse! ──────────────────────┘

Data stays in memory between operations!
```

---

## 3. Failures and Performance Trade-offs

### 3.1 Mean Time Between Failures (MTBF)

Research on datacenter failures reveals:

> *"The MTBF across all data centers we investigate (with hundreds of thousands of servers) is only 6.8 minutes, while the MTBF in different data centers varies between 32 minutes and 390 minutes."*

| Cluster Size | MTBF |
|--------------|------|
| Full datacenter (100K+ servers) | 6.8 minutes |
| 1,000 servers | ~680 minutes (~11 hours) |
| 100 servers | ~6,800 minutes (~4.7 days) |

### 3.2 The Key Insight

For a typical Spark job running on 100 machines:
- Job duration: Usually minutes to hours
- MTBF: ~4.7 days

```
┌─────────────────────────────────────────────────────────────────────┐
│                                                                      │
│   "Failures may be infrequent during the lifetime of an            │
│    application execution"                                            │
│                                                                      │
│   Therefore:                                                         │
│   → Optimize for PERFORMANCE first                                  │
│   → Handle failures through RECOVERY mechanisms (lineage)           │
│   → Don't pay for fault tolerance on every operation                │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 3.3 Spark's Approach

Instead of writing intermediate data to disk (like MapReduce does for fault tolerance), Spark:

1. **Keeps data in memory** → Maximum performance
2. **Tracks lineage** → Can recompute lost partitions if failure occurs
3. **Only checkpoints** when explicitly asked → User controls the trade-off

This is a fundamental design philosophy: **optimize for the common case (no failures), handle the rare case (failures) efficiently**.

---

## 4. From MapReduce to Spark

### 4.1 The Evolution

```
┌─────────────────────────────────────────────────────────────────────┐
│                          Google's MapReduce                          │
│                                                                      │
│   • 2004: Original paper                                            │
│   • Programming Model: Map → Shuffle → Reduce                        │
│   • Disk-based: Write to disk after every stage                     │
│   • Limited: Only map and reduce operations                         │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│                          Apache Hadoop                               │
│                                                                      │
│   • Open-source implementation of MapReduce                         │
│   • HDFS for distributed storage                                    │
│   • Widely adopted in industry                                      │
│   • Same limitations as Google's MapReduce                          │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│                          Apache Spark                                │
│                                                                      │
│   • 2012: RDD paper (Zaharia et al., NSDI)                          │
│   • In-memory processing                                            │
│   • Rich API: Many transformations and actions                      │
│   • Lazy evaluation for optimization                                │
│   • Unified engine for batch, streaming, ML, graphs                 │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 4.2 Key Differences

| Aspect | MapReduce | Spark |
|--------|-----------|-------|
| **Processing Model** | Disk-based | Memory-based |
| **Data Abstraction** | Files | RDDs (Resilient Distributed Datasets) |
| **Intermediate Data** | Written to HDFS | Kept in memory |
| **Operations** | Map, Reduce only | 80+ transformations/actions |
| **Iteration Support** | Poor (disk I/O each iteration) | Excellent (in-memory) |
| **Interactive Queries** | Poor (job startup overhead) | Excellent (keep data cached) |
| **Fault Tolerance** | Replication | Lineage-based recomputation |
| **Speed** | Baseline | 10-100x faster for iterative jobs |

---

## 5. Apache Spark

### 5.1 The Spark Ecosystem

```
┌─────────────────────────────────────────────────────────────────────┐
│                     Higher-Level Abstractions                        │
│                                                                      │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────┐│
│  │  Spark SQL   │  │   Spark      │  │    MLlib     │  │  GraphX  ││
│  │              │  │  Streaming   │  │              │  │          ││
│  │  DataFrames  │  │  DStreams    │  │  ML Library  │  │  Graph   ││
│  │  SQL Queries │  │  Real-time   │  │  Algorithms  │  │  Analytics│
│  └──────────────┘  └──────────────┘  └──────────────┘  └──────────┘│
└───────────────────────────────┬─────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────┐
│                        Core Spark Engine                             │
│                                                                      │
│   • RDDs (Resilient Distributed Datasets)                           │
│   • Transformations & Actions                                        │
│   • Batch Processing                                                 │
│   • Task Scheduling & Memory Management                              │
│   • Fault Recovery (Lineage)                                         │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────────┐
│                        Cluster Managers                              │
│                                                                      │
│          Standalone    │    YARN    │    Mesos    │    K8s          │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 5.2 Core Spark Engine

The foundation of Spark, providing:
- **RDDs**: The fundamental data abstraction
- **Transformations**: Operations that create new RDDs (lazy)
- **Actions**: Operations that trigger computation and return results
- **Batch Processing**: Processing large datasets efficiently

### 5.3 Higher-Level Abstractions

#### Spark SQL & DataFrames
- SQL-like queries on structured data
- DataFrames: RDDs with schema information
- Optimized execution via Catalyst optimizer

#### Spark Streaming
- Process live data streams
- Discretized Streams (DStreams): RDDs over time
- Near real-time processing

#### MLlib
- Machine learning library
- Classification, regression, clustering, collaborative filtering
- Scalable implementations of standard algorithms

#### GraphX
- Graph-parallel computation
- PageRank, connected components, triangle counting
- Built on RDDs for graphs

---

## 6. Spark: A Distributed Execution Engine

### 6.1 Key Components

Let's understand each component clearly:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              CLUSTER                                         │
│                                                                              │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                         DRIVER NODE                                   │   │
│   │   ┌─────────────────────────────────────────────────────────────┐   │   │
│   │   │                    DRIVER PROGRAM                            │   │   │
│   │   │                                                              │   │   │
│   │   │   ┌─────────────────────────┐    ┌───────────────────────┐   │   │   │
│   │   │   │     SparkContext        │    │   Local Variables     │   │   │   │
│   │   │   │   (Connection to        │    │   (Driver's Memory)   │   │   │   │
│   │   │   │    Cluster)             │    │                       │   │   │   │
│   │   │   └─────────────────────────┘    └───────────────────────┘   │   │   │
│   │   │                                                              │   │   │
│   │   │   Your main() function runs here                             │   │   │
│   │   └──────────────────────────────────────────────────────────────┘   │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                         │
│                                    │ Distributes tasks                       │
│                                    ▼                                         │
│   ┌──────────────────────┐  ┌──────────────────────┐  ┌──────────────────┐  │
│   │     WORKER NODE 1    │  │     WORKER NODE 2    │  │   WORKER NODE N  │  │
│   │  ┌────────────────┐  │  │  ┌────────────────┐  │  │ ┌──────────────┐ │  │
│   │  │   EXECUTOR     │  │  │  │   EXECUTOR     │  │  │ │   EXECUTOR   │ │  │
│   │  │                │  │  │  │                │  │  │ │              │ │  │
│   │  │ ┌────┐ ┌────┐  │  │  │  │ ┌────┐ ┌────┐  │  │  │ │ ┌────┐┌────┐│ │  │
│   │  │ │Task│ │Task│  │  │  │  │ │Task│ │Task│  │  │  │ │ │Task││Task││ │  │
│   │  │ └────┘ └────┘  │  │  │  │ └────┘ └────┘  │  │  │ │ └────┘└────┘│ │  │
│   │  │ ┌────┐ ┌────┐  │  │  │  │ ┌────┐ ┌────┐  │  │  │ │ ┌────┐┌────┐│ │  │
│   │  │ │Task│ │Task│  │  │  │  │ │Task│ │Task│  │  │  │ │ │Task││Task││ │  │
│   │  │ └────┘ └────┘  │  │  │  │ └────┘ └────┘  │  │  │ │ └────┘└────┘│ │  │
│   │  │                │  │  │  │                │  │  │ │              │ │  │
│   │  │  [Cache/Data]  │  │  │  │  [Cache/Data]  │  │  │ │ [Cache/Data] │ │  │
│   │  └────────────────┘  │  │  └────────────────┘  │  │ └──────────────┘ │  │
│   └──────────────────────┘  └──────────────────────┘  └──────────────────┘  │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 6.2 Component Definitions

#### Driver Program
- **What it is**: Your main application that runs the Spark logic
- **Where it runs**: On a single node (driver node)
- **Contains**:
  - The `main()` function
  - SparkContext creation
  - RDD definitions and transformations
  - Actions that trigger computation
- **Memory**: Has its own local memory (cannot be too large)

#### SparkContext
- **What it is**: The entry point to Spark functionality
- **What it does**:
  - Represents connection to the Spark cluster
  - Coordinates the execution of tasks
  - Provides methods to create RDDs
- **Creation**: One per Spark application

#### Worker Node
- **What it is**: A physical machine in the cluster
- **What it does**:
  - Hosts one or more executors
  - Provides CPU, memory, and storage resources
- **Analogy**: A server in your datacenter

#### Executor
- **What it is**: A JVM process running on a worker node
- **What it does**:
  - Runs tasks assigned by the driver
  - Stores RDD partitions in memory or disk
  - Returns results to the driver
- **Properties**:
  - **Exclusive to one application**: Each app gets its own executors
  - **Long-running**: Lives for the entire duration of the application
  - **Has its own memory**: Configurable memory allocation

#### Task
- **What it is**: The smallest unit of work
- **What it does**: Executes a single operation on a single partition
- **Properties**:
  - Runs as a **thread** within an executor
  - One task per partition per stage
  - Can run in parallel across multiple executors

### 6.3 Concrete Example: 4-Node Cluster

Let's say we have a cluster with the following specification:

```
Cluster Configuration:
├── Driver Node (1 machine)
│   └── 8 cores, 32 GB RAM
│
└── Worker Nodes (3 machines, each)
    └── 16 cores, 64 GB RAM each
```

**Spark Configuration:**
```python
# Typical configuration for this cluster
spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.executor.instances", 6) \      # 6 executors total
    .config("spark.executor.cores", 8) \          # 8 cores per executor
    .config("spark.executor.memory", "28g") \     # 28 GB per executor
    .config("spark.driver.memory", "8g") \        # 8 GB for driver
    .getOrCreate()
```

**What this gives us:**

```
┌─────────────────────────────────────────────────────────────────────────┐
│                              ACTUAL DEPLOYMENT                           │
│                                                                          │
│  Driver Node (8 cores, 32 GB RAM)                                       │
│  ┌────────────────────────────────────────────────────────────────────┐ │
│  │ Driver Program (8 GB)                                               │ │
│  │ - SparkContext                                                      │ │
│  │ - Your code runs here                                               │ │
│  │ - Collect() results come here                                       │ │
│  └────────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  Worker 1 (16 cores, 64 GB)    Worker 2 (16 cores, 64 GB)               │
│  ┌─────────────────────────┐   ┌─────────────────────────┐              │
│  │ Executor 1 (8c, 28GB)   │   │ Executor 3 (8c, 28GB)   │              │
│  │ ┌─────┐┌─────┐┌─────┐   │   │ ┌─────┐┌─────┐┌─────┐   │              │
│  │ │Task1││Task2││Task3│...│   │ │Task ││Task ││Task │...│              │
│  │ └─────┘└─────┘└─────┘   │   │ └─────┘└─────┘└─────┘   │              │
│  ├─────────────────────────┤   ├─────────────────────────┤              │
│  │ Executor 2 (8c, 28GB)   │   │ Executor 4 (8c, 28GB)   │              │
│  │ ┌─────┐┌─────┐┌─────┐   │   │ ┌─────┐┌─────┐┌─────┐   │              │
│  │ │Task ││Task ││Task │...│   │ │Task ││Task ││Task │...│              │
│  │ └─────┘└─────┘└─────┘   │   │ └─────┘└─────┘└─────┘   │              │
│  └─────────────────────────┘   └─────────────────────────┘              │
│                                                                          │
│  Worker 3 (16 cores, 64 GB)                                             │
│  ┌─────────────────────────┐                                            │
│  │ Executor 5 (8c, 28GB)   │                                            │
│  ├─────────────────────────┤   Maximum parallel tasks:                  │
│  │ Executor 6 (8c, 28GB)   │   6 executors × 8 cores = 48 tasks        │
│  └─────────────────────────┘                                            │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

**Summary:**
- **48 tasks** can run in parallel (6 executors × 8 cores)
- **168 GB** total executor memory (6 × 28 GB)
- **8 GB** driver memory for collecting results

---

## 7. Spark RDD (Resilient Distributed Dataset)

### 7.1 What is an RDD?

An **RDD** is Spark's fundamental data abstraction. It represents an **immutable, distributed collection of objects** that can be processed in parallel.

```
                                    RDD<Integer>
                                    ┌──────────┐
                                    │ 8975698  │
    Logical View                    │ 754843   │
    (As seen by Driver)             │ 866347   │
                                    │ 873876   │
                                    │ 45641    │
                                    │ 32764    │
                                    │ 23768423 │
                                    │ 364732   │
                                    │ 7586     │
                                    └──────────┘
                                          │
                                          │
          ┌───────────────────────────────┼───────────────────────────────┐
          │                               │                               │
          ▼                               ▼                               ▼
    ┌───────────┐                   ┌───────────┐                   ┌───────────┐
    │ Partition │                   │ Partition │                   │ Partition │
    │    P1     │                   │    P2     │                   │    P3     │
    │ ───────── │                   │ ───────── │                   │ ───────── │
    │  8975698  │                   │  873876   │                   │  23768423 │
    │  754843   │                   │  45641    │                   │  364732   │
    │  866347   │                   │  32764    │                   │  7586     │
    └───────────┘                   └───────────┘                   └───────────┘
          │                               │                               │
          ▼                               ▼                               ▼
      Worker A                        Worker B                        Worker C
    
    Physical Layout (Distributed across cluster)
```

### 7.2 Key Properties of RDDs

| Property | Meaning | Why It Matters |
|----------|---------|----------------|
| **Resilient** | Can be rebuilt if a partition is lost | Fault tolerance without replication |
| **Distributed** | Partitioned across multiple nodes | Parallel processing |
| **Dataset** | Collection of data elements | Work with data naturally |
| **Immutable** | Cannot be modified after creation | Enables lineage tracking |
| **Lazy** | Computed only when needed | Optimization opportunities |

### 7.3 RDD Characteristics

1. **Collection of homogeneous objects**
   - All elements have the same type
   - Can be any Python/Java/Scala object

2. **Distributed on workers**
   - Split into 1 or more partitions
   - Partitions stored on different machines

3. **Read-only & Immutable**
   - Cannot modify an existing RDD
   - Transformations create new RDDs

4. **Can be rebuilt**
   - Spark tracks how RDD was created (lineage)
   - Can recompute lost partitions

5. **Can be cached**
   - Keep in memory for reuse
   - Avoid recomputation

6. **MapReduce-like operations**
   - Parallel operations execute on workers
   - Driver coordinates execution

---

## 8. Creating and Operating on RDDs (PySpark)

### 8.1 Creating a SparkContext

In PySpark, you first need to create a SparkContext (or use SparkSession which includes it):

```python
# Method 1: Using SparkContext directly
from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local[*]").setAppName("My App")
sc = SparkContext(conf=conf)

# Method 2: Using SparkSession (Modern approach, Spark 2.0+)
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .master("local[*]") \
    .appName("My App") \
    .getOrCreate()

sc = spark.sparkContext  # Get SparkContext from SparkSession
```

**Master URL Options:**
| Master URL | Meaning |
|------------|---------|
| `local` | Run locally with 1 thread |
| `local[4]` | Run locally with 4 threads |
| `local[*]` | Run locally with all available cores |
| `spark://host:7077` | Connect to Spark cluster |
| `yarn` | Run on YARN cluster |

### 8.2 Creating RDDs

#### Method 1: Parallelize a Collection
```python
# Create RDD from a Python list
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# With explicit partitioning
rdd = sc.parallelize(data, numSlices=4)  # 4 partitions
```

**When to use**: Testing, prototyping, small datasets

#### Method 2: Load from External Storage
```python
# From a text file (local or HDFS)
lines = sc.textFile("README.md")

# From HDFS
lines = sc.textFile("hdfs://namenode:9000/user/data/file.txt")

# From multiple files
lines = sc.textFile("logs/*.txt")

# Whole text files (useful for small files)
files = sc.wholeTextFiles("data/")  # Returns (filename, content) pairs
```

**When to use**: Real-world data processing

### 8.3 Basic RDD Information

```python
# Number of partitions
rdd.getNumPartitions()

# First element
rdd.first()

# First n elements
rdd.take(5)

# Collect all elements to driver (be careful with large RDDs!)
rdd.collect()

# Count elements
rdd.count()
```

---

## 9. Passing Functions to Spark (PySpark)

Spark operations take functions as parameters. In Python, there are several ways to pass functions:

### 9.1 Lambda Functions (Inline)

Best for short, simple operations:

```python
# Filter lines containing "error"
errors = lines.filter(lambda line: "error" in line)

# Square each number
squared = nums.map(lambda x: x * x)

# Sum two numbers (for reduce)
total = nums.reduce(lambda x, y: x + y)
```

### 9.2 Named Functions

Better for complex logic or reuse:

```python
def contains_error(line):
    """Check if line contains error."""
    return "error" in line.lower()

def parse_log(line):
    """Parse a log line into components."""
    parts = line.split(",")
    return {
        "timestamp": parts[0],
        "level": parts[1],
        "message": parts[2]
    }

# Use named functions
errors = lines.filter(contains_error)
parsed = lines.map(parse_log)
```

### 9.3 Important Caution: Serialization

When passing functions, Spark serializes (pickles) them to send to executors.

**Problem: Referencing class members**

```python
# DON'T DO THIS - serializes entire object!
class LogProcessor:
    def __init__(self, keyword):
        self.keyword = keyword
    
    def process(self, rdd):
        # This references self.keyword, so entire object is serialized!
        return rdd.filter(lambda line: self.keyword in line)
```

**Solution: Extract to local variable**

```python
# DO THIS - only serializes the string
class LogProcessor:
    def __init__(self, keyword):
        self.keyword = keyword
    
    def process(self, rdd):
        # Extract to local variable first
        keyword = self.keyword
        return rdd.filter(lambda line: keyword in line)
```

---

## 10. Programming with RDDs: Transformations and Actions

### 10.1 Two Types of Operations

```
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  TRANSFORMATIONS                    ACTIONS                              │
│  ─────────────────                  ───────                              │
│  • Create new RDD                   • Return result to driver            │
│  • Lazy (not executed immediately)  • Trigger computation                │
│  • Return: RDD                      • Return: Value or write to storage │
│                                                                          │
│  Examples:                          Examples:                            │
│  - map()                            - count()                            │
│  - filter()                         - collect()                          │
│  - flatMap()                        - first()                            │
│  - union()                          - take(n)                            │
│  - distinct()                       - reduce()                           │
│  - groupByKey()                     - saveAsTextFile()                   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 10.2 Common Transformations

#### map(func)
Apply function to each element, return new RDD:

```python
nums = sc.parallelize([1, 2, 3, 4])
squared = nums.map(lambda x: x * x)
# Result: [1, 4, 9, 16]
```

#### filter(func)
Keep elements that satisfy predicate:

```python
nums = sc.parallelize([1, 2, 3, 4, 5, 6])
evens = nums.filter(lambda x: x % 2 == 0)
# Result: [2, 4, 6]
```

#### flatMap(func)
Map then flatten (one-to-many):

```python
lines = sc.parallelize(["hello world", "hi there"])
words = lines.flatMap(lambda line: line.split(" "))
# Result: ["hello", "world", "hi", "there"]
```

```
                map() vs flatMap()
                
Input RDD:  ["hello world", "hi"]

map(split):     [["hello", "world"], ["hi"]]  ← Nested lists
flatMap(split): ["hello", "world", "hi"]      ← Flattened
```

#### distinct()
Remove duplicates (expensive - requires shuffle):

```python
nums = sc.parallelize([1, 2, 2, 3, 3, 3])
unique = nums.distinct()
# Result: [1, 2, 3]
```

#### union(otherRDD)
Combine two RDDs:

```python
rdd1 = sc.parallelize([1, 2, 3])
rdd2 = sc.parallelize([3, 4, 5])
combined = rdd1.union(rdd2)
# Result: [1, 2, 3, 3, 4, 5]  (duplicates preserved!)
```

### 10.3 Common Actions

#### collect()
Return all elements as a list (use carefully!):

```python
result = rdd.collect()  # Returns Python list
```

⚠️ **Warning**: Only use when result fits in driver memory!

#### count()
Count number of elements:

```python
num_lines = lines.count()
```

#### first()
Return first element:

```python
first_line = lines.first()
```

#### take(n)
Return first n elements:

```python
top_5 = lines.take(5)
```

#### reduce(func)
Aggregate elements using associative function:

```python
nums = sc.parallelize([1, 2, 3, 4, 5])
total = nums.reduce(lambda x, y: x + y)
# Result: 15
```

#### saveAsTextFile(path)
Write to text file:

```python
rdd.saveAsTextFile("hdfs://path/to/output")
```

### 10.4 Transformation Summary Table

| Function | Purpose | Example | Result |
|----------|---------|---------|--------|
| `map(f)` | Apply f to each element | `rdd.map(x => x+1)` | {2,3,4,4} |
| `filter(f)` | Keep elements where f is true | `rdd.filter(x => x!=1)` | {2,3,3} |
| `flatMap(f)` | Map then flatten | `rdd.flatMap(x => x.to(3))` | {1,2,3,2,3,3,3} |
| `distinct()` | Remove duplicates | `rdd.distinct()` | {1,2,3} |
| `union(other)` | Combine RDDs | `rdd.union(other)` | {1,2,3,3,4,5} |
| `intersection(other)` | Common elements | `rdd.intersection(other)` | {3} |
| `subtract(other)` | Remove elements in other | `rdd.subtract(other)` | {1,2} |

---

## 11. Lazy Evaluation

### 11.1 What is Lazy Evaluation?

**Lazy evaluation** means that Spark does not execute transformations immediately. Instead, it records them as a **computation graph** and only executes when an **action** is called.

```
┌─────────────────────────────────────────────────────────────────────────┐
│                          LAZY EVALUATION                                 │
│                                                                          │
│   When you write:                                                        │
│   ─────────────────                                                      │
│   lines = sc.textFile("data.txt")   # Nothing happens yet!              │
│   errors = lines.filter(...)         # Still nothing!                   │
│   count = errors.count()             # NOW everything executes          │
│                     ▲                                                    │
│                     │                                                    │
│              Action triggers execution                                   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 11.2 Why Lazy Evaluation?

1. **Optimization Opportunity**: Spark can analyze entire computation graph before executing
2. **Reduce Passes**: Combine operations to minimize data reads
3. **Avoid Unnecessary Work**: Skip computation if result not needed

### 11.3 Detailed Example

Let's trace through what happens step by step:

```python
# Step 1: Load data
lines = sc.textFile("README.md")
# Spark does NOT read the file yet!
# It just records: "When needed, read from README.md"

# Step 2: Filter
pythonLines = lines.filter(lambda line: "Python" in line)
# Spark does NOT filter yet!
# It just records: "After reading, apply this filter"

# Step 3: Count (ACTION!)
count = pythonLines.count()
# NOW Spark:
#   1. Reads the file
#   2. Applies the filter
#   3. Counts the results
#   4. Returns the count to driver
```

**Visualization:**

```
          Transformations (Lazy)                    Action (Triggers)
          ─────────────────────                    ─────────────────
          
sc.textFile("README.md") ──▶ filter(contains Python) ──▶ count()
         │                          │                        │
         │                          │                        │
         ▼                          ▼                        ▼
    "Plan to read"          "Plan to filter"        "Execute everything!"
                                                            │
                                                            ▼
                                                    Return: 2
```

### 11.4 Benefits Demonstrated

**Without Lazy Evaluation (Hypothetical):**
```python
lines = sc.textFile("README.md")     # Read entire file into memory
filtered = lines.filter(...)          # Create new collection
first = filtered.first()              # Only need 1 element, but processed all!
```

**With Lazy Evaluation (Actual Spark):**
```python
lines = sc.textFile("README.md")     # Plan to read
filtered = lines.filter(...)          # Plan to filter
first = filtered.first()              # Execute - stop after finding first match!
```

Spark reads the file **only until it finds the first matching line**!

---

## 12. Lineage Graph

### 12.1 What is a Lineage Graph?

A **Lineage Graph** (also called **RDD Dependency Graph**) is a record of all the transformations used to build an RDD. Spark maintains this graph internally.

```
┌─────────────────────────────────────────────────────────────────────────┐
│                          LINEAGE GRAPH                                   │
│                                                                          │
│   Purpose:                                                               │
│   1. Enable lazy evaluation - know what to compute when action called   │
│   2. Enable fault tolerance - recompute lost partitions                  │
│   3. Enable optimization - combine operations, prune unnecessary work   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 12.2 Example Lineage Graph

```python
# Code
inputRDD = sc.textFile("log.txt")
errorsRDD = inputRDD.filter(lambda x: "error" in x)
warningsRDD = inputRDD.filter(lambda x: "warning" in x)
badLinesRDD = errorsRDD.union(warningsRDD)
count = badLinesRDD.count()
```

**Lineage Graph:**

```
                        sc.textFile("log.txt")
                               inputRDD
                                  │
                    ┌─────────────┴─────────────┐
                    │                           │
                    ▼                           ▼
           filter("error")             filter("warning")
              errorsRDD                  warningsRDD
                    │                           │
                    └─────────────┬─────────────┘
                                  │
                                  ▼
                              union()
                            badLinesRDD
                                  │
                                  ▼
                              count()
                               Result
```

### 12.3 Lineage for Fault Tolerance

If a partition is lost (machine failure), Spark uses the lineage to recompute just that partition:

```
         Partition P1                    P1 on Machine A fails!
         ────────────                    ─────────────────────────
         
         textFile ──▶ filter ──▶ P1     Machine A crashes, P1 lost
                                        
                                        Spark checks lineage:
                                        "P1 came from filtering a portion
                                         of the input file"
                                        
                                        Recompute on Machine B:
         textFile ──▶ filter ──▶ P1'    Read that portion, apply filter
```

**Key Insight**: Only the lost partition is recomputed, not the entire RDD!

### 12.4 Viewing Lineage

In PySpark, you can view the lineage using `toDebugString()`:

```python
>>> lines = sc.textFile("README.md")
>>> pythonLines = lines.filter(lambda x: "Python" in x)
>>> print(pythonLines.toDebugString())

(2) PythonRDD[2] at RDD at PythonRDD.scala:53 []
 |  README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
 |  README.md HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []
```

This shows:
- `HadoopRDD[0]`: Reading from file system
- `MapPartitionsRDD[1]`: Converting to strings
- `PythonRDD[2]`: Applying the Python filter function

---

## Summary

### Key Takeaways from Lecture 2.1 Part 2:

1. **Latency vs Bandwidth**:
   - Latency = time to start (waiting time)
   - Bandwidth = rate of transfer (throughput)
   - Memory >> Network >> Disk in both metrics

2. **Design for Performance**:
   - Failures are rare during job execution
   - Optimize for common case (no failures)
   - Use lineage for fault recovery when needed

3. **Spark Architecture**:
   - Driver: Runs your main program
   - SparkContext: Connection to cluster
   - Workers: Machines that do the work
   - Executors: JVM processes that run tasks
   - Tasks: Smallest unit of work (threads)

4. **RDD Fundamentals**:
   - Immutable, distributed collections
   - Partitioned across cluster
   - Two operations: Transformations (lazy) and Actions (eager)

5. **Lazy Evaluation**:
   - Transformations are recorded, not executed
   - Actions trigger execution
   - Enables optimization and fault tolerance

6. **Lineage Graph**:
   - Records how RDDs were derived
   - Enables recomputation of lost partitions
   - Foundation of Spark's fault tolerance

---

## References

1. Zaharia, M., et al., "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing", USENIX NSDI, 2012
2. Karau, H., Konwinski, A., Wendell, P., Zaharia, M., "Learning Spark", O'Reilly, 2015 (Chapters 2 & 3)
3. Latency Numbers Every Programmer Should Know: https://gist.github.com/jboner/2841832
4. Spark Programming Guide: https://spark.apache.org/docs/latest/rdd-programming-guide.html
