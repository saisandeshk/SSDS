# Lecture 2.2: Spark Transformations and Actions

## DS256 - Scalable Systems for Data Science
### Module 2: Processing Large Volumes of Big Data

---

## 1. Common Transformations: Element-wise

In Spark, **transformations** are operations that create a new RDD from an existing one. Element-wise transformations apply a function to each element of the RDD independently. Let's explore the three fundamental element-wise transformations.

### 1.1 Filter

**filter(func)** applies conditional logic to each element and returns a new RDD containing only elements where the function returns `true`.

```
┌─────────────────────────────────────────────────────────────────────┐
│                          FILTER TRANSFORMATION                       │
│                                                                      │
│   Input RDD: [1, 2, 3, 4, 5, 6]                                     │
│                                                                      │
│   Transformation: filter(lambda x: x > 3)                           │
│                                                                      │
│   Process:                                                           │
│   1 → false (discarded)                                             │
│   2 → false (discarded)                                             │
│   3 → false (discarded)                                             │
│   4 → true  (kept)                                                  │
│   5 → true  (kept)                                                  │
│   6 → true  (kept)                                                  │
│                                                                      │
│   Output RDD: [4, 5, 6]                                             │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**Key Characteristics:**
- User logic (lambda function) returns **true** or **false**
- If true, input element **copies to output** RDD
- If false, input element is **omitted** from output
- **Output RDD type is the same as input RDD type**

**Example:**

```python
# Create RDD
nums = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# Filter even numbers
evens = nums.filter(lambda x: x % 2 == 0)
# Result: [2, 4, 6, 8, 10]

# Filter numbers greater than 5
large_nums = nums.filter(lambda x: x > 5)
# Result: [6, 7, 8, 9, 10]

# Filter with string RDD
lines = sc.textFile("log.txt")
errors = lines.filter(lambda line: "ERROR" in line)
# Output: Only lines containing "ERROR"
```

### 1.2 Map

**map(func)** applies user logic to each element and returns exactly **one output for each input** item. The output type can be different from the input type.

```
┌─────────────────────────────────────────────────────────────────────┐
│                            MAP TRANSFORMATION                        │
│                                                                      │
│   Input RDD: [1, 2, 3, 4]                                           │
│                                                                      │
│   Transformation: map(lambda x: x * x)                              │
│                                                                      │
│   Process:                                                           │
│   1 → 1                                                             │
│   2 → 4                                                             │
│   3 → 9                                                             │
│   4 → 16                                                            │
│                                                                      │
│   Output RDD: [1, 4, 9, 16]                                         │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**Key Characteristics:**
- Applies user logic to **each element**
- Returns **exactly one output per input**
- **Output type can differ** from input type
- Can perform any user operation (parsing, computation, web fetching, etc.)

**Examples:**

```python
# Example 1: Simple arithmetic transformation
nums = sc.parallelize([1, 2, 3, 4])
squared = nums.map(lambda x: x * x)
# Result: [1, 4, 9, 16]

# Example 2: Type transformation (int → string)
nums = sc.parallelize([1, 2, 3, 4])
strings = nums.map(lambda x: "Number: " + str(x))
# Result: ["Number: 1", "Number: 2", "Number: 3", "Number: 4"]

# Example 3: Parsing strings
logs = sc.parallelize([
    "2024-01-01,ERROR,Connection failed",
    "2024-01-01,INFO,Server started",
    "2024-01-02,WARN,High memory usage"
])

parsed = logs.map(lambda line: {
    'date': line.split(',')[0],
    'level': line.split(',')[1],
    'message': line.split(',')[2]
})
# Result: List of dictionaries with structured data
```

### 1.3 FlatMap

**flatMap(func)** applies user logic to each element and returns **zero or more output items** for each input. The results are then flattened into a single RDD.

```
┌─────────────────────────────────────────────────────────────────────┐
│                         FLATMAP TRANSFORMATION                       │
│                                                                      │
│   Input RDD: ["hello world", "hi there"]                            │
│                                                                      │
│   Transformation: flatMap(lambda line: line.split())                │
│                                                                      │
│   Process:                                                           │
│   "hello world" → ["hello", "world"]                                │
│   "hi there"    → ["hi", "there"]                                   │
│                                                                      │
│   Flatten: [["hello", "world"], ["hi", "there"]]                    │
│            ↓                                                         │
│            ["hello", "world", "hi", "there"]                        │
│                                                                      │
│   Output RDD: ["hello", "world", "hi", "there"]                     │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**Key Characteristics:**
- Applies user logic to **each element**
- Returns **zero or more outputs** per input
- Results are **automatically flattened**
- **Output type can differ** from input type

**Examples:**

```python
# Example 1: Split sentences into words
lines = sc.parallelize(["hello world", "hi there", "goodbye"])
words = lines.flatMap(lambda line: line.split())
# Result: ["hello", "world", "hi", "there", "goodbye"]

# Example 2: Generate range of numbers
nums = sc.parallelize([1, 2, 3])
expanded = nums.flatMap(lambda x: range(1, x + 1))
# Input:  [1, 2, 3]
# 1 → [1]
# 2 → [1, 2]
# 3 → [1, 2, 3]
# Result: [1, 1, 2, 1, 2, 3]

# Example 3: Zero outputs (filtering effect)
nums = sc.parallelize([1, 2, 3, 4, 5])
result = nums.flatMap(lambda x: [x] if x > 2 else [])
# 1 → []
# 2 → []
# 3 → [3]
# 4 → [4]
# 5 → [5]
# Result: [3, 4, 5]
```

### 1.4 Map vs FlatMap: The Key Difference

```
                   map() vs flatMap()

Input RDD:  ["hello world", "hi"]

Using map(split):
  "hello world" → ["hello", "world"]
  "hi"          → ["hi"]

  Result: [["hello", "world"], ["hi"]]  ← Nested lists (RDD of lists)

Using flatMap(split):
  "hello world" → ["hello", "world"]
  "hi"          → ["hi"]

  Flatten: [["hello", "world"], ["hi"]]
           ↓
           ["hello", "world", "hi"]  ← Flattened (RDD of strings)
```

### 1.5 Filter Using Different Transformations

Let's see how to achieve filtering using different transformations:

**Goal:** Filter items greater than 10 from `[5, 15, 8, 12, 3, 20]`

```python
# Original RDD
rdd = sc.parallelize([5, 15, 8, 12, 3, 20])

# Method 1: Using filter() - THE CORRECT WAY
filtered = rdd.filter(lambda item: item > 10)
# Result: [15, 12, 20] ✓ Clean and correct

# Method 2: Using flatMap() - WORKS BUT AWKWARD
filtered = rdd.flatMap(lambda item: [item] if item > 10 else [])
# Process:
# 5  → [] (empty list flattened away)
# 15 → [15]
# 8  → []
# 12 → [12]
# 3  → []
# 20 → [20]
# Result: [15, 12, 20] ✓ Works, but unnecessary complexity

# Method 3: Using map() - WRONG! DON'T DO THIS
filtered = rdd.map(lambda item: item if item > 10 else None)
# Process:
# 5  → None
# 15 → 15
# 8  → None
# 12 → 12
# 3  → None
# 20 → 20
# Result: [None, 15, None, 12, None, 20] ✗ CONTAINS None VALUES!
```

**Why map() fails for filtering:**
- map() **must return exactly one item** for each input
- You can't "skip" an item in map()
- Returning `None` still creates an output element
- Result contains `None` values mixed with actual data

**Comparison Table:**

| Method | Result | Pros | Cons |
|--------|--------|------|------|
| **filter()** | `[15, 12, 20]` | Clean, clear intent, efficient | None |
| **flatMap()** | `[15, 12, 20]` | Works correctly | Unnecessarily complex for filtering |
| **map()** | `[None, 15, None, 12, None, 20]` | N/A | WRONG - includes None values! |

**Best Practice:** Always use `filter()` for filtering. Use `flatMap()` when you genuinely need zero-to-many transformations.

---

## 2. Common Transformations: Pseudo Set Operations

Spark provides set-like operations on RDDs. These are called "pseudo" set operations because RDDs can contain duplicates (unlike mathematical sets).

### 2.1 Distinct

**distinct()** removes duplicate elements from an RDD.

```python
nums = sc.parallelize([1, 2, 2, 3, 3, 3, 4, 5, 5])
unique = nums.distinct()
# Result: [1, 2, 3, 4, 5]
```

```
┌─────────────────────────────────────────────────────────────────────┐
│                         DISTINCT OPERATION                           │
│                                                                      │
│   Input RDD:  [1, 2, 2, 3, 3, 3, 4, 5, 5]                           │
│                                                                      │
│   Process:                                                           │
│   1. Hash each element by value                                     │
│   2. Shuffle to group identical values                              │
│   3. Keep one copy of each unique value                             │
│                                                                      │
│   Output RDD: [1, 2, 3, 4, 5]                                       │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**⚠️ Important:**
- `distinct()` is **expensive** - it requires a **shuffle operation**
- All data must be sent across the network to check for duplicates
- Use sparingly on large datasets

### 2.2 Union

**union(otherRDD)** concatenates two RDDs into one. **Duplicates are NOT removed**.

```python
rdd1 = sc.parallelize([1, 2, 3])
rdd2 = sc.parallelize([3, 4, 5])
combined = rdd1.union(rdd2)
# Result: [1, 2, 3, 3, 4, 5]  ← Note: 3 appears twice!
```

```
┌─────────────────────────────────────────────────────────────────────┐
│                          UNION OPERATION                             │
│                                                                      │
│   RDD1: [1, 2, 3]                                                   │
│   RDD2: [3, 4, 5]                                                   │
│                                                                      │
│   union(RDD1, RDD2):                                                │
│   Simply concatenates: [1, 2, 3] + [3, 4, 5]                        │
│                                                                      │
│   Result: [1, 2, 3, 3, 4, 5]  ← Duplicates preserved!               │
│                                                                      │
│   To get unique values:                                              │
│   rdd1.union(rdd2).distinct() → [1, 2, 3, 4, 5]                     │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**Key Points:**
- Very **cheap operation** (no shuffle needed)
- Just concatenates partitions from both RDDs
- Duplicates are **preserved**
- To remove duplicates, chain with `.distinct()`

### 2.3 Intersection

**intersection(otherRDD)** returns only elements that appear in **both** RDDs. Duplicates are removed.

```python
rdd1 = sc.parallelize([1, 2, 3, 3, 4])
rdd2 = sc.parallelize([3, 4, 5, 6])
common = rdd1.intersection(rdd2)
# Result: [3, 4]  ← Common elements, duplicates removed
```

```
┌─────────────────────────────────────────────────────────────────────┐
│                      INTERSECTION OPERATION                          │
│                                                                      │
│   RDD1: [1, 2, 3, 3, 4]                                             │
│   RDD2: [3, 4, 5, 6]                                                │
│                                                                      │
│   Process:                                                           │
│   1. Find elements in both RDDs                                     │
│   2. Remove duplicates                                               │
│                                                                      │
│   Common elements: 3 (appears in both), 4 (appears in both)         │
│                                                                      │
│   Result: [3, 4]                                                    │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**⚠️ Important:**
- **Expensive operation** - requires shuffle
- Automatically removes duplicates
- Use only when necessary

### 2.4 Subtract

**subtract(otherRDD)** returns elements from the first RDD that are **not** in the second RDD.

```python
rdd1 = sc.parallelize([1, 2, 3, 3, 4])
rdd2 = sc.parallelize([3, 4, 5])
difference = rdd1.subtract(rdd2)
# Result: [1, 2]  ← Elements in rdd1 but not in rdd2
```

```
┌─────────────────────────────────────────────────────────────────────┐
│                        SUBTRACT OPERATION                            │
│                                                                      │
│   RDD1: [1, 2, 3, 3, 4]                                             │
│   RDD2: [3, 4, 5]                                                   │
│                                                                      │
│   Process: Keep elements from RDD1 that don't appear in RDD2        │
│                                                                      │
│   Check each element in RDD1:                                       │
│   1 → Not in RDD2 → Keep                                            │
│   2 → Not in RDD2 → Keep                                            │
│   3 → In RDD2 → Remove (both copies)                                │
│   4 → In RDD2 → Remove                                              │
│                                                                      │
│   Result: [1, 2]                                                    │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 2.5 Cartesian Product

**cartesian(otherRDD)** returns **all possible pairs** combining each element from the first RDD with each element from the second.

```python
rdd1 = sc.parallelize([1, 2])
rdd2 = sc.parallelize(['a', 'b', 'c'])
pairs = rdd1.cartesian(rdd2)
# Result: [(1,'a'), (1,'b'), (1,'c'), (2,'a'), (2,'b'), (2,'c')]
```

```
┌─────────────────────────────────────────────────────────────────────┐
│                    CARTESIAN PRODUCT OPERATION                       │
│                                                                      │
│   RDD1: [1, 2]                                                      │
│   RDD2: ['a', 'b', 'c']                                             │
│                                                                      │
│   All combinations:                                                  │
│   1 × 'a' → (1, 'a')                                                │
│   1 × 'b' → (1, 'b')                                                │
│   1 × 'c' → (1, 'c')                                                │
│   2 × 'a' → (2, 'a')                                                │
│   2 × 'b' → (2, 'b')                                                │
│   2 × 'c' → (2, 'c')                                                │
│                                                                      │
│   Result: [(1,'a'), (1,'b'), (1,'c'), (2,'a'), (2,'b'), (2,'c')]    │
│                                                                      │
│   Size: |RDD1| × |RDD2| = 2 × 3 = 6 elements                        │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**⚠️ Warning:**
- **Extremely expensive** for large RDDs!
- Output size = size(RDD1) × size(RDD2)
- Example: 1000 × 1000 = 1,000,000 elements
- Avoid unless absolutely necessary

### 2.6 Sample

**sample(withReplacement, fraction, seed)** returns a random sample of the RDD.

```python
nums = sc.parallelize(range(100))

# Sample approximately 10% without replacement
sample1 = nums.sample(False, 0.1, seed=42)
# Result: ~10 elements, no duplicates

# Sample approximately 50% with replacement
sample2 = nums.sample(True, 0.5, seed=42)
# Result: ~50 elements, may contain duplicates
```

**Parameters:**
- **withReplacement**:
  - `False`: Each element can appear at most once
  - `True`: Elements can appear multiple times
- **fraction**: Approximate fraction to sample (0.0 to 1.0)
- **seed**: Random seed for reproducibility

**Important Notes:**
- Sample size is **approximate**, not exact
  - `fraction * count` gives expected size, not guaranteed size
- Same `seed` produces same sample **only if RDD hasn't changed**
- Sampling done **per partition** with same fraction
- Useful for:
  - Testing on subset of data
  - Stratified sampling
  - Approximate data exploration

**Examples:**

```python
# Example: Varying sample sizes
data = sc.parallelize(range(1000))

# Small sample
small = data.sample(False, 0.01, 123)  # ~10 items

# Medium sample
medium = data.sample(False, 0.1, 123)  # ~100 items

# Large sample
large = data.sample(False, 0.5, 123)   # ~500 items

# With replacement (for bootstrap sampling)
bootstrap = data.sample(True, 1.0, 123)  # ~1000 items, some duplicated
```

### 2.7 Summary Table: Set Operations

| Operation | Duplicates? | Shuffle? | Output Size | Use Case |
|-----------|-------------|----------|-------------|----------|
| **distinct()** | Removed | Yes | ≤ input size | Remove duplicates |
| **union(rdd2)** | Preserved | No | size1 + size2 | Combine datasets |
| **intersection(rdd2)** | Removed | Yes | ≤ min(size1, size2) | Find common elements |
| **subtract(rdd2)** | Removed | Yes | ≤ size1 | Remove elements |
| **cartesian(rdd2)** | N/A | Yes | size1 × size2 | All combinations |
| **sample(...)** | Depends | No | ≈ fraction × size | Random sampling |

---

## 3. Common Actions: reduce()

Actions trigger computation and return results to the driver or save to storage. Let's start with one of the most important actions: **reduce()**.

### 3.1 What is reduce()?

**reduce(mergeFunc)** combines elements of an RDD using an **aggregation function** to produce a single result.

```python
nums = sc.parallelize([1, 2, 3, 4, 5])
product = nums.reduce(lambda x, y: x * y)
# Result: 120  (1 * 2 * 3 * 4 * 5)
```

### 3.2 Requirements for mergeFunc

The merge function **must be**:

1. **Commutative**: `f(a, b) = f(b, a)`
   - Order of operands doesn't matter

2. **Associative**: `f(f(a, b), c) = f(a, f(b, c))`
   - Order of operations doesn't matter

**Why these requirements?**
- Spark processes partitions **in parallel**
- Combines results in **arbitrary order**
- Function must produce same result regardless of order

**Examples:**

```python
# ✓ Valid functions (commutative and associative):
sum:     lambda x, y: x + y
product: lambda x, y: x * y
max:     lambda x, y: max(x, y)
min:     lambda x, y: min(x, y)

# ✗ Invalid functions:
subtract: lambda x, y: x - y  # NOT commutative: 5-3 ≠ 3-5
divide:   lambda x, y: x / y  # NOT commutative or associative
concat:   lambda x, y: x + y  # For strings, NOT commutative
```

### 3.3 How reduce() Works: Two-Level Reduction

reduce() performs aggregation in **two phases**:

1. **Within each partition** (parallel) - combine elements in each partition
2. **Across partitions** (parallel or sequential) - combine partition results

```
┌─────────────────────────────────────────────────────────────────────┐
│                    REDUCE EXECUTION FLOW                             │
│                                                                      │
│  Input: [2, 5, 3, 1, 6, 4, 7] distributed across 2 partitions      │
│  Function: lambda x, y: x * y  (multiplication)                     │
│                                                                      │
│  ┌──────────────────────┐              ┌──────────────────────┐    │
│  │    Partition 1       │              │    Partition 2       │    │
│  │   [2, 5, 3, 1]       │              │     [6, 4, 7]        │    │
│  └──────────────────────┘              └──────────────────────┘    │
│           │                                      │                  │
│           │ PHASE 1: Reduce within partition    │                  │
│           │         (runs in parallel)           │                  │
│           ▼                                      ▼                  │
│  ┌──────────────────────┐              ┌──────────────────────┐    │
│  │  2 * 5 = 10          │              │   6 * 4 = 24         │    │
│  │ 10 * 3 = 30          │              │  24 * 7 = 168        │    │
│  │ 30 * 1 = 30          │              │                      │    │
│  │                      │              │                      │    │
│  │ Partition result: 30 │              │ Partition result:168 │    │
│  └──────────────────────┘              └──────────────────────┘    │
│           │                                      │                  │
│           └──────────────┬───────────────────────┘                  │
│                          │                                          │
│           PHASE 2: Reduce across partitions                         │
│                          │                                          │
│                          ▼                                          │
│                  ┌──────────────┐                                   │
│                  │  30 * 168    │                                   │
│                  │              │                                   │
│                  │ Final: 5040  │                                   │
│                  └──────────────┘                                   │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 3.4 Detailed Example: Product Calculation

Let's trace through a product calculation step-by-step:

```python
nums = sc.parallelize([2, 5, 3, 1, 6, 4, 7], 2)  # 2 partitions
product = nums.reduce(lambda x, y: x * y)
```

**Distribution:**
- Partition 1: `[2, 5, 3, 1]`
- Partition 2: `[6, 4, 7]`

**Phase 1: Within Partitions (Parallel)**

**Partition 1:**
```
Step 1: 2 * 5 = 10
Step 2: 10 * 3 = 30
Step 3: 30 * 1 = 30
Partition 1 result: 30
```

**Partition 2:**
```
Step 1: 6 * 4 = 24
Step 2: 24 * 7 = 168
Partition 2 result: 168
```

**Phase 2: Across Partitions**
```
30 * 168 = 5040
Final result: 5040
```

### 3.5 More reduce() Examples

```python
# Example 1: Sum
nums = sc.parallelize([1, 2, 3, 4, 5])
total = nums.reduce(lambda x, y: x + y)
# Result: 15

# Example 2: Maximum
nums = sc.parallelize([23, 45, 12, 67, 34])
maximum = nums.reduce(lambda x, y: max(x, y))
# Result: 67

# Example 3: Minimum
nums = sc.parallelize([23, 45, 12, 67, 34])
minimum = nums.reduce(lambda x, y: min(x, y))
# Result: 12

# Example 4: String concatenation (BE CAREFUL!)
words = sc.parallelize(['hello', 'world', 'spark'])
# This might produce different results depending on partition order!
result = words.reduce(lambda x, y: x + ' ' + y)
# Possible results: "hello world spark" or "world hello spark" etc.
# String concatenation is NOT commutative for order-dependent results!
```

### 3.6 Common Pitfalls

**Pitfall 1: Non-commutative functions**

```python
# WRONG: Subtraction is not commutative
nums = sc.parallelize([10, 5, 3], 2)
# Partition 1: [10, 5] → 10 - 5 = 5
# Partition 2: [3] → 3
# Across: 5 - 3 = 2  OR  3 - 5 = -2  ← Unpredictable!
result = nums.reduce(lambda x, y: x - y)  # Don't do this!
```

**Pitfall 2: Functions with side effects**

```python
# WRONG: Reduce function should be pure
count = 0
def bad_reduce(x, y):
    global count
    count += 1  # Side effect!
    return x + y

result = nums.reduce(bad_reduce)  # Count will be unpredictable
```

**Best Practices:**
- ✓ Use functions that are commutative and associative
- ✓ Keep functions pure (no side effects)
- ✓ Test your reduce function with different orderings
- ✗ Don't use reduce for operations that depend on order
- ✗ Don't modify external state in reduce functions

---

## 4. Common Actions: aggregate()

While `reduce()` is powerful, it has a limitation: the output type must be the same as the input type. What if you want to compute multiple statistics at once, like both sum and count? That's where **aggregate()** comes in.

### 4.1 What is aggregate()?

**aggregate(zeroValue, mergeValue, mergeCombiners)** is a more general version of reduce() that allows the accumulator type to differ from the element type.

**Signature:**
```python
rdd.aggregate(
    zeroValue,      # Initial accumulator value
    mergeValue,     # Combine accumulator with each element within partition
    mergeCombiners  # Combine accumulators across partitions
)
```

### 4.2 Three Parameters Explained

1. **zeroValue**: The initial value for the accumulator
   - Used as starting point for each partition
   - Type can be different from RDD element type

2. **mergeValue(accumulator, element)**: Combines an element into the accumulator
   - Called for each element in a partition
   - Returns updated accumulator

3. **mergeCombiners(acc1, acc2)**: Combines two accumulators
   - Called to combine results from different partitions
   - Returns merged accumulator

### 4.3 How aggregate() Works

```
┌─────────────────────────────────────────────────────────────────────┐
│                    AGGREGATE EXECUTION FLOW                          │
│                                                                      │
│  Phase 1: Within Each Partition                                     │
│  ─────────────────────────────────                                  │
│                                                                      │
│  1. Start with zeroValue                                            │
│  2. For each element: acc = mergeValue(acc, element)                │
│  3. Result: One accumulator per partition                           │
│                                                                      │
│  Phase 2: Across Partitions                                         │
│  ─────────────────────────────                                      │
│                                                                      │
│  1. Start with zeroValue                                            │
│  2. For each partition result: acc = mergeCombiners(acc, part_acc)  │
│  3. Result: Single final accumulator                                │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 4.4 Example 1: Simple Sum (Same as reduce)

Let's start with a simple example where aggregate() does the same thing as reduce():

```python
nums = sc.parallelize([2, 5, 3, 1, 6, 4, 7], 2)  # 2 partitions

# Using aggregate for sum
total = nums.aggregate(
    0,                          # zeroValue: Start with 0
    lambda acc, val: acc + val, # mergeValue: Add value to accumulator
    lambda acc1, acc2: acc1 + acc2  # mergeCombiners: Add accumulators
)
# Result: 28
```

**Execution Trace:**

**Partition 1: [2, 5, 3, 1]**
```
Start:     acc = 0 (zeroValue)
Element 2: acc = 0 + 2 = 2
Element 5: acc = 2 + 5 = 7
Element 3: acc = 7 + 3 = 10
Element 1: acc = 10 + 1 = 11
Partition 1 result: 11
```

**Partition 2: [6, 4, 7]**
```
Start:     acc = 0 (zeroValue)
Element 6: acc = 0 + 6 = 6
Element 4: acc = 6 + 4 = 10
Element 7: acc = 10 + 7 = 17
Partition 2 result: 17
```

**Combine Partitions:**
```
Start:             acc = 0 (zeroValue)
Partition 1 (11):  acc = 0 + 11 = 11
Partition 2 (17):  acc = 11 + 17 = 28
Final result: 28
```

### 4.5 Example 2: Computing Average (Different Accumulator Type)

Here's where aggregate() shines - computing the average requires tracking both sum and count:

```python
nums = sc.parallelize([2, 5, 3, 1, 6, 4, 7], 2)  # 2 partitions

# Accumulator type: (sum, count) tuple
sumCount = nums.aggregate(
    (0, 0),  # zeroValue: (sum=0, count=0)

    # mergeValue: Add value to sum, increment count
    lambda acc, val: (acc[0] + val, acc[1] + 1),

    # mergeCombiners: Add sums and counts
    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])
)

# Calculate average
average = sumCount[0] / float(sumCount[1])
# Result: 28 / 7 = 4.0
```

**Detailed Execution Trace:**

```
┌─────────────────────────────────────────────────────────────────────┐
│              AGGREGATE WITH (SUM, COUNT) ACCUMULATOR                │
│                                                                      │
│  Partition 1: [2, 5, 3, 1]                                          │
│  ──────────────────────────                                         │
│                                                                      │
│  Initial:   (sum=0, count=0)                                        │
│  + value 2: (0+2, 0+1)   = (2, 1)                                   │
│  + value 5: (2+5, 1+1)   = (7, 2)                                   │
│  + value 3: (7+3, 2+1)   = (10, 3)                                  │
│  + value 1: (10+1, 3+1)  = (11, 4)                                  │
│                                                                      │
│  Partition 1 Result: (11, 4)  ← sum=11, count=4                     │
│                                                                      │
│  ─────────────────────────────────────────────────────────────      │
│                                                                      │
│  Partition 2: [6, 4, 7]                                             │
│  ──────────────────────                                             │
│                                                                      │
│  Initial:   (sum=0, count=0)                                        │
│  + value 6: (0+6, 0+1)   = (6, 1)                                   │
│  + value 4: (6+4, 1+1)   = (10, 2)                                  │
│  + value 7: (10+7, 2+1)  = (17, 3)                                  │
│                                                                      │
│  Partition 2 Result: (17, 3)  ← sum=17, count=3                     │
│                                                                      │
│  ─────────────────────────────────────────────────────────────      │
│                                                                      │
│  Combine Partitions:                                                │
│  ──────────────────                                                 │
│                                                                      │
│  Initial:        (sum=0, count=0)                                   │
│  + Part 1 (11,4): (0+11, 0+4)   = (11, 4)                           │
│  + Part 2 (17,3): (11+17, 4+3)  = (28, 7)                           │
│                                                                      │
│  Final Result: (28, 7)                                              │
│                                                                      │
│  Average: 28 / 7 = 4.0                                              │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 4.6 Visual: Incremental Evaluation

```
                    WITHIN PARTITION 1              WITHIN PARTITION 2

zeroValue:             (0, 0)                          (0, 0)
                         ↓                               ↓
mergeValue:           (0, 0)                          (0, 0)
+ element 2            + 2                            + 6
                     ────────                        ────────
                     (2, 1)                          (6, 1)
                         ↓                               ↓
                     (2, 1)                          (6, 1)
+ element 5/4          + 5                            + 4
                     ────────                        ────────
                     (7, 2)                          (10, 2)
                         ↓                               ↓
                     (7, 2)                          (10, 2)
+ element 3/7          + 3                            + 7
                     ────────                        ────────
                     (10, 3)                         (17, 3)
                         ↓                               ↓
                     (10, 3)                         (17, 3)
+ element 1            + 1                             [done]
                     ────────                        ────────
                     (11, 4)                         (17, 3)
                         │                               │
                         └───────────┬───────────────────┘
                                     │
                        ACROSS PARTITIONS (mergeCombiners)
                                     │
                                     ▼
                    zeroValue:    (0, 0)
                                     ↓
                    + Part 1:   (0+11, 0+4) = (11, 4)
                                     ↓
                    + Part 2:   (11+17, 4+3) = (28, 7)
                                     ↓
                             Final: (28, 7)
                                     ↓
                           Average: 28/7 = 4.0
```

### 4.7 Example 3: Complex Example from Slides

From the lecture slides, here's an example that counts string lengths:

```python
strs = sc.parallelize(['ababab', 'ab', 'abcd'])

# Count total length of all strings
totalLength = strs.aggregate(
    0,                          # zeroValue: start with 0
    lambda acc, val: acc + len(val),  # mergeValue: add string length
    lambda acc1, acc2: acc1 + acc2    # mergeCombiners: add lengths
)
# Result: 6 + 2 + 4 = 12
```

### 4.8 aggregate() vs reduce()

| Feature | reduce() | aggregate() |
|---------|----------|-------------|
| **Accumulator Type** | Same as element type | Can be different |
| **Use Case** | Simple aggregations | Complex aggregations |
| **Example** | Sum, max, min | Average, statistics, custom objects |
| **Parameters** | 1 function | 3 parameters (zeroValue + 2 functions) |
| **Flexibility** | Limited | High |

### 4.9 When to Use aggregate()

Use **aggregate()** when:
- ✓ You need to compute multiple values at once (e.g., sum AND count)
- ✓ Your accumulator type differs from element type
- ✓ You need more control over the aggregation process

Use **reduce()** when:
- ✓ Simple aggregation with same input/output type
- ✓ Function is commutative and associative
- ✓ You don't need intermediate accumulator state

### 4.10 Important Notes

**Note 1: zeroValue is used twice!**
```
Within partitions: Start with zeroValue
Across partitions: Start with zeroValue again!
```

This means zeroValue should be the identity element for your operation:
- For addition: 0
- For multiplication: 1
- For tuple (sum, count): (0, 0)

**Note 2: Functions must be associative**

Both `mergeValue` and `mergeCombiners` should be associative to ensure consistent results regardless of processing order.

---

## 5. Common Actions: Other Actions

Beyond reduce() and aggregate(), Spark provides many other useful actions. Let's explore them with a concrete example.

### 5.1 Example Dataset

We'll use this sample RDD distributed across 3 partitions:

```python
# Create RDD with 3 partitions
rdd = sc.parallelize([
    3, 2, 1,      # Partition 1
    4, 1, 6,      # Partition 2
    2, 5, 6, 7, 5 # Partition 3
], 3)

# Visual representation:
# P1: [3, 2, 1]
# P2: [4, 1, 6]
# P3: [2, 5, 6, 7, 5]
```

### 5.2 count()

Returns the total number of elements in the RDD.

```python
count = rdd.count()
# Result: 12
# Explanation: 3 + 3 + 5 = 12 elements total
```

### 5.3 collect()

Returns **all elements** of the RDD to the driver as a Python list.

```python
all_elements = rdd.collect()
# Result: [3, 2, 1, 4, 1, 6, 2, 5, 6, 7, 5]
```

**⚠️ WARNING:**
```
┌─────────────────────────────────────────────────────────────────────┐
│                       collect() WARNING!                             │
│                                                                      │
│  • collect() brings ALL data to the driver                          │
│  • If RDD is large, driver will run out of memory and crash!        │
│  • Only use when you KNOW the result is small                       │
│                                                                      │
│  Safe:    rdd.filter(...).take(10)        # Get small sample        │
│  UNSAFE:  huge_rdd.collect()              # May crash driver!       │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 5.4 take(n)

Returns the **first n elements** from the RDD, reading from the **fewest partitions possible**.

```python
first_8 = rdd.take(8)
# Result: [3, 2, 1, 4, 1, 6, 2, 5]
# Explanation: Takes from P1 (3 items), P2 (3 items), P3 (2 items)
```

**Characteristics:**
- **Not evenly sampled**: Takes from first partitions until n elements collected
- **Not ordered**: Order depends on partition order
- **Efficient**: Doesn't read entire RDD

```
┌──────────────────────────────────────────────────────────────┐
│                    take(8) Execution                          │
│                                                               │
│  P1: [3, 2, 1]       → Take all 3 (total: 3)                │
│  P2: [4, 1, 6]       → Take all 3 (total: 6)                │
│  P3: [2, 5, 6, 7, 5] → Take first 2 (total: 8) STOP!        │
│                                                               │
│  Result: [3, 2, 1, 4, 1, 6, 2, 5]                            │
│                                                               │
└──────────────────────────────────────────────────────────────┘
```

### 5.5 takeOrdered(n, key=None)

Returns the **first n elements in sorted order** (ascending by default).

```python
# Default: ascending order
first_4_sorted = rdd.takeOrdered(4)
# Result: [1, 1, 2, 2]
# Explanation: Sorts entire RDD, returns first 4

# With custom key (descending)
top_4 = rdd.takeOrdered(4, key=lambda x: -x)
# Result: [7, 6, 6, 5]
# Explanation: Negative key reverses order
```

**Execution:**
1. Spark collects data from all partitions
2. Sorts the data
3. Returns first n elements

### 5.6 top(n)

Returns the **largest n elements** in descending order.

```python
top_4 = rdd.top(4)
# Result: [7, 6, 6, 5]
# Explanation: Returns 4 largest values in descending order
```

**Relationship:**
```python
rdd.top(n) == rdd.takeOrdered(n, key=lambda x: -x)
# Both return largest n elements in descending order
```

### 5.7 takeSample(withReplacement, num, seed=None)

Returns a **random sample** of exactly `num` elements, **evenly sampled** from all partitions.

```python
# Without replacement (no duplicates)
sample_no_replace = rdd.takeSample(False, 6, seed=42)
# Result: [1, 5, 3, 1, 6, 5]
# Explanation: 6 items sampled uniformly from all partitions

# With replacement (duplicates allowed)
sample_with_replace = rdd.takeSample(True, 6, seed=42)
# Result: [1, 5, 2, 2, 6, 5]
# Explanation: Same item can be picked multiple times
```

**Key Differences from take():**

| Feature | take(n) | takeSample(n) |
|---------|---------|---------------|
| **Sampling** | From first partitions | From all partitions uniformly |
| **Deterministic** | Yes (always same) | No (random, unless seed provided) |
| **Count** | Approximately n | Exactly n |
| **Replacement** | N/A | Can choose with/without |

### 5.8 forEach(func)

Applies a function to each element **for side effects only**. Results are NOT returned to driver.

```python
# Example: Save to database (side effect)
def save_to_db(value):
    # Imagine this writes to a database
    print(f"Saving {value} to database")

rdd.forEach(save_to_db)
# No return value
# Each element processed on executors, not returned to driver
```

**Use Cases:**
- Writing to external database
- Updating external counters
- Logging (on executors)

**⚠️ Important:**
- `forEach` runs on **executors**, not driver
- Cannot return values to driver
- Good for distributed side effects

### 5.9 countByValue()

Returns a dictionary mapping each **unique value** to its **count**.

```python
counts = rdd.countByValue()
# Result: {1: 2, 2: 2, 3: 1, 4: 1, 5: 2, 6: 2, 7: 1}
# Explanation:
#   1 appears 2 times
#   2 appears 2 times
#   3 appears 1 time
#   ... and so on
```

**Execution:**
1. Groups elements by value
2. Counts occurrences
3. Returns dictionary to driver

**⚠️ Warning:** Result is brought to driver. If many unique values, may cause memory issues.

### 5.10 Summary Table: All Actions

Using our example RDD: `[3, 2, 1, 4, 1, 6, 2, 5, 6, 7, 5]`

| Action | Result | Notes |
|--------|--------|-------|
| `count()` | `12` | Total number of elements |
| `collect()` | `[3,2,1,4,1,6,2,5,6,7,5]` | ⚠️ All data to driver |
| `take(8)` | `[3,2,1,4,1,6,2,5]` | First partitions, not sorted |
| `takeOrdered(4)` | `[1,1,2,2]` | Smallest 4, ascending |
| `top(4)` | `[7,6,6,5]` | Largest 4, descending |
| `takeSample(False,6)` | `[1,5,3,1,6,5]` | Random uniform sample |
| `takeSample(True,6)` | `[1,5,2,2,6,5]` | With possible duplicates |
| `forEach(print)` | None | Side effects only |
| `countByValue()` | `{1:2, 2:2, 3:1, 4:1, 5:2, 6:2, 7:1}` | Frequency map |

---

## 6. RDD Persistence

One of Spark's key advantages over MapReduce is the ability to **cache RDDs in memory** for reuse across multiple operations. This is crucial for iterative algorithms and interactive queries.

### 6.1 Why Persistence is Needed

**Problem: Recomputation on Every Action**

```python
# Load data
logs = sc.textFile("hdfs://server/logs/*.txt")

# Transform
errors = logs.filter(lambda line: "ERROR" in line)
warnings = logs.filter(lambda line: "WARN" in line)

# Actions
error_count = errors.count()      # Reads and filters logs
warning_count = warnings.count()  # Reads and filters logs AGAIN!
```

**Without persistence:**
```
Action 1 (errors.count()):
  Read logs from HDFS → Filter for ERROR → Count

Action 2 (warnings.count()):
  Read logs from HDFS AGAIN → Filter for WARN → Count
```

**With persistence:**
```python
logs = sc.textFile("hdfs://server/logs/*.txt")
logs.persist()  # Mark for caching

errors = logs.filter(lambda line: "ERROR" in line)
warnings = logs.filter(lambda line: "WARN" in line)

error_count = errors.count()      # Reads logs, CACHES in memory
warning_count = warnings.count()  # REUSES cached logs!
```

```
Action 1 (errors.count()):
  Read logs from HDFS → Cache in memory → Filter for ERROR → Count

Action 2 (warnings.count()):
  Read logs from MEMORY (fast!) → Filter for WARN → Count
```

### 6.2 How Persistence Works

**Step 1: Mark RDD for persistence**
```python
rdd.persist()  # or rdd.cache()
```
- Does **nothing immediately** (lazy!)
- Just marks RDD for caching

**Step 2: First action triggers caching**
```python
count = rdd.count()
```
- RDD is computed
- Results stored in memory (or disk, depending on level)

**Step 3: Subsequent actions reuse cached data**
```python
sample = rdd.take(10)
```
- Reads from cache instead of recomputing
- Much faster!

```
┌─────────────────────────────────────────────────────────────────────┐
│                      PERSISTENCE LIFECYCLE                           │
│                                                                      │
│  rdd.persist()  →  NO IMMEDIATE EFFECT (lazy)                       │
│       │                                                              │
│       ▼                                                              │
│  rdd.count()    →  COMPUTE + CACHE                                  │
│       │             ├─ Read from source                              │
│       │             ├─ Apply transformations                         │
│       │             ├─ Store in memory/disk                          │
│       │             └─ Return count                                  │
│       ▼                                                              │
│  rdd.take(10)   →  READ FROM CACHE (fast!)                          │
│       │             └─ No recomputation needed                       │
│       ▼                                                              │
│  rdd.collect()  →  READ FROM CACHE (fast!)                          │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 6.3 Levels of Persistence

Spark offers multiple storage levels with different trade-offs:

| Storage Level | Memory | Disk | Serialized | Recompute | Use Case |
|---------------|--------|------|------------|-----------|----------|
| **MEMORY_ONLY** | ✓ | ✗ | ✗ | If evicted | Default, fastest |
| **MEMORY_ONLY_SER** | ✓ | ✗ | ✓ | If evicted | Save memory |
| **MEMORY_AND_DISK** | ✓ | ✓ | ✗ | Never | Spill to disk |
| **MEMORY_AND_DISK_SER** | ✓ | ✓ | ✓ | Never | Save memory + reliable |
| **DISK_ONLY** | ✗ | ✓ | ✓ | Never | When memory scarce |

**Detailed Explanations:**

**1. MEMORY_ONLY** (Default)
```python
rdd.persist()  # Same as cache()
# or
rdd.persist(StorageLevel.MEMORY_ONLY)
```
- Stores RDD as **deserialized Java objects** in memory
- **Fastest** access (no deserialization overhead)
- If not enough memory, partitions **not cached** (recomputed on demand)
- Best for: Hot data that fits in memory

**2. MEMORY_ONLY_SER**
```python
from pyspark import StorageLevel
rdd.persist(StorageLevel.MEMORY_ONLY_SER)
```
- Stores RDD as **serialized objects** (more compact)
- Saves memory at cost of CPU (deserialization overhead)
- Good for: Large RDDs that barely fit in memory

**3. MEMORY_AND_DISK**
```python
rdd.persist(StorageLevel.MEMORY_AND_DISK)
```
- Stores in memory if possible
- **Spills to disk** if memory insufficient
- Never recomputes (always cached somewhere)
- Best for: Important RDDs that may not fit in memory

**4. MEMORY_AND_DISK_SER**
```python
rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)
```
- Serialized in memory, spills to disk if needed
- Most memory-efficient reliable option
- Best for: Large important RDDs with limited memory

**5. DISK_ONLY**
```python
rdd.persist(StorageLevel.DISK_ONLY)
```
- Stores only on disk
- Slower than memory but faster than recomputation
- Best for: Very large RDDs with expensive computations

### 6.4 LRU Eviction Policy

When memory is full, Spark uses **Least Recently Used (LRU)** eviction:

```
┌─────────────────────────────────────────────────────────────────────┐
│                         LRU EVICTION                                 │
│                                                                      │
│  Memory Full?                                                        │
│      │                                                               │
│      ├─ NO  → Cache new partition                                   │
│      │                                                               │
│      └─ YES → Evict least recently used partition                   │
│              → Cache new partition                                   │
│                                                                      │
│  Evicted partition:                                                  │
│  ├─ MEMORY_ONLY → Recompute if needed later                         │
│  └─ MEMORY_AND_DISK → Already on disk, read from there              │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 6.5 Fault Tolerance and Persistence

**What happens if a node fails?**

```
┌─────────────────────────────────────────────────────────────────────┐
│                    FAULT TOLERANCE WITH CACHING                      │
│                                                                      │
│  Scenario: Node with cached partition fails                         │
│                                                                      │
│  Storage Level           Action Taken                                │
│  ──────────────          ────────────                                │
│                                                                      │
│  MEMORY_ONLY         →  Recompute lost partition using lineage      │
│  MEMORY_ONLY_SER     →  Recompute lost partition                    │
│  MEMORY_AND_DISK     →  Read from disk on another node              │
│                         (if partition was replicated)                │
│                         Otherwise recompute                          │
│  DISK_ONLY           →  Read from disk replica                      │
│                                                                      │
│  Key Insight: Lineage provides fault tolerance                      │
│               No need for expensive replication                      │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 6.6 Manual Unpersist

You can manually remove an RDD from cache:

```python
# Persist RDD
rdd.persist()

# Use RDD multiple times
count1 = rdd.count()
count2 = rdd.filter(...).count()

# Done with RDD, free memory
rdd.unpersist()
```

**When to unpersist:**
- RDD no longer needed
- Need memory for other RDDs
- Cache is cluttered with old data

### 6.7 cache() vs persist()

```python
# These are equivalent:
rdd.cache()
rdd.persist()
rdd.persist(StorageLevel.MEMORY_ONLY)

# cache() is just shorthand for persist with default level
```

### 6.8 Example: Iterative Algorithm

**Without caching (slow):**
```python
data = sc.textFile("large_file.txt")

for i in range(10):
    # Each iteration reads file from disk!
    count = data.filter(lambda x: some_condition(x, i)).count()
    print(f"Iteration {i}: {count}")
# Time: Very slow (10 disk reads)
```

**With caching (fast):**
```python
data = sc.textFile("large_file.txt")
data.cache()  # Mark for caching

for i in range(10):
    # First iteration: read from disk and cache
    # Iterations 2-10: read from memory!
    count = data.filter(lambda x: some_condition(x, i)).count()
    print(f"Iteration {i}: {count}")
# Time: Much faster (1 disk read, 9 memory reads)
```

### 6.9 Best Practices

**When to cache:**
- ✓ RDD used multiple times
- ✓ Expensive transformations before the RDD
- ✓ Iterative algorithms (ML, graph processing)
- ✓ Interactive queries on same dataset

**When NOT to cache:**
- ✗ RDD used only once
- ✗ Very large RDDs that won't fit in memory (unless MEMORY_AND_DISK)
- ✗ Simple transformations (faster to recompute)

**Choosing storage level:**
```
┌─────────────────────────────────────────────────────────────────────┐
│               CHOOSING THE RIGHT STORAGE LEVEL                       │
│                                                                      │
│  Question 1: Does RDD fit in memory?                                │
│  ├─ YES → Use MEMORY_ONLY (default)                                 │
│  └─ NO  → Question 2: Is recomputation expensive?                   │
│           ├─ YES → Use MEMORY_AND_DISK                              │
│           └─ NO  → Don't cache or use MEMORY_ONLY                   │
│                                                                      │
│  Question 3: Need to save memory?                                   │
│  └─ YES → Use serialized versions (*_SER)                           │
│                                                                      │
│  Question 4: Can tolerate recomputation loss?                       │
│  ├─ YES → Use MEMORY_ONLY                                           │
│  └─ NO  → Use MEMORY_AND_DISK or DISK_ONLY                          │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 7. Working with Key/Value Pairs

So far, we've worked with RDDs containing simple values. But many real-world operations require us to work with **key/value pairs** - for example, grouping by key, joining datasets, or computing per-key statistics. Spark provides a special type of RDD called a **Pair RDD** for this purpose.

### 7.1 What is a Pair RDD?

A **Pair RDD** is an RDD where each element is a tuple of **(key, value)**.

```python
# Example Pair RDD
pairRDD = sc.parallelize([
    ("apple", 5),
    ("banana", 3),
    ("apple", 2),
    ("orange", 7),
    ("banana", 4)
])

# Type: RDD[(String, Int)]
# Each element is a 2-tuple (key, value)
```

```
┌─────────────────────────────────────────────────────────────────────┐
│                          PAIR RDD STRUCTURE                          │
│                                                                      │
│   Regular RDD:     [1, 2, 3, 4, 5]                                  │
│   (single values)                                                    │
│                                                                      │
│   Pair RDD:        [("key1", val1), ("key2", val2), ...]            │
│   (key-value)      ─────┬──────────                                 │
│                         │                                            │
│                   Tuple (K, V)                                       │
│                                                                      │
│   Characteristics:                                                   │
│   • Keys are NOT necessarily unique                                 │
│   • Single value per key-value pair (not a collection)              │
│   • Keys can appear multiple times with different values            │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 7.2 Key Characteristics

**Important:** Unlike dictionaries/maps in programming languages:
- **Keys are NOT distinct** - the same key can appear multiple times
- **Each (key, value) is a single pair** - not key → list of values (that comes from transformations like groupByKey)

```python
# In a Pair RDD, this is valid:
[("apple", 5), ("apple", 2), ("apple", 8)]
# Three separate pairs, same key "apple"

# This is NOT how Pair RDDs are stored:
{"apple": [5, 2, 8]}  # This is what you get from groupByKey()
```

### 7.3 Why Use Pair RDDs?

Pair RDDs unlock powerful operations:

| Operation | Purpose | Example |
|-----------|---------|---------|
| **Aggregation** | Combine values by key | Sum sales per product |
| **Grouping** | Collect all values per key | Group users by country |
| **Joins** | Combine two datasets by key | Join user profiles with purchases |
| **Sorting** | Sort by keys | Alphabetically sort products |
| **Counting** | Count occurrences per key | Word frequency |

### 7.4 Creating Pair RDDs

**Method 1: Using map() with tuples**

```python
# From regular RDD
data = sc.parallelize(["apple 5", "banana 3", "apple 2"])

# Extract key-value pairs
pairs = data.map(lambda line: (line.split()[0], int(line.split()[1])))
# Result: [("apple", 5), ("banana", 3), ("apple", 2)]
```

**Method 2: Using map() with custom logic**

```python
# Create user activity pairs
users = sc.parallelize([
    "user1:login",
    "user2:purchase",
    "user1:logout",
    "user3:login"
])

# Create (user, action) pairs
user_actions = users.map(lambda x: tuple(x.split(":")))
# Result: [("user1", "login"), ("user2", "purchase"),
#          ("user1", "logout"), ("user3", "login")]
```

**Method 3: From structured data**

```python
# From log entries
logs = sc.parallelize([
    "2024-01-01 ERROR Connection failed",
    "2024-01-01 INFO Server started",
    "2024-01-02 ERROR Timeout",
    "2024-01-02 WARN High memory"
])

# Create (severity, message) pairs
log_pairs = logs.map(lambda line:
    (line.split()[1], line)
)
# Result: [("ERROR", "2024-01-01 ERROR Connection failed"),
#          ("INFO", "2024-01-01 INFO Server started"), ...]
```

**Method 4: Using keyBy()**

```python
# Create pairs by extracting key from value
words = sc.parallelize(["hello", "world", "hi", "hadoop"])

# Key by first letter
letter_pairs = words.keyBy(lambda word: word[0])
# Result: [("h", "hello"), ("w", "world"), ("h", "hi"), ("h", "hadoop")]
```

### 7.5 Examples from Lecture Slides

**Python:**
```python
# Create Pair RDD
pairs = sc.parallelize([("a", 1), ("b", 2), ("c", 3)])
```

**Java (from slides):**
```java
// Create Pair RDD in Java
JavaPairRDD<String, Integer> pairs =
    sc.parallelizePairs(Arrays.asList(
        new Tuple2<>("a", 1),
        new Tuple2<>("b", 2),
        new Tuple2<>("c", 3)
    ));
```

### 7.6 Operations Available on Pair RDDs

Pair RDDs support **all regular RDD operations** plus additional transformations and actions:

**Regular RDD operations (still work):**
```python
pairs = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])

# Regular transformations
filtered = pairs.filter(lambda kv: kv[1] > 1)  # [("b", 2), ("a", 3)]
mapped = pairs.map(lambda kv: (kv[0], kv[1] * 2))  # [("a", 2), ("b", 4), ("a", 6)]

# Regular actions
count = pairs.count()  # 3
first = pairs.first()  # ("a", 1)
```

**Special Pair RDD operations (covered in next sections):**
- Transformations: mapValues, reduceByKey, groupByKey, join, etc.
- Actions: countByKey, collectAsMap, lookup, etc.

### 7.7 Accessing Keys and Values

Since each element is a tuple, you can access components:

```python
pairs = sc.parallelize([("apple", 5), ("banana", 3)])

# Method 1: Index access
keys = pairs.map(lambda kv: kv[0])      # ["apple", "banana"]
values = pairs.map(lambda kv: kv[1])    # [5, 3]

# Method 2: Unpacking in lambda
keys = pairs.map(lambda (k, v): k)      # ["apple", "banana"]
values = pairs.map(lambda (k, v): v)    # [5, 3]

# Method 3: Built-in methods
keys = pairs.keys()                      # ["apple", "banana"]
values = pairs.values()                  # [5, 3]
```

---

## 8. Transformations on Pair RDDs

Pair RDDs expose powerful transformations designed specifically for key-value data. Let's explore the fundamental ones.

### 8.1 mapValues()

**mapValues(func)** applies a function to **only the values**, keeping keys unchanged.

```python
pairs = sc.parallelize([
    ("apple", 5),
    ("banana", 3),
    ("apple", 2)
])

# Double all values
doubled = pairs.mapValues(lambda v: v * 2)
# Result: [("apple", 10), ("banana", 6), ("apple", 4)]
```

**Why use mapValues() instead of map()?**

```python
# Using map() - must manually preserve keys
result = pairs.map(lambda (k, v): (k, v * 2))

# Using mapValues() - cleaner and clearer intent
result = pairs.mapValues(lambda v: v * 2)
```

```
┌─────────────────────────────────────────────────────────────────────┐
│                         mapValues() BEHAVIOR                         │
│                                                                      │
│   Input:  [("apple", 5), ("banana", 3), ("apple", 2)]               │
│                                                                      │
│   mapValues(lambda v: v * 2)                                         │
│                                                                      │
│   Process:                                                           │
│   ("apple", 5)   →  ("apple", 5*2)   = ("apple", 10)                │
│   ("banana", 3)  →  ("banana", 3*2)  = ("banana", 6)                │
│   ("apple", 2)   →  ("apple", 2*2)   = ("apple", 4)                 │
│                                                                      │
│   Output: [("apple", 10), ("banana", 6), ("apple", 4)]              │
│                                                                      │
│   Key Insight: Keys remain unchanged!                               │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**More Examples:**

```python
# Example 1: Type transformation
users = sc.parallelize([("user1", "25"), ("user2", "30")])
users_int = users.mapValues(lambda age: int(age))
# Result: [("user1", 25), ("user2", 30)]

# Example 2: String manipulation
products = sc.parallelize([("P1", "apple"), ("P2", "banana")])
upper_products = products.mapValues(lambda name: name.upper())
# Result: [("P1", "APPLE"), ("P2", "BANANA")]

# Example 3: Complex transformation
scores = sc.parallelize([("Alice", 85), ("Bob", 92), ("Charlie", 78)])
grades = scores.mapValues(lambda score:
    "A" if score >= 90 else "B" if score >= 80 else "C"
)
# Result: [("Alice", "B"), ("Bob", "A"), ("Charlie", "C")]
```

### 8.2 reduceByKey()

**reduceByKey(func)** combines values with the **same key** using an associative reduce function.

```python
pairs = sc.parallelize([
    ("apple", 5),
    ("banana", 3),
    ("apple", 2),
    ("orange", 7),
    ("banana", 4)
])

# Sum values by key
totals = pairs.reduceByKey(lambda v1, v2: v1 + v2)
# Result: [("apple", 7), ("banana", 7), ("orange", 7)]
```

**Execution Flow:**

```
┌─────────────────────────────────────────────────────────────────────┐
│                     reduceByKey() EXECUTION                          │
│                                                                      │
│  Input: [("apple",5), ("banana",3), ("apple",2),                    │
│          ("orange",7), ("banana",4)]                                 │
│                                                                      │
│  Step 1: Group by key (conceptually):                               │
│  ────────────────────────────────────                                │
│  "apple"  → [5, 2]                                                   │
│  "banana" → [3, 4]                                                   │
│  "orange" → [7]                                                      │
│                                                                      │
│  Step 2: Reduce within each group:                                  │
│  ──────────────────────────────────                                  │
│  "apple":  5 + 2 = 7                                                 │
│  "banana": 3 + 4 = 7                                                 │
│  "orange": 7                                                         │
│                                                                      │
│  Output: [("apple", 7), ("banana", 7), ("orange", 7)]               │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**Key Characteristics:**

1. **Automatically uses map-side combiner** (optimization!)
   - Combines values on each partition before shuffle
   - Reduces network transfer

2. **Function must be associative and commutative**
   - Same requirements as reduce()

3. **More efficient than groupByKey() + reduce()**

```
┌─────────────────────────────────────────────────────────────────────┐
│            reduceByKey() with MAP-SIDE COMBINER                      │
│                                                                      │
│  Partition 1:                      Partition 2:                     │
│  ────────────                      ────────────                     │
│  ("apple", 5)                      ("orange", 7)                    │
│  ("banana", 3)                     ("banana", 4)                    │
│  ("apple", 2)                                                       │
│                                                                      │
│  MAP-SIDE COMBINE:                 MAP-SIDE COMBINE:                │
│  ─────────────────                 ─────────────────                │
│  "apple":  5 + 2 = 7               "orange": 7                      │
│  "banana": 3                       "banana": 4                      │
│        │                                  │                          │
│        └──────────SHUFFLE──────────────── ┘                         │
│                      │                                               │
│        ┌─────────────┴──────────────┐                               │
│        │   REDUCE ACROSS PARTITIONS │                               │
│        └────────────────────────────┘                               │
│                      │                                               │
│  "apple":  7                                                         │
│  "banana": 3 + 4 = 7                                                 │
│  "orange": 7                                                         │
│                                                                      │
│  Benefits:                                                           │
│  • Reduced shuffle data (7 items → 5 items)                         │
│  • Less network transfer                                             │
│  • Faster execution                                                  │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**More Examples:**

```python
# Example 1: Finding maximum per key
scores = sc.parallelize([
    ("Alice", 85), ("Bob", 90), ("Alice", 92), ("Bob", 87)
])
max_scores = scores.reduceByKey(lambda v1, v2: max(v1, v2))
# Result: [("Alice", 92), ("Bob", 90)]

# Example 2: Concatenating strings
words = sc.parallelize([
    ("key1", "hello"), ("key2", "world"), ("key1", "spark")
])
concatenated = words.reduceByKey(lambda v1, v2: v1 + " " + v2)
# Result: [("key1", "hello spark"), ("key2", "world")]
# Note: Order might vary!

# Example 3: Product (multiplication)
values = sc.parallelize([
    ("a", 2), ("b", 3), ("a", 4), ("b", 5)
])
products = values.reduceByKey(lambda v1, v2: v1 * v2)
# Result: [("a", 8), ("b", 15)]
```

**Example from Slides: Finding Average per Key**

The slides show using `reduceByKey` for computing averages. However, `reduceByKey` alone cannot compute averages because:
- Average requires tracking both sum AND count
- reduceByKey's output type must match input type

For per-key averages, we need `combineByKey()` (next section!).

### 8.3 combineByKey() - Overview

**combineByKey()** is the most general aggregation function for Pair RDDs. Unlike reduceByKey(), it allows the accumulator type to differ from the value type.

```python
combineByKey(
    createCombiner,      # Create accumulator for first value of a key
    mergeValue,          # Add a value to accumulator
    mergeCombiners,      # Merge two accumulators
    numPartitions=None   # Optional: number of output partitions
)
```

**When to use:**
- Computing per-key averages, standard deviations, etc.
- Accumulator type differs from value type
- Need fine-grained control over aggregation

We'll cover combineByKey() in detail in Section 9 with the per-key average example.

---

## 9. Example: Per Key Average using combineByKey

Computing the average per key is a classic use case for `combineByKey()`. Let's build this step-by-step.

### 9.1 The Problem

Given pairs of (key, value), compute the average value for each key.

```python
# Input
data = sc.parallelize([
    ("Alice", 85),
    ("Bob", 90),
    ("Alice", 92),
    ("Charlie", 78),
    ("Bob", 87),
    ("Alice", 88)
])

# Desired output: (key, average)
# ("Alice", 88.33)  ← (85 + 92 + 88) / 3
# ("Bob", 88.5)     ← (90 + 87) / 2
# ("Charlie", 78.0) ← 78 / 1
```

### 9.2 Why Not reduceByKey()?

```python
# ATTEMPT 1: Using reduceByKey() - WRONG!
averages = data.reduceByKey(lambda v1, v2: (v1 + v2) / 2)
# Problem: This computes a running average, not the true average!
# Alice: (85 + 92)/2 = 88.5, then (88.5 + 88)/2 = 88.25 ✗ WRONG!
# Correct: (85 + 92 + 88)/3 = 88.33
```

**The issue:** We need to track both sum AND count, but reduceByKey's output type must match input type.

### 9.3 The combineByKey() Solution

**Accumulator type:** (sum, count) tuple

**Strategy:**
1. For the first value: Create (value, 1)
2. For subsequent values: Add to sum, increment count
3. Across partitions: Merge sums and counts
4. Final step: Divide sum by count

```python
# Step 1: combineByKey to get (sum, count) per key
sum_counts = data.combineByKey(
    lambda value: (value, 1),              # createCombiner
    lambda acc, value: (acc[0] + value, acc[1] + 1),  # mergeValue
    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])  # mergeCombiners
)

# Step 2: Compute averages
averages = sum_counts.mapValues(lambda (sum, count): sum / float(count))

# Result:
# [("Alice", 88.33), ("Bob", 88.5), ("Charlie", 78.0)]
```

### 9.4 The Three Functions Explained

**1. createCombiner: `lambda value: (value, 1)`**
- Called when we see a key for the FIRST time on a partition
- Creates initial accumulator
- Input: First value for this key
- Output: (sum=value, count=1)

**2. mergeValue: `lambda acc, value: (acc[0] + value, acc[1] + 1)`**
- Called for SUBSEQUENT values of the same key on a partition
- Updates accumulator with new value
- Input: Current accumulator (sum, count) and new value
- Output: (sum+value, count+1)

**3. mergeCombiners: `lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])`**
- Called to merge accumulators from different partitions
- Input: Two accumulators (sum1, count1) and (sum2, count2)
- Output: (sum1+sum2, count1+count2)

### 9.5 Detailed Execution Trace

Let's trace the execution with our data distributed across 2 partitions:

**Data Distribution:**
- Partition 1: `[("Alice", 85), ("Bob", 90), ("Alice", 92)]`
- Partition 2: `[("Charlie", 78), ("Bob", 87), ("Alice", 88)]`

**Phase 1: Within Partition 1**

```
Process: [("Alice", 85), ("Bob", 90), ("Alice", 92)]

Alice (first time):
  createCombiner(85) → (85, 1)
  Current state: {"Alice": (85, 1)}

Bob (first time):
  createCombiner(90) → (90, 1)
  Current state: {"Alice": (85, 1), "Bob": (90, 1)}

Alice (second time):
  mergeValue((85, 1), 92) → (85+92, 1+1) = (177, 2)
  Current state: {"Alice": (177, 2), "Bob": (90, 1)}

Partition 1 Result: [("Alice", (177, 2)), ("Bob", (90, 1))]
```

**Phase 2: Within Partition 2**

```
Process: [("Charlie", 78), ("Bob", 87), ("Alice", 88)]

Charlie (first time):
  createCombiner(78) → (78, 1)
  Current state: {"Charlie": (78, 1)}

Bob (first time):
  createCombiner(87) → (87, 1)
  Current state: {"Charlie": (78, 1), "Bob": (87, 1)}

Alice (first time):
  createCombiner(88) → (88, 1)
  Current state: {"Charlie": (78, 1), "Bob": (87, 1), "Alice": (88, 1)}

Partition 2 Result: [("Charlie", (78, 1)), ("Bob", (87, 1)), ("Alice", (88, 1))]
```

**Phase 3: Merge Across Partitions**

```
Partition 1:          Partition 2:
────────────          ────────────
Alice: (177, 2)       Alice: (88, 1)
Bob:   (90, 1)        Bob:   (87, 1)
                      Charlie: (78, 1)

Merge Alice:
  mergeCombiners((177, 2), (88, 1)) → (177+88, 2+1) = (265, 3)

Merge Bob:
  mergeCombiners((90, 1), (87, 1)) → (90+87, 1+1) = (177, 2)

Charlie (only in partition 2):
  (78, 1)  ← No merging needed

Final Result: [("Alice", (265, 3)), ("Bob", (177, 2)), ("Charlie", (78, 1))]
```

**Phase 4: Compute Averages**

```python
averages = sum_counts.mapValues(lambda (sum, count): sum / float(count))

Alice:   265 / 3 = 88.33
Bob:     177 / 2 = 88.5
Charlie: 78 / 1  = 78.0

Result: [("Alice", 88.33), ("Bob", 88.5), ("Charlie", 78.0)]
```

### 9.6 Visual Diagram (from slides)

```
┌─────────────────────────────────────────────────────────────────────┐
│              combineByKey EXECUTION FOR AVERAGE                      │
│                                                                      │
│  PARTITION 1: [("Alice",85), ("Bob",90), ("Alice",92)]              │
│  ─────────────────────────────────────────────────────────           │
│                                                                      │
│  Alice, 85:  createCombiner(85)                → (85, 1)             │
│  Bob, 90:    createCombiner(90)                → (90, 1)             │
│  Alice, 92:  mergeValue((85,1), 92)            → (177, 2)            │
│                                                                      │
│  Partition 1 Accumulators:                                          │
│  {"Alice": (177, 2), "Bob": (90, 1)}                                │
│                                                                      │
│  ═════════════════════════════════════════════════════════          │
│                                                                      │
│  PARTITION 2: [("Charlie",78), ("Bob",87), ("Alice",88)]            │
│  ──────────────────────────────────────────────────────              │
│                                                                      │
│  Charlie, 78: createCombiner(78)               → (78, 1)             │
│  Bob, 87:     createCombiner(87)               → (87, 1)             │
│  Alice, 88:   createCombiner(88)               → (88, 1)             │
│                                                                      │
│  Partition 2 Accumulators:                                          │
│  {"Charlie": (78, 1), "Bob": (87, 1), "Alice": (88, 1)}             │
│                                                                      │
│  ═════════════════════════════════════════════════════════          │
│                                                                      │
│  MERGE ACCUMULATORS ACROSS PARTITIONS:                              │
│  ─────────────────────────────────────                               │
│                                                                      │
│  Alice:  mergeCombiners((177,2), (88,1))  → (265, 3)                │
│  Bob:    mergeCombiners((90,1), (87,1))   → (177, 2)                │
│  Charlie: (78, 1)  [no merge needed]                                │
│                                                                      │
│  Final Accumulators:                                                │
│  [("Alice", (265,3)), ("Bob", (177,2)), ("Charlie", (78,1))]        │
│                                                                      │
│  ═════════════════════════════════════════════════════════          │
│                                                                      │
│  COMPUTE AVERAGES:                                                   │
│  ─────────────────                                                   │
│                                                                      │
│  Alice:   265/3 = 88.33                                              │
│  Bob:     177/2 = 88.5                                               │
│  Charlie: 78/1  = 78.0                                               │
│                                                                      │
│  Result: [("Alice", 88.33), ("Bob", 88.5), ("Charlie", 78.0)]       │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 9.7 Code Example from Slides

```python
# User-provided functions
def createCombiner(value):
    return (value, 1)  # (sum, count)

def mergeValue(acc, value):
    return (acc[0] + value, acc[1] + 1)

def mergeCombiner(acc1, acc2):
    return (acc1[0] + acc2[0], acc1[1] + acc2[1])

# Apply combineByKey
sumCounts = data.combineByKey(
    createCombiner,
    mergeValue,
    mergeCombiner
)

# Compute averages
averages = sumCounts.mapValues(lambda (sum, count): sum / float(count))
```

### 9.8 combineByKey() vs reduceByKey()

| Feature | reduceByKey() | combineByKey() |
|---------|---------------|----------------|
| **Accumulator Type** | Same as value type | Can be different |
| **Simplicity** | Easier to use | More complex |
| **Flexibility** | Limited | Very flexible |
| **Use Case** | Simple reductions (sum, max) | Complex aggregations (average, stats) |
| **Performance** | Faster (simpler) | Slightly slower (more general) |

### 9.9 Common Use Cases for combineByKey()

```python
# Use Case 1: Variance per key
def create(v):
    return (v, v*v, 1)  # (sum, sum_of_squares, count)

def merge_val(acc, v):
    return (acc[0]+v, acc[1]+v*v, acc[2]+1)

def merge_comb(a1, a2):
    return (a1[0]+a2[0], a1[1]+a2[1], a1[2]+a2[2])

stats = data.combineByKey(create, merge_val, merge_comb)
variance = stats.mapValues(lambda (s, ss, c):
    (ss/c) - (s/c)**2  # Variance formula
)

# Use Case 2: Set of unique values per key
def create(v):
    return {v}  # Set with single value

def merge_val(acc, v):
    acc.add(v)
    return acc

def merge_comb(a1, a2):
    return a1.union(a2)

unique_sets = data.combineByKey(create, merge_val, merge_comb)
```

---

## 10. Grouping Transforms on Pair RDDs

Sometimes we want to collect all values for each key. Spark provides several grouping transformations with different use cases.

### 10.1 groupByKey()

**groupByKey()** groups all values for each key into an **iterable** collection.

```python
pairs = sc.parallelize([
    ("apple", 5),
    ("banana", 3),
    ("apple", 2),
    ("orange", 7),
    ("banana", 4),
    ("apple", 1)
])

# Group all values by key
grouped = pairs.groupByKey()

# Convert iterables to lists for viewing
result = grouped.mapValues(list).collect()
# Result: [("apple", [5, 2, 1]), ("banana", [3, 4]), ("orange", [7])]
```

**Output Type:**
```python
# groupByKey() returns RDD[(K, Iterable[V])]
# Each key maps to an iterable of all its values
```

```
┌─────────────────────────────────────────────────────────────────────┐
│                      groupByKey() EXECUTION                          │
│                                                                      │
│  Input:  [("apple",5), ("banana",3), ("apple",2),                   │
│           ("orange",7), ("banana",4), ("apple",1)]                   │
│                                                                      │
│  Step 1: Shuffle data by keys                                       │
│  Step 2: Collect all values per key                                 │
│                                                                      │
│  Output:                                                             │
│  ("apple",  Iterator[5, 2, 1])                                       │
│  ("banana", Iterator[3, 4])                                          │
│  ("orange", Iterator[7])                                             │
│                                                                      │
│  Note: Values are in an Iterator, not a materialized list           │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**⚠️ WARNING: groupByKey() can cause Out Of Memory (OOM) errors**

```
┌─────────────────────────────────────────────────────────────────────┐
│                       groupByKey() PITFALL!                          │
│                                                                      │
│  Problem: ALL values for a key must fit in memory on ONE machine    │
│                                                                      │
│  Bad scenario:                                                       │
│  ─────────────                                                       │
│  Key "popular_product" has 1 million values                         │
│  → All 1 million values sent to one machine                         │
│  → Machine runs out of memory!                                      │
│  → Job crashes                                                       │
│                                                                      │
│  Alternative: Use reduceByKey() or aggregateByKey()                 │
│  ────────────                                                        │
│  • Process values incrementally                                      │
│  • Keep only aggregated result (much smaller)                       │
│  • More efficient and safer                                          │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**groupByKey() vs reduceByKey():**

```python
# BAD: groupByKey() + sum
pairs = sc.parallelize([("a", 1), ("a", 2), ("a", 3)])
sums = pairs.groupByKey().mapValues(lambda vals: sum(vals))
# Process: Group all values → [1,2,3] → sum = 6
# Problem: All values must fit in memory

# GOOD: reduceByKey()
sums = pairs.reduceByKey(lambda v1, v2: v1 + v2)
# Process: Incremental reduction → 1+2=3, 3+3=6
# Benefit: Uses map-side combiner, less memory, faster
```

**When to use groupByKey():**
- ✓ When you actually need all values grouped together
- ✓ Number of values per key is small
- ✓ No simple reduction function available
- ✗ For aggregation (use reduceByKey or aggregateByKey instead)

### 10.2 cogroup()

**cogroup(otherRDD)** groups values from **two RDDs** with the same keys.

```python
rdd1 = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])
rdd2 = sc.parallelize([("a", "x"), ("c", "y"), ("a", "z")])

# Cogroup by key
cogrouped = rdd1.cogroup(rdd2)

# Convert to lists for viewing
result = cogrouped.mapValues(lambda (iter1, iter2): (list(iter1), list(iter2))).collect()
# Result: [("a", ([1, 3], ["x", "z"])),
#          ("b", ([2], [])),
#          ("c", ([], ["y"]))]
```

**Key Points:**
- Returns `(key, (Iterable[V1], Iterable[V2]))`
- If key missing in one RDD, that iterator is empty
- Useful for joining related datasets

```
┌─────────────────────────────────────────────────────────────────────┐
│                        cogroup() EXECUTION                           │
│                                                                      │
│  RDD1: [("a",1), ("b",2), ("a",3)]                                  │
│  RDD2: [("a","x"), ("c","y"), ("a","z")]                            │
│                                                                      │
│  cogroup():                                                          │
│                                                                      │
│  Key "a": (values from RDD1, values from RDD2)                      │
│           ([1, 3], ["x", "z"])                                       │
│                                                                      │
│  Key "b": (values from RDD1, values from RDD2)                      │
│           ([2], [])  ← empty because "b" not in RDD2                │
│                                                                      │
│  Key "c": (values from RDD1, values from RDD2)                      │
│           ([], ["y"])  ← empty because "c" not in RDD1              │
│                                                                      │
│  Output: [("a", ([1,3], ["x","z"])),                                │
│           ("b", ([2], [])),                                          │
│           ("c", ([], ["y"]))]                                        │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**Example from Slides:**

```python
rdd1 = sc.parallelize([(1, 2), (3, 4), (3, 6)])
rdd2 = sc.parallelize([(3, 9), (4, 7)])

result = rdd1.cogroup(rdd2)
# Result: [(1, ([2], [])),
#          (3, ([4, 6], [9])),
#          (4, ([], [7]))]
```

**Can work with more than 2 RDDs:**

```python
rdd1 = sc.parallelize([("a", 1)])
rdd2 = sc.parallelize([("a", 2)])
rdd3 = sc.parallelize([("a", 3)])

result = rdd1.cogroup(rdd2, rdd3)
# Result: [("a", ([1], [2], [3]))]
```

### 10.3 subtractByKey()

**subtractByKey(otherRDD)** removes entries from the first RDD where the key exists in the second RDD.

```python
rdd1 = sc.parallelize([
    ("apple", 5),
    ("banana", 3),
    ("orange", 7)
])

rdd2 = sc.parallelize([
    ("banana", 100),  # Value doesn't matter, only key
    ("grape", 50)
])

# Remove keys that exist in rdd2
result = rdd1.subtractByKey(rdd2)
# Result: [("apple", 5), ("orange", 7)]
# "banana" removed because it exists in rdd2
```

**Key Insight:** Only the **keys** matter in rdd2, values are ignored.

```
┌─────────────────────────────────────────────────────────────────────┐
│                    subtractByKey() EXECUTION                         │
│                                                                      │
│  RDD1: [("apple",5), ("banana",3), ("orange",7)]                    │
│  RDD2: [("banana",100), ("grape",50)]                               │
│                                                                      │
│  Process:                                                            │
│  ────────                                                            │
│  1. Extract keys from RDD2: {"banana", "grape"}                     │
│  2. Keep pairs from RDD1 where key NOT in RDD2 keys                 │
│                                                                      │
│  Check each pair in RDD1:                                           │
│  • ("apple", 5)  → "apple" not in RDD2 → KEEP                       │
│  • ("banana", 3) → "banana" in RDD2    → REMOVE                     │
│  • ("orange", 7) → "orange" not in RDD2 → KEEP                      │
│                                                                      │
│  Output: [("apple", 5), ("orange", 7)]                              │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**Example from Slides:**

```python
rdd1 = sc.parallelize([(1, 2), (3, 4), (3, 6)])
rdd2 = sc.parallelize([(3, 9)])

result = rdd1.subtractByKey(rdd2)
# Result: [(1, 2)]
# All pairs with key=3 removed
```

**Use Cases:**
- Filtering out processed records
- Removing blacklisted items
- Set difference based on keys

---

## 11. Join Transforms on Pair RDDs

Joins combine two Pair RDDs based on matching keys. Spark supports several types of joins similar to SQL.

### 11.1 Inner Join (join)

**join(otherRDD)** returns pairs where the key exists in **both** RDDs. Performs a **cross product** of values for the same key.

```python
rdd1 = sc.parallelize([
    ("apple", 5),
    ("banana", 3),
    ("apple", 2)
])

rdd2 = sc.parallelize([
    ("apple", "red"),
    ("banana", "yellow"),
    ("grape", "purple")
])

# Inner join
result = rdd1.join(rdd2)
# Result: [("apple", (5, "red")),
#          ("apple", (2, "red")),
#          ("banana", (3, "yellow"))]
# "grape" excluded (only in rdd2)
```

**Join Behavior with Multiple Values:**

```python
rdd1 = sc.parallelize([(1, 2), (3, 4), (3, 6)])
rdd2 = sc.parallelize([(3, 9)])

result = rdd1.join(rdd2)
# Key 3 in rdd1: [4, 6]
# Key 3 in rdd2: [9]
# Cross product: (3, (4, 9)), (3, (6, 9))
# Result: [(3, (4, 9)), (3, (6, 9))]
```

```
┌─────────────────────────────────────────────────────────────────────┐
│                       INNER JOIN EXECUTION                           │
│                                                                      │
│  RDD1: [("apple",5), ("banana",3), ("apple",2)]                     │
│  RDD2: [("apple","red"), ("banana","yellow"), ("grape","purple")]   │
│                                                                      │
│  Step 1: Find common keys                                           │
│  ────────────────────────────                                        │
│  Keys in RDD1: {apple, banana}                                      │
│  Keys in RDD2: {apple, banana, grape}                               │
│  Common keys:  {apple, banana}  ← Only these will be in result      │
│                                                                      │
│  Step 2: Cross product of values for each common key                │
│  ──────────────────────────────────────────────────────              │
│                                                                      │
│  "apple":                                                            │
│    RDD1 values: [5, 2]                                               │
│    RDD2 values: ["red"]                                              │
│    Cross product: (5, "red"), (2, "red")                            │
│                                                                      │
│  "banana":                                                           │
│    RDD1 values: [3]                                                  │
│    RDD2 values: ["yellow"]                                           │
│    Cross product: (3, "yellow")                                     │
│                                                                      │
│  Output: [("apple", (5, "red")),                                    │
│           ("apple", (2, "red")),                                    │
│           ("banana", (3, "yellow"))]                                │
│                                                                      │
│  Note: "grape" excluded (only in RDD2, not in both)                 │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 11.2 Left Outer Join

**leftOuterJoin(otherRDD)** returns **all** keys from the **left (first) RDD**. For keys not in the right RDD, value is `None`.

```python
rdd1 = sc.parallelize([
    ("apple", 5),
    ("banana", 3),
    ("orange", 7)
])

rdd2 = sc.parallelize([
    ("apple", "red"),
    ("grape", "purple")
])

# Left outer join
result = rdd1.leftOuterJoin(rdd2)
# Result: [("apple", (5, Some("red"))),
#          ("banana", (3, None)),
#          ("orange", (7, None))]
# All keys from rdd1 present, None for missing rdd2 values
```

**Example from Slides:**

```python
rdd1 = sc.parallelize([(1, 2), (3, 4), (3, 6)])
rdd2 = sc.parallelize([(3, 9)])

result = rdd1.leftOuterJoin(rdd2)
# Result: [(1, (2, None)),
#          (3, (4, Some(9))),
#          (3, (6, Some(9)))]
```

```
┌─────────────────────────────────────────────────────────────────────┐
│                     LEFT OUTER JOIN DIAGRAM                          │
│                                                                      │
│  RDD1: [("a",1), ("b",2), ("c",3)]  ← LEFT side (all kept)          │
│  RDD2: [("a","x"), ("d","y")]       ← RIGHT side (may have nulls)   │
│                                                                      │
│  leftOuterJoin():                                                    │
│                                                                      │
│  Key "a": In both → ("a", (1, Some("x")))                           │
│  Key "b": Only in left → ("b", (2, None))                           │
│  Key "c": Only in left → ("c", (3, None))                           │
│  Key "d": Only in right → NOT INCLUDED                              │
│                                                                      │
│  Output: [("a", (1, Some("x"))),                                    │
│           ("b", (2, None)),                                          │
│           ("c", (3, None))]                                          │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 11.3 Right Outer Join

**rightOuterJoin(otherRDD)** returns **all** keys from the **right (second) RDD**. For keys not in the left RDD, value is `None`.

```python
rdd1 = sc.parallelize([
    ("apple", 5),
    ("banana", 3)
])

rdd2 = sc.parallelize([
    ("apple", "red"),
    ("banana", "yellow"),
    ("grape", "purple")
])

# Right outer join
result = rdd1.rightOuterJoin(rdd2)
# Result: [("apple", (Some(5), "red")),
#          ("banana", (Some(3), "yellow")),
#          ("grape", (None, "purple"))]
# All keys from rdd2 present, None for missing rdd1 values
```

**Example from Slides:**

```python
rdd1 = sc.parallelize([(1, 2), (3, 4), (3, 6)])
rdd2 = sc.parallelize([(3, 9), (4, 2)])

result = rdd1.rightOuterJoin(rdd2)
# Result: [(3, (Some(4), 9)),
#          (3, (Some(6), 9)),
#          (4, (None, 2))]
```

```
┌─────────────────────────────────────────────────────────────────────┐
│                    RIGHT OUTER JOIN DIAGRAM                          │
│                                                                      │
│  RDD1: [("a",1), ("b",2)]           ← LEFT side (may have nulls)    │
│  RDD2: [("a","x"), ("c","y")]       ← RIGHT side (all kept)         │
│                                                                      │
│  rightOuterJoin():                                                   │
│                                                                      │
│  Key "a": In both → ("a", (Some(1), "x"))                           │
│  Key "b": Only in left → NOT INCLUDED                               │
│  Key "c": Only in right → ("c", (None, "y"))                        │
│                                                                      │
│  Output: [("a", (Some(1), "x")),                                    │
│           ("c", (None, "y"))]                                        │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 11.4 Join Summary Table

| Join Type | Keys Included | Missing Value | Example |
|-----------|---------------|---------------|---------|
| **Inner Join** | Only in both RDDs | N/A (excluded) | `join()` |
| **Left Outer** | All from left RDD | None for missing right | `leftOuterJoin()` |
| **Right Outer** | All from right RDD | None for missing left | `rightOuterJoin()` |
| **Full Outer** | All from both RDDs | None for either side | `fullOuterJoin()` |

**Choosing the right join:**

```python
# Use inner join when: You only want matched records
users.join(purchases)  # Users who made purchases

# Use left outer join when: Keep all from left, even if no match
users.leftOuterJoin(purchases)  # All users, with/without purchases

# Use right outer join when: Keep all from right, even if no match
users.rightOuterJoin(purchases)  # All purchases, even deleted users

# Use full outer join when: Keep everything from both sides
users.fullOuterJoin(purchases)  # All users and all purchases
```

---

## 12. Sorting Transforms on Pair RDDs

### 12.1 sortByKey()

**sortByKey(ascending=True)** sorts the RDD by **keys only**. Values are NOT considered in sorting.

```python
pairs = sc.parallelize([
    (3, "three"),
    (1, "one"),
    (4, "four"),
    (2, "two")
])

# Sort by key (ascending)
sorted_asc = pairs.sortByKey(ascending=True)
# Result: [(1, "one"), (2, "two"), (3, "three"), (4, "four")]

# Sort by key (descending)
sorted_desc = pairs.sortByKey(ascending=False)
# Result: [(4, "four"), (3, "three"), (2, "two"), (1, "one")]
```

**Key Characteristics:**
- Only keys are used for sorting
- Values do NOT affect sort order
- Default is ascending order

**Example with Duplicate Keys:**

```python
pairs = sc.parallelize([
    (1, 2),
    (3, 6),
    (3, 5),  # Same key, different value
    (2, 4)
])

sorted_pairs = pairs.sortByKey()
# Result: [(1, 2), (2, 4), (3, 6), (3, 5)]
# OR:     [(1, 2), (2, 4), (3, 5), (3, 6)]
# Order of values with same key is not guaranteed!
```

### 12.2 Custom Key Functions

You can transform keys before sorting using a custom key function:

```python
# Sort by length of string keys
words = sc.parallelize([
    ("hi", 1),
    ("hello", 2),
    ("hey", 3)
])

sorted_by_length = words.sortByKey(keyfunc=lambda k: len(k))
# Result: [("hi", 1), ("hey", 3), ("hello", 2)]

# Treat numeric keys as strings
nums = sc.parallelize([(1, "one"), (10, "ten"), (2, "two")])
sorted_as_str = nums.sortByKey(keyfunc=lambda k: str(k))
# Numeric sort: [(1, "one"), (2, "two"), (10, "ten")]
# String sort:  [(1, "one"), (10, "ten"), (2, "two")]
```

### 12.3 Secondary Sort

For sorting by value when keys are equal, you need to use a custom key:

```python
# Want to sort by key, then by value
pairs = sc.parallelize([
    (3, 6),
    (1, 2),
    (3, 5),
    (2, 4)
])

# Create composite key  (original_key, value)
sorted_pairs = pairs.map(lambda (k, v): ((k, v), (k, v))) \
                    .sortByKey() \
                    .map(lambda (_, (k, v)): (k, v))
# Result: [(1, 2), (2, 4), (3, 5), (3, 6)]
```

**Reference:** For more advanced secondary sorting, see: http://codingjunkie.net/spark-secondary-sort/

---

## 13. Stratified Sampling

### 13.1 sampleByKey()

**sampleByKey(withReplacement, fractions, seed)** allows you to specify **different sampling rates for different keys**.

```python
from pyspark import SparkContext
sc = SparkContext()

# Data with different keys
data = sc.parallelize([
    ("male", 1), ("female", 2), ("male", 3),
    ("female", 4), ("male", 5), ("female", 6),
    ("male", 7), ("female", 8)
])

# Sample different fractions per key
fractions = {
    "male": 0.5,    # Sample 50% of males
    "female": 1.0   # Sample 100% of females
}

sampled = data.sampleByKey(False, fractions, seed=42)
# Approximately:
# - 50% of male records: ~2 records
# - 100% of female records: ~4 records
```

**Signature:**
```python
sampleByKey(
    withReplacement,  # True: can pick same item multiple times
    fractions,        # Dict: {key: fraction}
    seed=None         # Random seed for reproducibility
)
```

**Key Point:** The result is **approximate**, not exact.

```
┌─────────────────────────────────────────────────────────────────────┐
│                    sampleByKey() EXPLANATION                         │
│                                                                      │
│  Input: RDD with keys and values                                    │
│  fractions = {k: fₖ} where fₖ is sampling fraction for key k        │
│                                                                      │
│  For each key k:                                                     │
│  • Count of values: nₖ                                               │
│  • Sampling fraction: fₖ                                             │
│  • Expected sample size: fₖ × nₖ                                     │
│  • Actual sample size: ≈ fₖ × nₖ (approximate!)                      │
│                                                                      │
│  Total expected output: Σₖ (fₖ × nₖ)                                 │
│                                                                      │
│  Use Case: Balanced sampling across categories                      │
│  Example: Equal representation of male/female in dataset            │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**Example from Slides:**

```python
# Suppose we have user activity data
activities = sc.parallelize([
    ("login", 1), ("purchase", 2), ("login", 3),
    ("logout", 4), ("purchase", 5), ("login", 6)
])

# Sample different rates for different activity types
fractions = {
    "login": 0.3,     # 30% of logins
    "purchase": 1.0,  # 100% of purchases (keep all)
    "logout": 0.5     # 50% of logouts
}

sample = activities.sampleByKey(False, fractions, 12345)
```

**Use Cases:**
- Balancing skewed datasets
- Proportional sampling from different categories
- Testing with representative samples

---

## 14. Actions on Pair RDDs

Pair RDDs support all normal RDD actions, plus some specialized ones.

### 14.1 Normal Actions (Still Work)

All actions from Section 5 work on Pair RDDs:

```python
pairs = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])

# Regular actions
count = pairs.count()           # 3
all_pairs = pairs.collect()     # [("a", 1), ("b", 2), ("a", 3)]
first = pairs.first()           # ("a", 1)
sample = pairs.take(2)          # [("a", 1), ("b", 2)]
```

### 14.2 countByKey()

Returns a **dictionary** mapping each key to its **count** (number of occurrences).

```python
pairs = sc.parallelize([
    ("apple", 5),
    ("banana", 3),
    ("apple", 2),
    ("orange", 7),
    ("apple", 1)
])

counts = pairs.countByKey()
# Result: {"apple": 3, "banana": 1, "orange": 1}
# Dictionary: {key: count}
```

**Use Case:** Frequency analysis by key.

### 14.3 collectAsMap()

Returns the RDD as a **Python dictionary**. If a key appears multiple times, only one value is kept (non-deterministic which one).

```python
pairs = sc.parallelize([
    ("apple", 5),
    ("banana", 3),
    ("orange", 7)
])

dict_result = pairs.collectAsMap()
# Result: {"apple": 5, "banana": 3, "orange": 7}
# Type: Python dict
```

**⚠️ Warning with Duplicate Keys:**

```python
pairs = sc.parallelize([
    ("apple", 5),
    ("apple", 2),  # Duplicate key!
    ("banana", 3)
])

dict_result = pairs.collectAsMap()
# Result: {"apple": 5, "banana": 3}  OR  {"apple": 2, "banana": 3}
# Which value for "apple" is kept is non-deterministic!
```

**Best Practice:** Only use `collectAsMap()` when keys are unique, or you've already reduced by key.

### 14.4 lookup(key)

Returns **list of all values** for a specific key.

```python
pairs = sc.parallelize([
    ("apple", 5),
    ("banana", 3),
    ("apple", 2),
    ("orange", 7),
    ("apple", 1)
])

apple_values = pairs.lookup("apple")
# Result: [5, 2, 1]
# Returns list of all values for key "apple"

banana_values = pairs.lookup("banana")
# Result: [3]

missing_values = pairs.lookup("grape")
# Result: []  (empty list for missing key)
```

**Performance Note:**
- Efficient: Scans only partitions that might contain the key
- Returns all values, not just one

### 14.5 Summary: Special Pair RDD Actions

| Action | Returns | Example |
|--------|---------|---------|
| **countByKey()** | Dict {key: count} | `{"a": 3, "b": 1}` |
| **collectAsMap()** | Dict {key: value} | `{"a": 5, "b": 3}` |
| **lookup(key)** | List of values for key | `[5, 2, 1]` |

---

## 15. Case Study 1: Web Crawl - Extract Title & Build Inverted Index

Now let's apply what we've learned to a real-world scenario: processing web crawl data.

### 15.1 Context: CommonCrawl Dataset

**CommonCrawl** (https://commoncrawl.org) is a massive dataset containing petabytes of web crawl data:
- Billions of web pages
- Stored in HDFS
- Each record: (URL, HTML content)

**Our Tasks:**
1. Extract page titles from HTML
2. Build an inverted index (keyword → list of URLs)

### 15.2 Task 1: Extract Title for URL

**Goal:** Parse HTML and extract the `<title>` tag.

**Input:** Pair RDD of (URL, HTML)
**Output:** Pair RDD of (URL, title)

```python
# Input: RDD[(URL, HTML_content)]
HTMLRdd = sc.parallelize([
    ("http://example.com/page1", "<html><title>Example Page</title><body>...</body></html>"),
    ("http://example.com/page2", "<html><title>Another Page</title><body>...</body></html>")
])

# Function to extract title from HTML
def parseOutTitle(html):
    """Extract title from HTML string."""
    import re
    match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
    if match:
        return match.group(1)
    return "No Title"

# Apply transformation
titleRdd = HTMLRdd.mapValues(lambda html: parseOutTitle(html))

# Result:
# [("http://example.com/page1", "Example Page"),
#  ("http://example.com/page2", "Another Page")]
```

**Execution:**
```
┌─────────────────────────────────────────────────────────────────────┐
│                    EXTRACT TITLE TRANSFORMATION                      │
│                                                                      │
│  Input HTMLRdd:                                                      │
│  ("url1", "<html><title>Page 1</title>...</html>")                  │
│  ("url2", "<html><title>Page 2</title>...</html>")                  │
│                                                                      │
│  mapValues(parseOutTitle):                                           │
│  │                                                                   │
│  ├─ url1: parseOutTitle(...) → "Page 1"                             │
│  └─ url2: parseOutTitle(...) → "Page 2"                             │
│                                                                      │
│  Output titleRdd:                                                    │
│  ("url1", "Page 1")                                                  │
│  ("url2", "Page 2")                                                  │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 15.3 Task 2: Build Inverted Index

**Goal:** Create a mapping from keywords to list of URLs containing that keyword.

**Conceptual Flow:**
```
URL → HTML → Words → (URL, Word) → (Word, URL) → (Word, [URLs])
```

**Step-by-Step Implementation:**

**Step 1: Extract words from HTML**

```python
# Function to parse HTML and extract words
def parseOutWords(html):
    """Extract all words from HTML."""
    import re
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', ' ', html)
    # Extract words (alphanumeric sequences)
    words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
    return words

# flatMap to get (URL, word) pairs
HTMLWordRdd = HTMLRdd.flatMap(lambda (url, html):
    [(url, word) for word in parseOutWords(html)]
)

# Example output:
# [("url1", "the"), ("url1", "quick"), ("url1", "brown"),
#  ("url2", "the"), ("url2", "lazy"), ...]
```

**Step 2: Filter stopwords and identify keywords**

```python
# Stopwords list (words to ignore)
STOPWORDS = {"the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for"}

# Filter out stopwords
HTMLKeywordRdd = HTMLWordRdd.filter(lambda (url, word):
    word not in STOPWORDS
)

# Example output:
# [("url1", "quick"), ("url1", "brown"), ("url2", "lazy"), ...]
```

**Step 3: Invert to (keyword, URL)**

```python
# Swap key and value
keyUrlRdd = HTMLKeywordRdd.map(lambda (url, keyword):
    (keyword, url)
)

# Example output:
# [("quick", "url1"), ("brown", "url1"), ("lazy", "url2"), ...]
```

**Step 4: Group by keyword**

```python
# Group all URLs for each keyword
keyUrlsRdd = keyUrlRdd.groupByKey()

# Convert iterables to lists
invertedIndex = keyUrlsRdd.mapValues(list)

# Example output:
# [("quick", ["url1", "url3"]),
#  ("brown", ["url1"]),
#  ("lazy", ["url2", "url4"]),
#  ...]
```

### 15.4 Complete Code

```python
# Complete inverted index pipeline
def buildInvertedIndex(HTMLRdd):
    """Build inverted index from (URL, HTML) RDD."""

    # Step 1: Extract words
    HTMLWordRdd = HTMLRdd.flatMap(lambda (url, html):
        [(url, word) for word in parseOutWords(html)]
    )

    # Step 2: Filter stopwords
    STOPWORDS = {"the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for"}
    HTMLKeywordRdd = HTMLWordRdd.filter(lambda (url, word): word not in STOPWORDS)

    # Step 3: Invert to (keyword, URL)
    keyUrlRdd = HTMLKeywordRdd.map(lambda (url, keyword): (keyword, url))

    # Step 4: Group by keyword
    keyUrlsRdd = keyUrlRdd.groupByKey().mapValues(list)

    return keyUrlsRdd

# Usage
invertedIndex = buildInvertedIndex(HTMLRdd)
```

### 15.5 Example Execution Trace

```
┌─────────────────────────────────────────────────────────────────────┐
│                  INVERTED INDEX CONSTRUCTION                         │
│                                                                      │
│  Input (3 documents):                                                │
│  ──────────────────────                                              │
│  url1: "The quick brown fox"                                         │
│  url2: "The lazy dog"                                                │
│  url3: "Quick brown dog jumps"                                       │
│                                                                      │
│  STEP 1: Extract words (flatMap)                                    │
│  ─────────────────────────────────                                   │
│  [("url1", "the"), ("url1", "quick"), ("url1", "brown"),            │
│   ("url1", "fox"), ("url2", "the"), ("url2", "lazy"),               │
│   ("url2", "dog"), ("url3", "quick"), ("url3", "brown"),            │
│   ("url3", "dog"), ("url3", "jumps")]                                │
│                                                                      │
│  STEP 2: Filter stopwords                                           │
│  ──────────────────────────                                          │
│  [("url1", "quick"), ("url1", "brown"), ("url1", "fox"),            │
│   ("url2", "lazy"), ("url2", "dog"), ("url3", "quick"),             │
│   ("url3", "brown"), ("url3", "dog"), ("url3", "jumps")]            │
│                                                                      │
│  STEP 3: Invert (map)                                               │
│  ─────────────────────                                               │
│  [("quick", "url1"), ("brown", "url1"), ("fox", "url1"),            │
│   ("lazy", "url2"), ("dog", "url2"), ("quick", "url3"),             │
│   ("brown", "url3"), ("dog", "url3"), ("jumps", "url3")]            │
│                                                                      │
│  STEP 4: Group by keyword (groupByKey)                              │
│  ───────────────────────────────────────                             │
│  quick  → ["url1", "url3"]                                           │
│  brown  → ["url1", "url3"]                                           │
│  fox    → ["url1"]                                                   │
│  lazy   → ["url2"]                                                   │
│  dog    → ["url2", "url3"]                                           │
│  jumps  → ["url3"]                                                   │
│                                                                      │
│  Final Inverted Index:                                               │
│  ──────────────────────                                              │
│  {                                                                   │
│    "quick":  ["url1", "url3"],                                       │
│    "brown":  ["url1", "url3"],                                       │
│    "fox":    ["url1"],                                               │
│    "lazy":   ["url2"],                                               │
│    "dog":    ["url2", "url3"],                                       │
│    "jumps":  ["url3"]                                                │
│  }                                                                   │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**Usage:** Search for documents containing "quick":
```python
results = invertedIndex.lookup("quick")
# Returns: ["url1", "url3"]
```

---

## 16. Case Study 2: Web Graph and PageRank

PageRank is Google's original algorithm for ranking web pages based on link structure.

### 16.1 Task 1: Build WWW Link Graph

**Goal:** Extract all links from web pages and build an adjacency list representation of the web graph.

**Input:** (URL, HTML content)
**Output:** (source_URL, [destination_URLs])

**Step 1: Extract links from HTML**

```python
def parseOutLinks(html):
    """Extract all <a href="..."> URLs from HTML."""
    import re
    # Find all href attributes
    links = re.findall(r'href=["\']([^"\']+)["\']', html, re.IGNORECASE)
    return links

# Apply to HTMLRdd
links = HTMLRdd.mapValues(lambda html: parseOutLinks(html))

# Result: (src_url, [dest_url1, dest_url2, ...])
# Example:
# [("http://A.com", ["http://B.com", "http://C.com"]),
#  ("http://B.com", ["http://C.com", "http://D.com"]),
#  ...]
```

**Adjacency List Representation:**

```
┌─────────────────────────────────────────────────────────────────────┐
│                      WEB GRAPH STRUCTURE                             │
│                                                                      │
│  Page A ──┐                                                          │
│           ├──▶ Page B ────▶ Page D                                  │
│           │       │                                                  │
│           └───────┼──▶ Page C ◀────┐                                │
│                   │                 │                                │
│                   └─────────────────┘                                │
│                                                                      │
│  Adjacency List:                                                     │
│  ───────────────                                                     │
│  A: [B, C]                                                           │
│  B: [C, D]                                                           │
│  C: []                                                               │
│  D: []                                                               │
│                                                                      │
│  Interpretation:                                                     │
│  • Page A links to B and C                                           │
│  • Page B links to C and D                                           │
│  • Pages C and D have no outgoing links                             │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 16.2 Task 2: Calculate PageRank

**PageRank Algorithm:**

PageRank is an iterative algorithm that computes the "importance" of each page based on the link structure.

**Core Idea:**
- A page is important if it's linked to by important pages
- Importance is distributed among all outgoing links

**Formula:**
```
PR(page) = (1 - d) + d × Σ(PR(linking_page) / num_outlinks(linking_page))

Where:
- PR(page) = PageRank of the page
- d = damping factor (typically 0.85)
- Σ = sum over all pages linking to this page
- num_outlinks = number of outgoing links from linking page
```

### 16.3 PageRank Implementation in Spark

**Algorithm Overview:**

1. Initialize all pages with rank = 1.0
2. For each iteration:
   a. Each page distributes its rank equally to its neighbors
   b. Each page receives contributions from its inbound links
   c. Update ranks with formula: `rank' = 0.15 + 0.85 × contributions`
3. Repeat for 30 iterations (or until convergence)

**Complete Code:**

```python
def computeContribs(urls, rank):
    """
    Compute contributions from a page to its neighbors.

    Args:
        urls: List of destination URLs
        rank: Current PageRank of source page

    Yields:
        (dest_url, contribution) for each destination
    """
    num_urls = len(urls)
    for url in urls:
        yield (url, rank / num_urls)


# Load link graph: (source_url, [dest_urls])
links = sc.parallelize([
    ("A", ["B", "C"]),
    ("B", ["C", "D"]),
    ("C", ["A"]),
    ("D", ["C"])
])

# Initialize all ranks to 1.0
ranks = links.map(lambda (url, neighbors): (url, 1.0))

# Run PageRank for 30 iterations
for iteration in range(30):
    # Step 1: Join links with current ranks
    # Result: (src_url, ([dest_urls], rank))
    links_with_ranks = links.join(ranks)

    # Step 2: Compute contributions
    # flatMap to emit (dest_url, contribution) for each link
    contribs = links_with_ranks.flatMap(
        lambda (src, (urls, rank)): computeContribs(urls, rank)
    )

    # Step 3: Aggregate contributions per page and compute new rank
    ranks = contribs.reduceByKey(lambda v1, v2: v1 + v2) \
                    .mapValues(lambda contrib: 0.15 + 0.85 * contrib)

# Collect final ranks
final_ranks = ranks.collect()
for (url, rank) in final_ranks:
    print(f"{url} has rank: {rank}")
```

### 16.4 Detailed Execution Trace (One Iteration)

```
┌─────────────────────────────────────────────────────────────────────┐
│                  PAGERANK ITERATION EXAMPLE                          │
│                                                                      │
│  Link Graph:                                                         │
│  ───────────                                                         │
│  A → [B, C]                                                          │
│  B → [C]                                                             │
│  C → [A]                                                             │
│                                                                      │
│  Initial Ranks (iteration 0):                                       │
│  ─────────────────────────────                                       │
│  A: 1.0                                                              │
│  B: 1.0                                                              │
│  C: 1.0                                                              │
│                                                                      │
│  ITERATION 1:                                                        │
│  ────────────                                                        │
│                                                                      │
│  Step 1: Join links with ranks                                      │
│  ──────────────────────────────                                      │
│  A: ([B, C], 1.0)                                                    │
│  B: ([C], 1.0)                                                       │
│  C: ([A], 1.0)                                                       │
│                                                                      │
│  Step 2: Compute contributions (flatMap)                            │
│  ────────────────────────────────────────                            │
│  A sends to B: 1.0 / 2 = 0.5                                         │
│  A sends to C: 1.0 / 2 = 0.5                                         │
│  B sends to C: 1.0 / 1 = 1.0                                         │
│  C sends to A: 1.0 / 1 = 1.0                                         │
│                                                                      │
│  Contributions RDD:                                                  │
│  [("B", 0.5), ("C", 0.5), ("C", 1.0), ("A", 1.0)]                   │
│                                                                      │
│  Step 3: Aggregate contributions (reduceByKey)                      │
│  ──────────────────────────────────────────────                      │
│  A: 1.0                                                              │
│  B: 0.5                                                              │
│  C: 0.5 + 1.0 = 1.5                                                  │
│                                                                      │
│  Step 4: Apply PageRank formula (mapValues)                         │
│  ────────────────────────────────────────────                        │
│  A: 0.15 + 0.85 × 1.0 = 1.0                                          │
│  B: 0.15 + 0.85 × 0.5 = 0.575                                        │
│  C: 0.15 + 0.85 × 1.5 = 1.425                                        │
│                                                                      │
│  Updated Ranks (after iteration 1):                                 │
│  ───────────────────────────────────                                 │
│  A: 1.0                                                              │
│  B: 0.575                                                            │
│  C: 1.425                                                            │
│                                                                      │
│  Repeat for 30 iterations until convergence...                      │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 16.5 Understanding the Algorithm

**Key Steps:**

1. **join(links, ranks)**: Combine link structure with current page ranks
2. **flatMap(computeContribs)**: Each page distributes its rank to neighbors
3. **reduceByKey(add)**: Sum all contributions received by each page
4. **mapValues(applyFormula)**: Apply damping factor and compute new rank

**Why 30 iterations?**
- PageRank converges (stabilizes) after ~10-50 iterations
- More iterations = more accurate ranks
- Can also check for convergence (rank change < threshold)

**Damping Factor (0.85):**
- Models random web surfing behavior
- 85% chance: Follow a link
- 15% chance: Jump to random page
- Prevents rank from accumulating on dead ends

### 16.6 Real-World Considerations

```python
# In production, add:
# 1. Handle pages with no outlinks (dangling nodes)
# 2. Normalize ranks to sum to 1.0
# 3. Check for convergence
# 4. Cache links RDD (used in every iteration)

links.cache()  # Important! Reused in every iteration

# Convergence check:
for iteration in range(100):
    new_ranks = compute_iteration(links, ranks)

    # Check if ranks changed significantly
    diff = new_ranks.join(ranks) \
                    .mapValues(lambda (new, old): abs(new - old)) \
                    .values() \
                    .max()

    if diff < 0.001:  # Converged
        print(f"Converged after {iteration} iterations")
        break

    ranks = new_ranks
```

---

## 17. Case Study 3: Web Search

Now let's combine the inverted index and PageRank to implement a simple search engine.

### 17.1 Search Workflow

```
┌─────────────────────────────────────────────────────────────────────┐
│                      WEB SEARCH PIPELINE                             │
│                                                                      │
│  1. User Query: "spark apache"                                      │
│          │                                                           │
│          ▼                                                           │
│  2. Lookup Keywords in Inverted Index                               │
│     ────────────────────────────────────                             │
│     "spark"  → [url1, url2, url3, url5]                             │
│     "apache" → [url2, url3, url4]                                   │
│          │                                                           │
│          ▼                                                           │
│  3. Find Intersection (URLs with ALL keywords)                      │
│     ───────────────────────────────────────────                      │
│     [url1, url2, url3, url5] ∩ [url2, url3, url4] = [url2, url3]    │
│          │                                                           │
│          ▼                                                           │
│  4. Lookup PageRank for Matching URLs                               │
│     ──────────────────────────────────────                           │
│     url2 → PR = 0.85                                                │
│     url3 → PR = 1.23                                                │
│          │                                                           │
│          ▼                                                           │
│  5. Sort by PageRank (Descending)                                   │
│     ─────────────────────────────────                                │
│     url3: PR=1.23  ← Best result                                    │
│     url2: PR=0.85                                                   │
│          │                                                           │
│          ▼                                                           │
│  6. Select Top N Results                                            │
│     ───────────────────────                                          │
│     Top 10 results                                                   │
│          │                                                           │
│          ▼                                                           │
│  7. Join with Titles and Return                                     │
│     ──────────────────────────────                                   │
│     [(url3, "Apache Spark Documentation", 1.23),                    │
│      (url2, "Spark Tutorial", 0.85)]                                │
│          │                                                           │
│          ▼                                                           │
│  8. Display to User                                                  │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 17.2 Implementation Approach 1: Using Driver Memory

**Simpler but not scalable:**

```python
def search_v1(query, invertedIndex, pageRanks, titles):
    """
    Search using driver memory for intersection.

    Args:
        query: Search phrase (e.g., "spark apache")
        invertedIndex: RDD[(keyword, [urls])]
        pageRanks: RDD[(url, rank)]
        titles: RDD[(url, title)]

    Returns:
        Top 10 results with titles and ranks
    """
    # Step 1: Split query into keywords
    keywords = query.lower().split()

    # Step 2: Lookup each keyword in inverted index (brings to driver!)
    url_lists = []
    for keyword in keywords:
        urls = invertedIndex.lookup(keyword)
        if urls:
            url_lists.append(set(urls[0]))  # lookup returns list of values

    # Step 3: Find intersection on driver
    if not url_lists:
        return []

    matching_urls = set.intersection(*url_lists)

    # Step 4: Lookup PageRank for matching URLs
    # Filter pageRanks to matching URLs
    ranked_matches = pageRanks.filter(lambda (url, rank): url in matching_urls)

    # Step 5: Sort by PageRank (descending) and take top 10
    top_urls = ranked_matches.map(lambda (url, rank): (rank, url)) \
                              .top(10)  # top() gives descending order

    # Step 6: Get top URLs
    top_url_list = [url for (rank, url) in top_urls]

    # Step 7: Join with titles
    results = titles.filter(lambda (url, title): url in top_url_list) \
                    .collectAsMap()

    # Step 8: Combine with ranks
    final_results = []
    for (rank, url) in top_urls:
        title = results.get(url, "No Title")
        final_results.append((url, title, rank))

    return final_results
```

**Limitation:** Uses driver memory for intersection - not scalable for many keywords.

### 17.3 Implementation Approach 2: Fully Distributed

**Better approach using only RDD operations:**

```python
def search_v2(query, invertedIndex, pageRanks, titles):
    """
    Fully distributed search using only RDD operations.

    More scalable than v1.
    """
    keywords = query.lower().split()

    # Step 1: Get RDD of (keyword, url) from inverted index
    # Explode the grouped structure
    keyword_url_pairs = invertedIndex.flatMap(
        lambda (kw, urls): [(kw, url) for url in urls]
    )

    # Step 2: Filter to only our query keywords
    relevant_pairs = keyword_url_pairs.filter(
        lambda (kw, url): kw in keywords
    )

    # Step 3: Count how many keywords each URL matches
    url_keyword_counts = relevant_pairs.map(lambda (kw, url): (url, 1)) \
                                       .reduceByKey(lambda a, b: a + b)

    # Step 4: Keep only URLs that match ALL keywords
    num_keywords = len(keywords)
    matching_urls = url_keyword_counts.filter(
        lambda (url, count): count == num_keywords
    ).keys()  # Get just the URLs

    # Step 5: Join with PageRank
    # Create (url, None) pairs to join with ranks
    url_pairs = matching_urls.map(lambda url: (url, None))
    ranked_results = url_pairs.join(pageRanks)  # (url, (None, rank))

    # Step 6: Sort by rank and take top 10
    top_10 = ranked_results.map(lambda (url, (_, rank)): (rank, url)) \
                           .top(10)

    # Step 7: Get URLs from top 10
    top_urls = sc.parallelize([url for (rank, url) in top_10])

    # Step 8: Join with titles
    url_title_map = top_urls.map(lambda url: (url, None)) \
                            .join(titles) \
                            .collectAsMap()

    # Step 9: Combine results
    results = []
    for (rank, url) in top_10:
        title = url_title_map.get(url, ("", "No Title"))[1]
        results.append({
            "url": url,
            "title": title,
            "score": rank
        })

    return results
```

### 17.4 Word Co-occurrence for Search Suggestions

**Goal:** Suggest related searches based on word co-occurrence.

```python
# Build co-occurrence matrix from web pages
def build_cooccurrence(HTMLRdd):
    """
    Build word co-occurrence matrix.

    Output: (word1, word2) → count
    """
    # For each page, get all word pairs
    word_pairs = HTMLRdd.flatMap(lambda (url, html):
        words = parseOutWords(html)
        # Generate all pairs
        pairs = []
        for i in range(len(words)):
            for j in range(i + 1, len(words)):
                if words[i] < words[j]:  # Normalize order
                    pairs.append(((words[i], words[j]), 1))
                else:
                    pairs.append(((words[j], words[i]), 1))
        return pairs
    )

    # Count co-occurrences
    cooccurrence = word_pairs.reduceByKey(lambda a, b: a + b)
    return cooccurrence


# Use for suggestions
def suggest_related_words(word, cooccurrence, top_n=5):
    """Find words that frequently co-occur with given word."""
    # Filter to pairs containing our word
    related = cooccurrence.filter(lambda ((w1, w2), count):
        w1 == word or w2 == word
    )

    # Extract the other word and count
    other_words = related.map(lambda ((w1, w2), count):
        (w2 if w1 == word else w1, count)
    )

    # Sort by count and take top N
    suggestions = other_words.top(top_n, key=lambda (word, count): count)
    return [word for (word, count) in suggestions]


# Example usage
cooccurrence = build_cooccurrence(HTMLRdd)
suggestions = suggest_related_words("spark", cooccurrence, 5)
# Might return: ["apache", "scala", "hadoop", "cluster", "rdd"]
```

### 17.5 Complete Search System

```python
# Initialize all components
HTMLRdd = sc.textFile("hdfs://web_crawl/*.html")  # Load web pages

# Build inverted index
invertedIndex = buildInvertedIndex(HTMLRdd)
invertedIndex.cache()  # Cache for fast lookups

# Extract titles
titles = HTMLRdd.mapValues(parseOutTitle)
titles.cache()

# Build link graph and compute PageRank
links = HTMLRdd.mapValues(parseOutLinks)
links.cache()
pageRanks = computePageRank(links, iterations=30)
pageRanks.cache()

# Build co-occurrence matrix for suggestions
cooccurrence = build_cooccurrence(HTMLRdd)
cooccurrence.cache()

# Search function
def search(query):
    """Main search function."""
    # Get results
    results = search_v2(query, invertedIndex, pageRanks, titles)

    # Get suggestions
    keywords = query.split()
    suggestions = []
    for kw in keywords:
        suggestions.extend(suggest_related_words(kw, cooccurrence, 3))

    return {
        "results": results,
        "suggestions": list(set(suggestions))
    }

# Example usage
search_results = search("apache spark")
print(f"Found {len(search_results['results'])} results")
print(f"Suggestions: {', '.join(search_results['suggestions'])}")
```

---

## Summary

### Key Takeaways from Lecture 2.2:

**1. Transformations on RDDs:**
- **Element-wise**: filter, map, flatMap
  - filter: Keep elements matching predicate
  - map: One-to-one transformation
  - flatMap: One-to-many transformation with flattening
- **Set operations**: distinct, union, intersection, subtract, cartesian, sample
  - Most require shuffle (expensive!)
  - union is cheap (no shuffle)

**2. Actions on RDDs:**
- **reduce(func)**: Aggregate using commutative & associative function
  - Two-level reduction: within partitions, then across partitions
- **aggregate(zeroVal, mergeValue, mergeComb)**: General aggregation
  - Accumulator type can differ from element type
  - Perfect for computing averages, statistics
- **Others**: collect, take, takeOrdered, top, takeSample, forEach, countByValue

**3. RDD Persistence:**
- **Why**: Avoid recomputation when RDD used multiple times
- **How**: persist() or cache() marks RDD; cached on first action
- **Levels**: MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY, etc.
- **LRU eviction**: Automatically manages memory
- **Best for**: Iterative algorithms, interactive queries

**4. Pair RDDs (Key/Value):**
- **Creation**: map to tuples, keyBy()
- **Keys not unique**: Multiple values per key allowed
- **All regular operations** still work

**5. Pair RDD Transformations:**
- **mapValues**: Transform values, keep keys
- **reduceByKey**: Aggregate values by key (uses map-side combiner!)
- **combineByKey**: General aggregation with different accumulator type
  - createCombiner, mergeValue, mergeCombiner
  - Perfect for per-key averages
- **groupByKey**: Collect all values per key (⚠️ can cause OOM!)
- **cogroup**: Group values from two RDDs by key
- **subtractByKey**: Remove keys present in another RDD

**6. Joins:**
- **join (inner)**: Only keys in both RDDs
- **leftOuterJoin**: All keys from left, None for missing right
- **rightOuterJoin**: All keys from right, None for missing left
- Cross product when multiple values per key

**7. Other Operations:**
- **sortByKey**: Sort by keys (values ignored)
- **sampleByKey**: Different sampling rates per key

**8. Pair RDD Actions:**
- **countByKey**: Count per key
- **collectAsMap**: Convert to dictionary
- **lookup(key)**: Get all values for a key

**9. Case Study 1 - Web Crawl:**
- Extract titles: mapValues(parseTitle)
- Build inverted index: flatMap → filter → map → groupByKey
- Keyword → [URLs] mapping

**10. Case Study 2 - PageRank:**
- Build link graph: mapValues(parseLinks)
- Iterative algorithm: join, flatMap, reduceByKey, mapValues
- Damping factor: 0.15 + 0.85 × contributions
- 30 iterations to convergence

**11. Case Study 3 - Web Search:**
- Combine inverted index + PageRank
- Intersection of keyword results
- Sort by PageRank, return top N
- Word co-occurrence for suggestions

### Performance Tips:

1. **Use reduceByKey over groupByKey** whenever possible
2. **Cache/persist RDDs** that are used multiple times
3. **Avoid collect()** on large RDDs (OOM risk)
4. **Minimize shuffles** (distinct, groupByKey, joins are expensive)
5. **Use map-side combiners** (reduceByKey does this automatically)
6. **Choose right persistence level** based on memory/recomputation trade-off

### Common Patterns:

```python
# Word count
words.map(lambda w: (w, 1)).reduceByKey(lambda a, b: a + b)

# Per-key average
data.combineByKey(
    lambda v: (v, 1),
    lambda acc, v: (acc[0] + v, acc[1] + 1),
    lambda a1, a2: (a1[0] + a2[0], a1[1] + a2[1])
).mapValues(lambda (sum, count): sum / count)

# Join and filter
users.join(purchases).filter(lambda (id, (user, purchase)): purchase > 100)
```

---

## References

1. Zaharia, M., et al., "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing", USENIX NSDI, 2012

2. Karau, H., Konwinski, A., Wendell, P., Zaharia, M., "Learning Spark: Lightning-Fast Big Data Analysis", O'Reilly Media, 2015
   - Chapter 3: Programming with RDDs
   - Chapter 4: Working with Key/Value Pairs

3. Apache Spark Documentation:
   - RDD Programming Guide: https://spark.apache.org/docs/latest/rdd-programming-guide.html
   - Spark API Documentation: https://spark.apache.org/docs/latest/api/python/

4. Page, L., Brin, S., Motwani, R., Winograd, T., "The PageRank Citation Ranking: Bringing Order to the Web", Stanford InfoLab, 1999

5. Common Crawl: https://commoncrawl.org/

6. Secondary Sort in Spark: http://codingjunkie.net/spark-secondary-sort/

7. Course Materials:
   - Lecture Slides: DS256 L2.2 - Big Data Processing with Apache Spark
   - Professor: Yogesh Simmhan, IISc Bangalore

