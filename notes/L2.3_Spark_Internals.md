# Lecture 2.3: Spark Internals — Logical Plans, Physical Plans & Execution

## DS256 - Scalable Systems for Data Science
### Module 2: Processing Large Volumes of Big Data

---

## 1. Spark Architecture Overview

Before diving into Spark's internal execution model, let's understand the high-level architecture of how a Spark application runs on a cluster.

### 1.1 Components of a Spark Application

```
┌─────────────────────────────────────────────────────────────────────┐
│                        SPARK APPLICATION                             │
│                                                                      │
│   ┌──────────────────────────┐                                       │
│   │      Driver Program      │                                       │
│   │  ┌────────────────────┐  │                                       │
│   │  │   SparkContext      │  │  ← Entry point for all Spark ops     │
│   │  │                    │  │  ← Coordinates with Cluster Manager   │
│   │  │  User Code:        │  │                                       │
│   │  │  - Define RDDs     │  │                                       │
│   │  │  - Transformations │  │                                       │
│   │  │  - Actions         │  │                                       │
│   │  └────────┬───────────┘  │                                       │
│   └───────────┼──────────────┘                                       │
│               │                                                      │
│               │  Logical Plan → Physical Plan → Task Scheduling      │
│               │                                                      │
│   ┌───────────▼──────────────────────────────────────────────────┐   │
│   │                    Cluster of Workers                         │   │
│   │                                                               │   │
│   │  ┌─────────────┐  ┌─────────────┐       ┌─────────────┐     │   │
│   │  │  Worker 1    │  │  Worker 2    │  ...  │  Worker N    │     │   │
│   │  │ ┌─────────┐  │  │ ┌─────────┐  │       │ ┌─────────┐  │     │   │
│   │  │ │Executor │  │  │ │Executor │  │       │ │Executor │  │     │   │
│   │  │ │ ┌─────┐ │  │  │ │ ┌─────┐ │  │       │ │ ┌─────┐ │  │     │   │
│   │  │ │ │Task │ │  │  │ │ │Task │ │  │       │ │ │Task │ │  │     │   │
│   │  │ │ │Task │ │  │  │ │ │Task │ │  │       │ │ │Task │ │  │     │   │
│   │  │ │ │Task │ │  │  │ │ │Task │ │  │       │ │ │Task │ │  │     │   │
│   │  │ │ └─────┘ │  │  │ │ └─────┘ │  │       │ │ └─────┘ │  │     │   │
│   │  │ └─────────┘  │  │ └─────────┘  │       │ └─────────┘  │     │   │
│   │  └─────────────┘  └─────────────┘       └─────────────┘     │   │
│   └──────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
```

### 1.2 The Two Plans

When you write a Spark application, Spark converts it into execution through **two key planning phases**:

| Phase | What It Does | Key Abstraction |
|-------|-------------|-----------------|
| **Logical Plan** | Determines the chain of RDDs and their dependencies (the "what") | RDD lineage graph with dependency types |
| **Physical Plan** | Determines how to execute the logical plan on the cluster (the "how") | Stages, Tasks, Shuffle boundaries |

Think of it this way:
- The **Logical Plan** is like an architect's blueprint — it describes the structure and relationships
- The **Physical Plan** is like the construction schedule — it describes the order, the workers, and how materials move

### 1.3 From User Code to Execution

```
User Code              Logical Plan            Physical Plan           Execution
─────────          ──────────────          ──────────────          ─────────────
                   
 RDD               RDD DAG with            Stages split            Tasks scheduled
 transformations → dependency edges    →   at shuffle          →   on Executors
 + action          (Narrow/Wide)           boundaries              (pipelined within
                                           + Tasks per stage       each stage)
```

---

## 2. Logical Plan: RDD Dependencies

The logical plan is the **computing chain** — a directed acyclic graph (DAG) of RDDs connected by dependencies. Understanding dependencies is the single most important concept for understanding Spark internals.

### 2.1 The General Logical Plan

Every Spark job follows this general structure:

```
Step 1:  Create initial RDDs         (from HDFS, memory, etc.)
         │
         ▼
Step 2:  Apply transformations       (map, filter, join, groupByKey, ...)
         │                           Each produces one or more new RDDs
         ▼
Step 3:  Call an action              (count, collect, save, foreach, ...)
         │                           Triggers actual computation
         ▼
Step 4:  Results sent to Driver      (driver applies a final function)
```

**Key insight:** Transformations are **lazy** — they just build up the logical plan (the DAG). No computation happens until an **action** is called. This is what allows Spark to optimize the entire plan before executing anything.

### 2.2 How RDDs Are Produced

Each `transformation()` produces **one or more new RDDs**. The number of RDDs produced is often **more than you'd expect**, because some transformations are internally composed of multiple sub-transformations.

For simple transformations, the mapping is straightforward:

| Transformation | RDD Produced | compute() Logic |
|:--------------|:-------------|:----------------|
| `map(f)` | MappedRDD | `iterator(partition).map(f)` |
| `filter(f)` | FilteredRDD | `iterator(partition).filter(f)` |
| `flatMap(f)` | FlatMappedRDD | `iterator(partition).flatMap(f)` |
| `mapPartitions(f)` | MapPartitionsRDD | `f(iterator(partition))` |
| `mapPartitionsWithIndex(f)` | MapPartitionsRDD | `f(partition.index, iterator(partition))` |
| `sample(...)` | PartitionwiseSampledRDD | `PoissonSampler.sample(iterator(partition))` |

For complex transformations (like `groupByKey`, `reduceByKey`, `join`, `distinct`), **multiple intermediate RDDs** are created internally. We'll examine these in detail below.

### 2.3 Every RDD Has Five Properties

Internally, each RDD is represented through a **common interface** that exposes five pieces of information (from the RDD paper):

| Property | Description | Example |
|----------|-------------|---------|
| `partitions()` | List of Partition objects (the atomic pieces of the dataset) | An HDFS-backed RDD has one partition per HDFS block |
| `dependencies()` | List of dependencies on parent RDDs | `map` → NarrowDependency; `groupByKey` → ShuffleDependency |
| `compute(partition, context)` | Function to compute elements of a partition given parent iterators | `parent.iterator(split).map(f)` for a MappedRDD |
| `preferredLocations(partition)` | Nodes where partition can be accessed faster (data locality) | HDFS block locations |
| `partitioner()` | Metadata about hash/range partitioning (if any) | `HashPartitioner(numPartitions)` |

---

## 3. Narrow vs. Wide Dependencies (The Most Important Concept)

This is the **foundational distinction** that drives everything in Spark's execution model — how stages are formed, how tasks are pipelined, how fault recovery works, and how shuffles happen.

### 3.1 Narrow Dependency (Full Dependency)

**Definition:** Each partition of the **parent RDD** is used by **at most one partition** of the child RDD.

Equivalently: each partition of the child RDD depends on a **small, fixed number** of entire parent partitions.

```
Parent RDD                  Child RDD
┌──────────┐               ┌──────────┐
│ Part 0   │──────────────▶│ Part 0   │    ← 1:1 mapping
├──────────┤               ├──────────┤
│ Part 1   │──────────────▶│ Part 1   │    ← Each parent partition
├──────────┤               ├──────────┤       is fully consumed by
│ Part 2   │──────────────▶│ Part 2   │       exactly one child partition
└──────────┘               └──────────┘
```

**Key properties of Narrow Dependencies:**
- Parent and child partitions **can be on the same worker** — no data needs to move across the network
- Multiple narrow-dependency transformations can be **pipelined** in a single task (no intermediate data stored)
- Fault recovery is **cheap**: only the lost partition (and its specific parent) needs to be recomputed
- **No shuffle required**

**Types of Narrow Dependencies:**

| Type | Pattern | Example Transformations |
|------|---------|------------------------|
| **OneToOneDependency** (1:1) | Each child partition depends on exactly one parent partition | `map()`, `filter()`, `flatMap()`, `mapPartitions()` |
| **RangeDependency** (1:1 with ranges) | Retains partition boundaries from parent | `union()` |
| **N:1 NarrowDependency** | Multiple parent partitions → one child partition | `coalesce(shuffle=false)`, co-partitioned `join()` |

### 3.2 Wide Dependency (Shuffle Dependency / Partial Dependency)

**Definition:** Each partition of the **parent RDD** may be depended upon by **multiple partitions** of the child RDD. Equivalently: each child partition depends on **a part of** (not the entire) each parent partition.

```
Parent RDD                  Child RDD
┌──────────┐          ┌───▶┌──────────┐
│ Part 0   │─────┬────┤    │ Part 0   │    ← Each child partition
├──────────┤     │    └───▶├──────────┤       needs a PIECE of
│ Part 1   │──┬──┼────┬───▶│ Part 1   │       EVERY parent partition
├──────────┤  │  │    │    ├──────────┤
│ Part 2   │──┼──┴────┼───▶│ Part 2   │    ← This is a SHUFFLE!
└──────────┘  │       │    └──────────┘
              └───────┘
```

**Key properties of Wide Dependencies:**
- Data must **move across the network** (shuffle) — records from one parent partition go to different child partitions
- The shuffle is a **barrier**: all tasks on the upstream side must complete before downstream tasks can start
- Intermediate shuffle data is **materialized to disk** (not pipelined)
- Fault recovery is **expensive**: a single failed node might cause the loss of data needed by ALL child partitions, requiring recomputation of the entire parent stage
- **Shuffles are costly!** They involve disk I/O, network communication, serialization/deserialization, and barrier synchronization

**Examples of Wide Dependencies:**

| Transformation | Why It's Wide |
|---------------|---------------|
| `groupByKey()` | Records with the same key from ALL parent partitions must be gathered into ONE child partition |
| `reduceByKey()` | Same as above, but with map-side combine |
| `sortByKey()` | Records must be repartitioned by key ranges |
| `distinct()` | Duplicate records could be in any parent partition |
| `join()` (non-co-partitioned) | Matching keys from two RDDs could be in any partition |
| `repartition()` | Explicitly redistributes data |

### 3.3 Visual Summary: Narrow vs Wide

```
NARROW DEPENDENCIES                          WIDE DEPENDENCIES
(Black arrows — no shuffle)                  (Red arrows — SHUFFLE!)

  ┌───┐    ┌───┐                              ┌───┐    ┌───┐
  │ A │───▶│ B │   1:1 (map, filter)          │ A │╲  ╱│ B │
  └───┘    └───┘                              └───┘ ╲╱ └───┘
                                              ┌───┐ ╱╲ ┌───┐
  ┌───┐    ┌───┐                              │   │╱  ╲│   │
  │ A │───▶│   │                              └───┘    └───┘
  └───┘    │ B │   Range (union)
  ┌───┐    │   │                              groupByKey, reduceByKey,
  │ C │───▶│   │                              sortByKey, join, distinct
  └───┘    └───┘

  ┌───┐    ┌───┐
  │ A │───▶│   │
  └───┘    │ B │   N:1 (co-partitioned join,
  ┌───┐    │   │        coalesce)
  │ C │───▶│   │
  └───┘    └───┘

Key rule: If parent partitions' records go to
EXACTLY ONE child partition → Narrow
If they split across MULTIPLE children → Wide
```

### 3.4 When Does a Wide Dependency Become Narrow?

An important subtlety: some transformations that are **usually wide** can become **narrow** if certain conditions are met:

**`join(otherRDD)` can be narrow if:**
- Both input RDDs have the **same partitioner** (e.g., both HashPartitioner)
- AND the **same number of partitions**
- This is called a **co-partitioned join** (or hash join)
- Since records with the same key are already on the same partition, no shuffle is needed!

**`cogroup(otherRDD)` can be narrow if:**
- Same partitioner type AND same number of partitions for all input RDDs

This is why **Spark tracks the partitioner** used to generate each RDD — it can use this information to avoid unnecessary shuffles. For example:

```python
# RDD A was created with HashPartitioner(3)
# RDD B was created with HashPartitioner(3)
# join(A, B) → NarrowDependency (no shuffle needed!)

# RDD A uses HashPartitioner(3)
# RDD C uses RangePartitioner(3)
# join(A, C) → ShuffleDependency for at least one of them
```

---

## 4. Detailed Logical Plans for Complex Transformations

Many transformations are internally more complex than they appear. Understanding their internal RDD chains is crucial for performance tuning.

### 4.1 groupByKey(numPartitions)

```
                    ShuffleDependency
Input RDD ─────────────────────────────────▶ ShuffledRDD
RDD[(K,V)]          (shuffle all            RDD[(K, Iterable[V])]
                     records by key)              │
                                                  │ OneToOneDependency
                                                  ▼
                                            MapPartitionsRDD
                                            RDD[(K, Iterable[V])]
                                            (cast ArrayBuffer → Iterable)
```

**Important:** `groupByKey()` has **NO map-side combine**. Why?
- Map-side combine for `groupByKey` would insert all records into a hash table on the map side
- This doesn't reduce the amount of data shuffled (all records must still be sent)
- It just creates many objects in the Old Generation of the JVM heap, causing GC pressure
- So it's actually **worse** to do map-side combine for `groupByKey`!

### 4.2 reduceByKey(func, numPartitions)

```
                 OneToOneDependency           ShuffleDependency          OneToOneDependency
Input RDD ─────────────────────────▶ MapPartitionsRDD ──────────────▶ ShuffledRDD ──────────────▶ MapPartitionsRDD
RDD[(K,V)]                          (map-side combine)               (shuffled data)             (reduce-side aggregate)
```

**Key difference from `groupByKey`:** `reduceByKey` **does** perform map-side combine!
- Before shuffle: applies the reduce function locally to combine values with the same key
- After shuffle: applies the reduce function again to combine across partitions
- This is equivalent to having a Combiner in MapReduce
- **Result: significantly less data is shuffled**

```
Example: Word Count — reduceByKey vs groupByKey

Data: [(a,1), (b,1), (a,1), (b,1), (a,1)]  across 2 partitions

With groupByKey:
  Partition 1: (a,1), (b,1), (a,1)  ──shuffle──▶  (a, [1,1,1,1])  then sum
  Partition 2: (b,1), (a,1)         ──shuffle──▶  (b, [1,1])       then sum
  Shuffled records: 5 (ALL records moved)

With reduceByKey:
  Partition 1: (a,1), (b,1), (a,1) → combine → (a,2), (b,1)  ──shuffle──▶  (a, 2+2=4)
  Partition 2: (b,1), (a,1)        → combine → (a,1), (b,1)  ──shuffle──▶  (b, 1+1=2)
  Shuffled records: 4 (FEWER records moved — and even fewer with more data)
```

### 4.3 distinct(numPartitions)

`distinct()` deduplicates records. Since duplicates can exist in different partitions, a shuffle is needed.

```
                  OneToOne          (internally uses reduceByKey)          OneToOne
Input RDD ──────────────────▶ MappedRDD ──────────────────────────────▶ MapPartitionsRDD
RDD[T]      map(x → (x, null))  RDD[(T,null)]   reduceByKey            RDD[(T,null)]
                                                 (shuffle + dedup)           │
                                                                             │ map(x → x._1)
                                                                             ▼
                                                                        MappedRDD
                                                                        RDD[T]
```

**Steps:**
1. Map each record `x` to `(x, null)` — transforms RDD[T] into RDD[(K,V)] format required for shuffle
2. `reduceByKey` with a no-op combiner — this shuffles by key and keeps only one copy per key (deduplication)
3. Map back: extract just the key from `(key, null)` to get the deduplicated RDD[T]

### 4.4 cogroup(otherRDD, numPartitions)

`cogroup()` groups records from **two or more RDDs** by key. It's the building block for `join()`, `intersection()`, and other multi-RDD operations.

```
RDD a ──────────┐
RDD[(K,V)]      │     ShuffleDependency      OneToOneDependency
                ├───▶ CoGroupedRDD ─────────▶ MapPartitionsRDD
RDD b ──────────┘     or                      RDD[(K, (Iterable[V], Iterable[W]))]
RDD[(K,W)]            OneToOneDependency
                      (depends on partitioners)
```

**Critical question: Is the dependency Narrow or Wide?**

The dependency between each parent RDD and CoGroupedRDD depends on TWO factors:

| Factor | Narrow (OneToOne) | Wide (Shuffle) |
|--------|-------------------|----------------|
| **Same # of partitions?** | ✓ Required | ✗ Different counts |
| **Same partitioner type?** | ✓ Required | ✗ Different types |

```
Example scenarios:

Scenario 1: Both RDDs have HashPartitioner(3)
  RDD a (Hash, 3 parts) ──OneToOne──▶ CoGroupedRDD (Hash, 3 parts)
  RDD b (Hash, 3 parts) ──OneToOne──▶

Scenario 2: Different partitioners
  RDD a (Range, 3 parts) ──OneToOne──▶ CoGroupedRDD (Range, 3 parts)
  RDD b (Hash, 3 parts)  ──Shuffle───▶

Scenario 3: Different partition counts
  RDD a (Hash, 3 parts) ──Shuffle──▶ CoGroupedRDD (Hash, 4 parts)
  RDD b (Hash, 3 parts) ──Shuffle──▶
```

### 4.5 join(otherRDD, numPartitions)

`join()` performs an inner join of two `RDD[(K,V)]` and `RDD[(K,W)]`. Internally, it uses `cogroup()` as the building block:

```
                    cogroup                        mapValues              flatMap
RDD a ──┐                                    
        ├──▶ CoGroupedRDD ──▶ MappedValuesRDD ──────────────────────▶ FlatMappedValuesRDD
RDD b ──┘    RDD[(K,(Iter[V],  RDD[(K,(Iter[V],   Cartesian product    RDD[(K, (V, W))]
              Iter[W]))]        Iter[W]))]         of V and W values
```

**Steps:**
1. `cogroup()` groups both RDDs by key → `RDD[(K, (Iterable[V], Iterable[W]))]`
2. For each key, compute the **Cartesian product** of the two Iterables
3. `flatMap()` flattens the results

**Performance:** If both input RDDs are hash-partitioned with the same partitioner, the cogroup step uses OneToOneDependency (no shuffle needed) — this is called a **hash join**.

### 4.6 sortByKey(ascending, numPartitions)

```
                    ShuffleDependency                  OneToOneDependency
Input RDD ──────────────────────────────▶ ShuffledRDD ──────────────────────▶ MapPartitionsRDD
RDD[(K,V)]    uses RangePartitioner       (records partitioned              (records sorted within
              to determine partition       by key ranges)                    each partition)
              boundaries
```

**How it works:**
1. A **RangePartitioner** is used — it samples the RDD to determine partition boundaries (e.g., partition 0 gets keys A-F, partition 1 gets G-M, etc.)
2. Shuffle distributes records to the correct partition based on key range
3. Within each partition, records are sorted using **TimSort** (a hybrid merge-sort/insertion-sort)
4. The final result: records across all partitions are in global sorted order

### 4.7 union(otherRDD)

```
RDD a ──┐     RangeDependency (1:1)
        ├───▶ UnionRDD
RDD b ──┘     (simply concatenates partition lists)
```

- `union()` **never moves data** — it just creates a new RDD whose partitions are the union of the parent partitions
- Uses `RangeDependency` to retain the borders of original RDDs
- **Very cheap operation** — no shuffle, no data copy

### 4.8 cartesian(otherRDD)

```
RDD a (m partitions) ──┐
                        ├──▶ CartesianRDD (m × n partitions)
RDD b (n partitions) ──┘
```

- Output has `m × n` partitions — the i-th partition of CartesianRDD depends on partition `i/n` of RDD a and partition `i%n` of RDD b
- **NarrowDependency** — each parent partition is fully consumed
- But each partition of RDD a or RDD b is used by multiple child partitions (this looks like a wide dependency but it's technically narrow because each parent partition is used **in its entirety** — no partial dependency)

### 4.9 coalesce(numPartitions, shuffle)

Reduces or increases the number of partitions:

```
Case 1: coalesce(3, shuffle=false) — Decrease partitions only
                    NarrowDependency (N:1)
RDD (5 partitions) ────────────────────────▶ CoalescedRDD (3 partitions)
                    (just groups parent       
                     partitions together)     

Case 2: coalesce(10, shuffle=true) — Can increase partitions
                    OneToOne                 ShuffleDependency           
RDD (5 partitions) ──────────▶ MappedRDD ───────────────────▶ ShuffledRDD ──▶ CoalescedRDD
                   (assign     (records      (shuffle by        (10 partitions)
                    increasing  with keys)    hash of key)
                    keys via
                    round-robin)
```

- `shuffle=false`: Can only **decrease** partitions (merging). NarrowDependency. No network I/O.
- `shuffle=true`: Can increase or decrease. Assigns monotonically increasing keys in a round-robin fashion, then shuffles by hash of key for uniform distribution.
- `repartition(n)` is exactly `coalesce(n, shuffle=true)`

### 4.10 The Primitive: combineByKey()

Many shuffle-based transformations (`groupByKey`, `reduceByKey`, `aggregateByKey`) are all internally implemented using **`combineByKey()`**, which is the most general aggregation primitive in Spark:

```scala
def combineByKey[C](
    createCombiner: V => C,          // First record with a key: create initial combiner
    mergeValue: (C, V) => C,         // Subsequent records: merge value into combiner
    mergeCombiners: (C, C) => C,     // After shuffle: merge combiners from diff. partitions
    partitioner: Partitioner,
    mapSideCombine: Boolean = true
): RDD[(K, C)]
```

**How it works step by step:**

```
Partition 1: (a,1), (b,2), (a,3)           Partition 2: (a,4), (b,5)

Step 1: createCombiner — first occurrence of each key
  (a,1) → combiner_a = createCombiner(1) = 1
  (b,2) → combiner_b = createCombiner(2) = 2

Step 2: mergeValue — subsequent occurrences of same key
  (a,3) → combiner_a = mergeValue(1, 3) = 4
  (a,4) → combiner_a' = createCombiner(4) = 4  [on partition 2]
  (b,5) → combiner_b' = createCombiner(5) = 5  [on partition 2]

  After map-side combine:
  Partition 1: (a, 4), (b, 2)
  Partition 2: (a, 4), (b, 5)

Step 3: Shuffle by key

Step 4: mergeCombiners — combine results from different partitions
  key a: mergeCombiners(4, 4) = 8
  key b: mergeCombiners(2, 5) = 7
```

**Why this matters:** Understanding `combineByKey` explains why `reduceByKey` (which uses it with `mapSideCombine=true`) is more efficient than `groupByKey` (which uses it with `mapSideCombine=false`).

---

## 5. Physical Plan: From Logical DAG to Stages and Tasks

The physical plan answers: **"Given this DAG of RDDs, how do we actually execute it on a cluster?"**

### 5.1 The Problem: How to Execute a Complex DAG?

Consider a complex DAG with many RDDs connected by both narrow and wide dependencies:

```
      RDD_A                                    RDD_E
        │ (narrow)                               │ (narrow)
      RDD_B                                    RDD_F
        │ (narrow)                             ╱
      RDD_C ──────────(wide/shuffle)──────▶ RDD_G
                                               │ (narrow)
                                             RDD_H (final)
```

**Naive approach 1: One task per RDD pair**
- Create a task for every arrow in the graph
- Problem: **too many intermediate results stored** — every RDD would be materialized

**Naive approach 2: One giant task for everything**
- Try to compute everything in a single task from the final RDD backwards
- Problem: **shuffle dependency blocks pipelining** — you can't pipeline across a shuffle because you need ALL parent partitions to compute ANY child partition

### 5.2 The Solution: Cut at Shuffle Boundaries → Stages

**Spark's strategy:**

> **Check backwards from the final RDD. Add each NarrowDependency into the current stage. Break out for a new stage when there's a ShuffleDependency.**

```
Stage 2                          Stage 1                          Stage 0 (final)
┌─────────────────────┐         ┌─────────────────────┐         ┌─────────────────────┐
│                     │         │                     │         │                     │
│  RDD_A              │         │  RDD_E              │         │  RDD_G              │
│    │ (narrow)       │         │    │ (narrow)        │         │    │ (narrow)       │
│  RDD_B              │         │  RDD_F              │         │  RDD_H (final)      │
│    │ (narrow)       │         │                     │         │                     │
│  RDD_C              │         │                     │         │                     │
│                     │         │                     │         │                     │
└──────────┬──────────┘         └──────────┬──────────┘         └─────────────────────┘
           │                               │                              ▲
           │        SHUFFLE                │         SHUFFLE              │
           └───────────────────────────────┴──────────────────────────────┘
```

**Rules for stage creation:**
1. Start from the **final RDD** (the one on which the action was called)
2. Walk **backwards** through the lineage graph
3. Keep adding RDDs to the current stage while following **NarrowDependencies**
4. When you hit a **ShuffleDependency**, **cut** — the RDD on the other side starts a new parent stage
5. Recurse on the parent stages

**Stage numbering:** Since stages are determined backwards, the **last stage** (containing the final RDD) gets **id 0**.

### 5.3 Tasks: The Unit of Execution

Within each stage, the number of tasks is determined by the **number of partitions in the last RDD of the stage**.

```
Stage 2 (3 partitions in RDD_C)     →  3 ShuffleMapTasks
Stage 1 (2 partitions in RDD_F)     →  2 ShuffleMapTasks
Stage 0 (4 partitions in RDD_H)     →  4 ResultTasks
```

**Two types of tasks:**

| Task Type | When Used | What It Does |
|-----------|-----------|-------------|
| **ShuffleMapTask** | Stages that produce intermediate shuffle output | Computes partition data, partitions it by key, writes shuffle output to local disk for the next stage to fetch |
| **ResultTask** | The final stage that produces the job's result | Computes partition data and sends the result back to the driver |

**Analogy to MapReduce:**
- `ShuffleMapTask` ≈ Mapper (produces partitioned output for the next stage)
- `ResultTask` ≈ Reducer (produces the final result) when it reads from a shuffle, or Mapper when the stage has no parents

### 5.4 Pipelining Within a Stage

Within a single stage (all NarrowDependencies), Spark **pipelines** the computation. This is the key optimization that makes Spark much more efficient than approaches that materialize every intermediate RDD.

**Pipelining means: no intermediate data is stored.**

```
Without pipelining (bad):                    With pipelining (Spark):
─────────────────────────                    ──────────────────────

for record in input:                         for record in input:
    temp1 = f(record)                            result = g(f(record))
    store temp1                                  emit result
                                             
for record in temp1:                         // record, f(record) are immediately
    result = g(record)                       // garbage collected after g() runs
    emit result                              // No intermediate storage needed!
```

**At the record level, pipelining looks like this:**

```
Record 1 ──▶ f(record1) ──▶ g(f(record1)) ──▶ emit
Record 2 ──▶ f(record2) ──▶ g(f(record2)) ──▶ emit
Record 3 ──▶ f(record3) ──▶ g(f(record3)) ──▶ emit
   ...
```

Each record flows through the **entire chain** of transformations within the stage before the next record starts. This is equivalent to:

```scala
for (record <- records) {
    g(f(record))     // f and g are pipelined — no intermediate storage
}
```

**Why pipelining stops at shuffle boundaries:**
- With a ShuffleDependency, computing ONE partition of the child RDD requires data from ALL partitions of the parent RDD
- You can't stream records one-by-one from parent to child — you need the complete shuffle output first
- So shuffle data must be **materialized** (written to disk), creating a barrier between stages

**Exception — not all narrow dependencies can be fully pipelined:**

Some transformations within a narrow dependency require consuming ALL records before producing output (e.g., `sortByKey` within a partition, or `mapPartitions` where `f` needs all records). In these cases, intermediate results must be stored in memory within the task, but still no data moves across the network.

### 5.5 Complete Example: From Logical to Physical Plan

Let's trace a complete example:

```scala
val data1 = sc.parallelize(Array((1,'a'), (2,'b'), (3,'c'), (4,'d')), 3)
val hashPairs1 = data1.partitionBy(new HashPartitioner(3))

val data2 = sc.parallelize(Array((1,"A"), (2,"B"), (3,"C"), (4,"D")), 2)
val rangePairs2 = data2.map(x => (x._1, x._2.charAt(0)))

val data3 = sc.parallelize(Array((1,'X'), (2,'Y')), 2)

val rangePairs = rangePairs2.union(data3)

val result = hashPairs1.join(rangePairs)

result.foreach(println)    // ACTION triggers execution
```

**Logical Plan (DAG):**

```
ParallelCollectionRDD (data1)
     │ (narrow)
 ShuffledRDD (partitionBy → HashPartitioner(3))     ← shuffle boundary
     │
     │                  ParallelCollectionRDD (data2)
     │                       │ (narrow)
     │                  MappedRDD (map)
     │                       │                  ParallelCollectionRDD (data3)
     │                       │ (narrow)              │ (narrow)
     │                  UnionRDD (union) ◄────────────┘
     │                       │
     └───── join (cogroup) ──┘                    ← shuffle boundary (for UnionRDD side)
                  │ (narrow)
           MappedValuesRDD
                  │ (narrow)
          FlatMappedValuesRDD (final)
```

**Physical Plan (Stages):**

```
Stage 2: ParallelCollectionRDD → ShuffledRDD
         (3 ShuffleMapTasks — writes shuffle output)

Stage 1: ParallelCollectionRDD(data2) → MappedRDD ─┐
         ParallelCollectionRDD(data3) ──────────────┤→ UnionRDD
         (4 ShuffleMapTasks — writes shuffle output)

Stage 0: CoGroupedRDD → MappedValuesRDD → FlatMappedValuesRDD
         (3 ResultTasks — produces final output)
         [This stage starts AFTER Stages 1 and 2 complete]
```

**Execution order:**
1. **Stage 2** and **Stage 1** can execute **in parallel** (no dependency between them)
2. **Stage 0** waits for both Stage 1 and Stage 2 to complete
3. Stage 0 fetches shuffle output from Stages 1 and 2, then pipelines through CoGroupedRDD → MappedValuesRDD → FlatMappedValuesRDD

---

## 6. Job Creation and Scheduling

### 6.1 Jobs, Stages, and Tasks Hierarchy

```
Application
├── Job 1 (triggered by action 1, e.g., count())
│   ├── Stage 0 (ResultStage)
│   │   ├── ResultTask 0
│   │   ├── ResultTask 1
│   │   └── ResultTask 2
│   ├── Stage 1 (ShuffleMapStage)
│   │   ├── ShuffleMapTask 0
│   │   └── ShuffleMapTask 1
│   └── Stage 2 (ShuffleMapStage)
│       ├── ShuffleMapTask 0
│       ├── ShuffleMapTask 1
│       └── ShuffleMapTask 2
├── Job 2 (triggered by action 2, e.g., saveAsTextFile())
│   ├── Stage 3 (ResultStage)
│   └── Stage 4 (ShuffleMapStage)
└── ...
```

**Key relationships:**
- **One application** = one SparkContext = one driver program
- **Each `action()`** in the driver creates **one job**
- Each job is split into **stages** at shuffle boundaries
- Each stage contains **tasks** (one per output partition)
- A **TaskSet** is the collection of all tasks in a stage

### 6.2 How Actions Create Jobs

| Action | processPartition (per partition) | resultHandler (combine results) |
|--------|-------------------------------|-------------------------------|
| `reduce(func)` | Reduce records within partition → partial result | Reduce partial results across partitions |
| `collect()` | Return array of records | Concatenate all arrays |
| `count()` | Count records in partition | Sum all counts |
| `foreach(f)` | Apply f to each record | No combination needed |
| `take(n)` | Take up to n records | Take first n from results |
| `saveAsHadoopFile(path)` | Write records to HDFS | No result returned |
| `countByKey()` | Count per key in partition → Map | Merge maps |

### 6.3 The DAGScheduler: Orchestrating Execution

The DAGScheduler is the brain of Spark's execution engine. Here's the complete flow:

```
User calls action (e.g., rdd.count())
         │
         ▼
   DAGScheduler.runJob(rdd, processPartition, resultHandler)
         │
         ▼
   Create JobId, submit JobSubmitted event
         │
         ▼
   handleJobSubmitted():
     1. newStage() — walk backwards from final RDD,
        cut at ShuffleDependencies → create stage DAG
     2. submitStage(finalStage)
         │
         ▼
   submitStage(stage):
     1. Find missingParentStages (parent stages not yet computed)
     2. If parents missing → recursively submit parents first,
        add current stage to waitingStages
     3. If no parents missing → submitMissingTasks(stage)
         │
         ▼
   submitMissingTasks(stage):
     1. Create ShuffleMapTask or ResultTask for each partition
     2. Package into TaskSet
     3. Submit to TaskScheduler
         │
         ▼
   TaskScheduler.submitTasks(taskSet):
     1. Wrap in TaskSetManager
     2. Add to scheduling queue (FIFO or Fair)
     3. backend.reviveOffers() → DriverActor sends tasks to Executors
         │
         ▼
   Executor receives task, deserializes, runs compute() chain
```

### 6.4 Task Scheduling: FIFO vs Fair Scheduler

| Scheduler | Behavior | Use Case |
|-----------|----------|----------|
| **FIFO** (default) | First job submitted runs first. Later jobs wait. | Simple batch processing, single-user |
| **Fair** | All jobs get a fair share of resources. Jobs run concurrently. | Multi-user, interactive queries |

---

## 7. Shuffle Mechanisms

Shuffle is the most expensive operation in Spark. Let's understand how it works internally.

### 7.1 Hash Shuffle (Spark < 1.2)

In the original hash shuffle, each map task creates **one file per reducer**:

```
Map Task 0:  ┌── file_0_0 (for reducer 0)
             ├── file_0_1 (for reducer 1)
             └── file_0_2 (for reducer 2)

Map Task 1:  ┌── file_1_0 (for reducer 0)
             ├── file_1_1 (for reducer 1)
             └── file_1_2 (for reducer 2)

Map Task 2:  ┌── file_2_0 (for reducer 0)
             ├── file_2_1 (for reducer 1)
             └── file_2_2 (for reducer 2)

Total files = M × R  (M map tasks × R reducers)
```

**Problem:** With 1000 mappers and 1000 reducers → **1,000,000 files!** This creates enormous disk I/O, file system overhead, and memory pressure for file handles.

### 7.2 Sort Shuffle (Spark >= 1.2, Default)

Sort shuffle creates **one file per map task** with an index of offsets for each reducer:

```
Map Task 0:  ┌── shuffle_0.data  (single sorted file)
             └── shuffle_0.index (offsets for each reducer partition)
             
             shuffle_0.data:
             ┌─────────────────┬─────────────────┬─────────────────┐
             │  Reducer 0 data │  Reducer 1 data │  Reducer 2 data │
             └─────────────────┴─────────────────┴─────────────────┘
                    ▲                  ▲                  ▲
             index: 0                1024              2048

Total files = M × 2  (M map tasks × 2 files each: data + index)
```

**Uses TimSort** — a hybrid of merge sort and insertion sort, efficient for real-world data with existing order.

**Much better:** With 1000 mappers → only **2000 files** (vs. 1,000,000).

### 7.3 Why Shuffle Is Costly

```
┌──────────────────────────────────────────────────────────────────┐
│                    SHUFFLE COST BREAKDOWN                         │
│                                                                  │
│  1. DISK I/O                                                     │
│     - Map side: serialize + write shuffle output to local disk   │
│     - Reduce side: read fetched data from local disk             │
│                                                                  │
│  2. NETWORK COMMUNICATION                                        │
│     - Reduce tasks fetch shuffle data from ALL map tasks         │
│     - Data crosses the network (potentially across racks)        │
│                                                                  │
│  3. SERIALIZATION / DESERIALIZATION                              │
│     - Objects must be serialized for network transfer            │
│     - Deserialized on the reduce side                            │
│                                                                  │
│  4. BARRIER SYNCHRONIZATION                                      │
│     - ALL map tasks must complete before ANY reduce task starts   │
│     - One slow map task delays the entire next stage             │
│                                                                  │
│  5. MEMORY PRESSURE                                              │
│     - Shuffle buffers, sort buffers consume memory               │
│     - Can trigger spills to disk if memory is insufficient       │
└──────────────────────────────────────────────────────────────────┘
```

---

## 8. Worker and Executor Model

### 8.1 Hierarchy

```
Cluster
├── Worker 1 (one physical/virtual machine)
│   └── Executor 1 (one JVM process)
│       ├── Thread Pool
│       │   ├── Thread → Task A (one output partition)
│       │   ├── Thread → Task B (one output partition)
│       │   └── Thread → Task C (one output partition)
│       ├── Block Manager (manages cached RDD partitions)
│       └── Shuffle Manager (coordinates shuffle I/O)
│
├── Worker 2
│   └── Executor 2
│       ├── Thread Pool
│       │   ├── Thread → Task D
│       │   └── Thread → Task E
│       ├── Block Manager
│       └── Shuffle Manager
│
└── Worker N
    └── Executor N
        └── ...
```

### 8.2 Key Design Decisions

| Component | Design | Why |
|-----------|--------|-----|
| **Worker** | One machine | Physical resource boundary |
| **Executor** | One JVM process per Worker | Memory isolation, independent GC |
| **Task** | One thread within Executor | Lightweight — thread creation overhead is much lower than process creation |

**Each task is responsible for computing ONE output partition.** The total number of concurrent tasks an executor can run = number of cores allocated to it.

### 8.3 Block Manager

Each Executor has a **Block Manager** that manages:
- **Cached/persisted RDD partitions** (in memory and/or on disk)
- **Shuffle output blocks** (written by ShuffleMapTasks)
- **Broadcast variables** (shared read-only data sent to all executors)

The Block Manager coordinates with the **DAGScheduler** for:
- Determining data locality (schedule tasks where the data already is)
- Managing persistence levels (MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY, etc.)

---

## 9. Spark Tuning

### 9.1 Controlling Parallelism

```
More Partitions → More Tasks → More Parallelism → Faster Execution
(up to the number of available cores)
```

| What to Control | How to Control It | When to Use |
|----------------|-------------------|-------------|
| **Input partitions** | `sc.textFile(path, minPartitions)` | Increase if input is small but cluster is large |
| **Shuffle partitions** | `numPartitions` parameter in wide transforms | Default may be too few for large datasets |
| **Repartition** | `rdd.repartition(n)` or `rdd.coalesce(n)` | After filter reduces data size, or to increase parallelism |
| **Spark SQL shuffle partitions** | `spark.sql.shuffle.partitions` (default: 200) | Adjust based on data size |

### 9.2 Data Partitioning Strategies

**Why partition matters:** Controlling partitioning can **eliminate shuffles entirely** for subsequent operations.

```python
# BAD: Join without pre-partitioning → full shuffle on both sides
result = large_rdd.join(small_rdd)   # 2 shuffles

# GOOD: Pre-partition large RDD, then join → shuffle only small RDD
large_rdd = large_rdd.partitionBy(HashPartitioner(100)).persist()
result = large_rdd.join(small_rdd)   # only 1 shuffle (small_rdd)

# BEST: Pre-partition both → no shuffle at all
large_rdd = large_rdd.partitionBy(HashPartitioner(100)).persist()
small_rdd = small_rdd.partitionBy(HashPartitioner(100)).persist()
result = large_rdd.join(small_rdd)   # 0 shuffles (co-partitioned!)
```

**Spark tracks partitioners:**
- Transformations like `partitionBy`, `groupByKey`, `reduceByKey`, `join`, `sort` **set** a partitioner on the output RDD
- `map()` and `flatMap()` **unset** the partitioner (because the user function might change the key)
- `filter()`, `mapValues()`, `flatMapValues()` **retain** the partitioner (they don't change the key)
- You can check with `rdd.partitioner` — returns `Some(HashPartitioner(n))` or `None`

### 9.3 Built-in Partitioners

| Partitioner | How It Works | When to Use |
|-------------|-------------|-------------|
| **HashPartitioner** | `partition = hash(key) % numPartitions` | Default. Good for uniformly distributed keys |
| **RangePartitioner** | Samples data, creates roughly equal key ranges per partition | `sortByKey()`. Good when you need ordered output |
| **Custom Partitioner** | User defines `numPartitions` and `getPartition(key)` | Domain-specific co-location (e.g., partition URLs by domain) |

### 9.4 Caching and Persistence

```python
# Cache in memory (default: MEMORY_ONLY)
rdd.cache()              # Alias for persist(MEMORY_ONLY)

# Persist with specific storage level
rdd.persist(StorageLevel.MEMORY_AND_DISK)
rdd.persist(StorageLevel.DISK_ONLY)
rdd.persist(StorageLevel.MEMORY_ONLY_SER)     # Serialized, more compact
rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)
```

**When to cache/persist:**
- ✅ RDD is used **across multiple actions** (e.g., iterative algorithms like PageRank)
- ✅ RDD is **expensive to recompute** (long chain of transformations, or involves a shuffle)
- ✅ RDD is used by **multiple downstream transformations** that branch the DAG
- ❌ RDD is too large to fit in memory (will cause eviction thrashing)
- ❌ RDD is cheap to recompute (e.g., simple map on an in-memory dataset)

**Eviction policy:** LRU (Least Recently Used) at the RDD partition level. If a new partition can't fit, the least recently used partition from a **different RDD** is evicted. Partitions from the same RDD are not evicted (to avoid cycling).

### 9.5 Memory Allocation

```
┌─────────────────────────────────────────────────────────┐
│                 Executor JVM Heap                        │
│                                                         │
│  ┌──────────────────────────────────────────────────┐   │
│  │         Unified Memory (60% of heap)              │   │
│  │                                                    │   │
│  │  ┌──────────────────┐ ┌────────────────────────┐  │   │
│  │  │ Execution Memory │ │    Storage Memory      │  │   │
│  │  │  (shuffle, join,  │ │  (cached RDDs,         │  │   │
│  │  │   sort buffers)   │ │   broadcast vars)      │  │   │
│  │  │                  │ │                        │  │   │
│  │  │  ◄── can borrow ──►                        │  │   │
│  │  │      from each   ──►                        │  │   │
│  │  │      other        │ │                        │  │   │
│  │  └──────────────────┘ └────────────────────────┘  │   │
│  └──────────────────────────────────────────────────┘   │
│                                                         │
│  ┌────────────────────────────────┐                     │
│  │   User Memory (40% of heap)    │ ← User data        │
│  │   + Reserved Memory (300 MB)   │ ← Buffer for OOM   │
│  └────────────────────────────────┘                     │
└─────────────────────────────────────────────────────────┘
```

- **Execution memory** and **Storage memory** can **borrow from each other** dynamically
- If execution needs more memory and storage has unused space, execution can borrow it (and vice versa)
- Execution memory can evict storage (cached data can be recomputed), but storage cannot evict execution (shuffle data cannot be recomputed without re-running the task)

### 9.6 Static vs. Dynamic Resource Allocation

| Mode | Behavior | Trade-off |
|------|----------|-----------|
| **Static** | Fixed number of executors allocated at submission time | Guaranteed resources, but wastes resources during idle phases |
| **Dynamic** | Executors added/removed based on task queue demand and idle time | Better utilization, but startup latency when scaling up |

---

## 10. RDD Fault Tolerance via Lineage

One of the most elegant aspects of Spark's design is how it handles fault tolerance through **lineage** rather than data replication.

### 10.1 How Lineage-Based Recovery Works

```
Original execution:
  HDFS Block → RDD_A → RDD_B → RDD_C → RDD_D (result)
                (map)   (filter)  (map)

If a partition of RDD_C is lost (executor crash):
  1. Spark checks RDD_C's lineage: "I was created by filter() on RDD_B"
  2. Checks RDD_B's lineage: "I was created by map() on RDD_A"
  3. Checks RDD_A's lineage: "I was created from HDFS block X"
  4. Recomputes: read HDFS block X → map → filter → RDD_C partition recovered!

Only the LOST PARTITION is recomputed, not the entire RDD.
Recomputation can happen on a DIFFERENT node.
```

### 10.2 Narrow vs. Wide Dependencies and Fault Recovery

```
NARROW DEPENDENCY RECOVERY (Cheap):
  Lost: Partition 2 of RDD_C
  Need to recompute: Only Partition 2 of RDD_B → Partition 2 of RDD_C
  Other partitions: UNAFFECTED

WIDE DEPENDENCY RECOVERY (Expensive):
  Lost: Partition 2 of ShuffledRDD
  Need to recompute: ALL partitions of the parent RDD that contributed
                     to Partition 2 of ShuffledRDD
  This means: Re-run ALL map tasks of the parent stage!
  That's why: Shuffle output is WRITTEN TO DISK (not just kept in memory)
              → so it can survive executor failures without full re-execution
```

### 10.3 When to Checkpoint

For very long lineage chains (e.g., iterative algorithms with hundreds of iterations), recomputation from the beginning would be too expensive. In such cases, use **checkpointing**:

```python
# Save RDD to reliable storage (HDFS), truncating its lineage
sc.setCheckpointDir("hdfs://...")
rdd.checkpoint()    # Must be called before any action on this RDD
```

After checkpointing, the RDD's lineage is replaced with "I was loaded from HDFS checkpoint", so recovery is fast regardless of the original lineage length.

**Difference from persist/cache:**
- `cache()/persist()` stores data in executor memory/disk — **lost if executor dies**
- `checkpoint()` stores data in HDFS — **survives executor failures** — but is slower to write

---

## 11. Putting It All Together: Complete Execution Walkthrough

Let's trace the complete execution of a word count job:

```python
lines = sc.textFile("hdfs://input.txt")           # HadoopRDD (4 partitions)
words = lines.flatMap(lambda l: l.split())         # FlatMappedRDD (narrow)
pairs = words.map(lambda w: (w, 1))                # MappedRDD (narrow)
counts = pairs.reduceByKey(lambda a, b: a + b)     # ShuffledRDD + MapPartitionsRDD (wide)
counts.saveAsTextFile("hdfs://output")             # ACTION → triggers execution
```

### Step 1: Build Logical Plan

```
HadoopRDD ──(narrow)──▶ FlatMappedRDD ──(narrow)──▶ MappedRDD ──(SHUFFLE)──▶ ShuffledRDD ──(narrow)──▶ MapPartitionsRDD
                                                                                                              │
                                                                                                         saveAsTextFile
```

### Step 2: Create Physical Plan (Stages)

```
Stage 1 (ShuffleMapStage):                     Stage 0 (ResultStage):
┌────────────────────────────────────┐        ┌────────────────────────────┐
│ HadoopRDD                         │        │ ShuffledRDD                │
│   │ flatMap (pipelined)            │        │   │ mapPartitions          │
│ FlatMappedRDD                     │        │ MapPartitionsRDD           │
│   │ map (pipelined)               │        │   │ saveAsTextFile         │
│ MappedRDD                         │──────▶ │ (write to HDFS)           │
│   │ map-side combine (pipelined)  │shuffle │                            │
│ MapPartitionsRDD                  │        │ 4 ResultTasks              │
│                                    │        └────────────────────────────┘
│ 4 ShuffleMapTasks                 │
└────────────────────────────────────┘
```

### Step 3: Execute Stage 1 (4 ShuffleMapTasks in parallel)

```
Task 0 (on Worker A):
  Read HDFS block 0
  → flatMap(split) → pipelined, no intermediate storage
  → map(w → (w,1)) → pipelined
  → map-side combine: {(hello,3), (world,2), ...}
  → Write shuffle output to local disk (sorted by partition key)

Task 1 (on Worker B): same for HDFS block 1
Task 2 (on Worker C): same for HDFS block 2
Task 3 (on Worker A): same for HDFS block 3

All 4 tasks run in parallel. Records are PIPELINED (streamed one-by-one
through flatMap → map → combine). No intermediate RDD is materialized.
```

### Step 4: Execute Stage 0 (4 ResultTasks, after Stage 1 completes)

```
Task 0 (on Worker B):
  Fetch shuffle partition 0 from all map tasks (across network)
  → Merge-combine: reduce values for each key
  → Write result partition to HDFS

Task 1 (on Worker C): same for shuffle partition 1
Task 2 (on Worker A): same for shuffle partition 2
Task 3 (on Worker B): same for shuffle partition 3
```

### Step 5: Job Complete

All 4 result partitions written to HDFS. `saveAsTextFile` returns.

---

## 12. Summary: Key Concepts Cheat Sheet

### Dependencies:

```
┌─────────────────────────────────────────────────────────────────────┐
│  NARROW (no shuffle)              │  WIDE (shuffle required)        │
│  ─────────────────               │  ────────────────────           │
│  map, filter, flatMap            │  groupByKey, reduceByKey        │
│  mapPartitions, mapValues        │  sortByKey, distinct            │
│  union, coalesce(shuffle=false)  │  join (non-co-partitioned)      │
│  co-partitioned join/cogroup     │  repartition, coalesce(true)    │
│  cartesian                       │  intersection                   │
│                                  │                                  │
│  ✓ Pipelined within stage        │  ✗ Barrier between stages       │
│  ✓ No network I/O               │  ✗ Disk + network I/O           │
│  ✓ Cheap fault recovery          │  ✗ Expensive fault recovery     │
└─────────────────────────────────────────────────────────────────────┘
```

### Execution Hierarchy:

```
Application ──has──▶ Jobs ──split into──▶ Stages ──contain──▶ Tasks
   │                  │                    │                    │
   │                  │                    │                    │
 1 per              1 per               Cut at               1 per
 SparkContext       action()            shuffle              output
                                        boundaries           partition
```

### Performance Rules:

1. **Minimize shuffles** — they are by far the most expensive operation
2. **Use `reduceByKey` over `groupByKey`** — map-side combine reduces shuffle data
3. **Pre-partition and persist** — co-partitioned RDDs avoid shuffles on joins
4. **Cache wisely** — cache RDDs reused across actions, not one-time RDDs
5. **Control partition count** — too few = underutilized cluster; too many = scheduling overhead
6. **Use `mapValues`/`flatMapValues`** instead of `map` when possible — preserves partitioner
7. **Checkpoint long lineage chains** — prevents expensive recomputation in iterative algorithms
8. **Pipeline-friendly transformations** (map, filter) are essentially free within a stage

---

## References

1. Zaharia, M., Chowdhury, M., Das, T., Dave, A., Ma, J., McCauley, M., Franklin, M.J., Shenker, S., Stoica, I., "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing", USENIX NSDI, 2012

2. Armbrust, M., Xin, R.S., Lian, C., Huai, Y., Liu, D., Bradley, J.K., Meng, X., Kaftan, T., Franklin, M.J., Ghodsi, A., Zaharia, M., "Spark SQL: Relational Data Processing in Spark", ACM SIGMOD, 2015

3. Lijie Xu (Jerry Lead), "Spark Internals", https://github.com/JerryLead/SparkInternals
   - Chapter 2: Job Logical Plan
   - Chapter 3: Job Physical Plan

4. Jacek Laskowski, "The Internals of Apache Spark", https://books.japila.pl/apache-spark-internals/overview/

5. Karau, H., Konwinski, A., Wendell, P., Zaharia, M., "Learning Spark: Lightning-Fast Big Data Analysis", O'Reilly Media, 2nd Edition, Chapter 7

6. Apache Spark Documentation:
   - RDD Programming Guide: https://spark.apache.org/docs/latest/rdd-programming-guide.html
   - Spark Configuration: https://spark.apache.org/docs/latest/configuration.html

7. Course Materials:
   - Lecture Slides: DS256 L2.3 — Big Data Processing with Apache Spark
   - Professor: Yogesh Simmhan, IISc Bangalore
