# Lecture 2.4: Spark DataFrames, SQL & Catalyst Optimizer

## DS256 - Scalable Systems for Data Science
### Module 2: Processing Large Volumes of Big Data

---

## 1. Limitations of Spark RDD

Before understanding DataFrames, it is critical to understand **why** they were needed — i.e., the fundamental limitations of the RDD abstraction that motivated a higher-level API.

### 1.1 Why RDDs Fall Short

Spark RDDs only offer high-level constructs on **iterations over RDD items** and **invocation patterns**. The core problems are:

```
┌──────────────────────────────────────────────────────────────────────┐
│                    LIMITATIONS OF SPARK RDDs                         │
│                                                                      │
│  1. OPAQUE LAMBDA EXPRESSIONS                                       │
│     ─────────────────────────                                       │
│     • User functions (lambdas) are "black boxes" to Spark           │
│     • Spark cannot inspect, analyze, or optimize them               │
│     • No automatic query optimization is possible                   │
│                                                                      │
│  2. OPAQUE TYPES                                                    │
│     ──────────────                                                  │
│     • RDD elements are arbitrary Java/Python objects                │
│     • Spark only knows they are homogeneous (same type)             │
│     • No type-specific behavior or storage optimization             │
│                                                                      │
│  3. IMPERATIVE PROGRAMMING MODEL                                    │
│     ────────────────────────────                                    │
│     • Users tell Spark HOW to execute (step by step)                │
│     • Relies entirely on users to optimize code                     │
│     • Spark engine has no freedom to rearrange operations           │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

**Example of the Problem:**

```python
# RDD approach — Spark sees opaque functions, cannot optimize
rdd.filter(lambda x: x.age > 21) \
   .map(lambda x: (x.name, x.age)) \
   .reduceByKey(lambda a, b: a + b)

# Spark has NO IDEA:
#   - What "x.age > 21" means (it's just arbitrary Python code)
#   - That only "name" and "age" columns are needed (not all fields)
#   - How to reorder or combine these operations for efficiency
```

Because Spark cannot look inside user functions, it cannot apply classic database optimizations like **predicate pushdown**, **projection pruning**, or **constant folding** — optimizations that relational databases have used for decades.

---

## 2. DataFrames

### 2.1 What Are DataFrames?

**DataFrames** are a higher-level abstraction built on top of RDDs, inspired by the `pandas` DataFrame in Python and data frames in R. While RDDs were inspired by Python native operators like `map`, DataFrames bring a **declarative, SQL-like** programming model to Spark.

```
┌──────────────────────────────────────────────────────────────────────┐
│                      RDD vs. DataFrame                               │
│                                                                      │
│   RDD:                           DataFrame:                         │
│   ────                           ──────────                         │
│   • "How to do it"               • "What to do"                    │
│   • Opaque user functions        • SQL-like DSL operators           │
│   • Row-based                    • Row AND column based             │
│   • User must optimize           • Spark optimizes for you          │
│   • Different APIs per language  • Uniform across languages         │
│                                                                      │
│   Key Insight: DataFrames let Spark PARSE your query,               │
│   UNDERSTAND your intention, and OPTIMIZE execution.                │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

**Core Design Principles:**
- **More expressive and simpler**: Compose SQL-like queries using a high-level Domain Specific Language (DSL) and APIs
- **Tell Spark what to do**: Spark parses the query, understands the intention, and optimizes/arranges operations for efficient execution
- **Uniform across language bindings**: Avoids user-defined opaque code as much as possible, so Python, Scala, Java, and R all get the same performance
- **Interoperable**: You can drop down to the RDD level whenever you need fine-grained control

### 2.2 RDD vs. Datasets vs. DataFrames

```
┌──────────────────────────────────────────────────────────────────────┐
│                 RDD vs. Dataset vs. DataFrame                        │
│                                                                      │
│  ┌─────────┐    ┌───────────┐    ┌─────────────┐                    │
│  │   RDD   │    │  Dataset   │    │  DataFrame  │                    │
│  └─────────┘    └───────────┘    └─────────────┘                    │
│                                                                      │
│  Immutable       Immutable         Immutable                        │
│  Distributed     Distributed       Distributed                      │
│  Collection      Collection        Collection                       │
│                                                                      │
│  • Lower-level   • Strongly-typed  • Data organized into            │
│  • More control     (Java/Scala      named columns                  │
│  • More coding      only)          • Imposes structure              │
│    required      • Easier debug    • Domain specific                │
│  • Row-based     • Compact           language API                   │
│                    bytecode        • Untyped at compile             │
│                    (Tungsten)        time (types checked             │
│                                      at runtime)                    │
│                                                                      │
│  PERFORMANCE: DataFrame ≥ Dataset >> RDD                            │
│  Both DataFrame and Dataset use Catalyst Optimizer + Tungsten       │
│                                                                      │
│  INTEROPERABILITY: dataset.rdd.take(10)  ← Move between them       │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

**Static Typing and Runtime Type Safety:**
- **Dataset**: Easier to debug due to strong typing (compile-time type checks in Java/Scala)
- **RDD** on Java/Scala: Also strongly typed, but no optimizer benefits
- **DataFrame**: Untyped at compile time — types are checked at runtime, but faster for interactive/ad-hoc queries

**Performance Hierarchy:**
- **Dataset and DataFrame** use SparkSQL's **Catalyst optimizer** for speed and space efficiency
- **Dataset** is efficient for batch execution with specialized **Tungsten serialization/deserialization** and compact bytecode
- **DataFrame** is untyped and **faster for interactive** workloads
- **RDD** gives the most **fine-grained control** but leaves optimization entirely to the user

---

## 3. Declarative Programming

DataFrames embody the **declarative programming** paradigm — the same philosophy used by SQL and relational database management systems (RDBMS).

```
┌──────────────────────────────────────────────────────────────────────┐
│          IMPERATIVE vs. DECLARATIVE PROGRAMMING                      │
│                                                                      │
│  Imperative (RDD):                Declarative (DataFrame/SQL):      │
│  ─────────────────                ────────────────────────────       │
│  "Tell HOW to do it"              "Tell WHAT to do"                 │
│                                                                      │
│  rdd.filter(lambda x: x > 3)     SELECT Part, Items                │
│     .map(lambda x: (x, 1))       FROM Widget                       │
│     .reduceByKey(add)             WHERE Part = 'Bolts'              │
│                                                                      │
│  User specifies each step.        System determines the best        │
│  User must optimize.              execution plan using the          │
│                                   schema of the data/table.         │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

**Why Declarative Matters:**
- The **system** (Spark's Catalyst optimizer) determines the best execution plan
- Uses the **schema** (column names, types, sizes) for the data to plan execution
- Enables automatic optimizations like predicate pushdown, column pruning, join reordering, etc.

---

## 4. Working with DataFrames

### 4.1 DataFrames as Distributed In-Memory Tables

DataFrames are **distributed in-memory tables** with named columns and schemas.

**Key Properties:**
- Schema can be **defined explicitly** by the user
- **Immutable** — Spark keeps a lineage of all transformations (just like RDDs)
- Adding or changing column names/types **creates new DataFrames** while previous versions are preserved
- Backed by RDDs under the hood

### 4.2 Defining Schema Using DDL

You can define a schema explicitly using DDL (Data Definition Language) syntax:

```python
# Define schema using DDL string
schema = "name STRING, age INT, city STRING"

# Or using StructType
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("city", StringType(), True)
])

df = spark.createDataFrame(data, schema)
```

### 4.3 Projections and Filters

**Projections** select specific columns, and **filters** select specific rows:

```python
# Projection: Select specific columns
df.select("name", "age")

# Filter: Select specific rows
df.where(df.age > 21)
# or equivalently
df.filter(df.age > 21)

# Combine projection and filter
df.select("name", "age").where(df.age > 21)
```

### 4.4 Aggregation and Join

```python
# Aggregation with groupBy
# Note: count() here is the aggregator for groupBy(), NOT the action df.count()
df.groupBy("city").count()

# Join two DataFrames
airports = ...  # DataFrame with City, State, Country, IATA columns
flights = ...   # DataFrame with Origin, Destination, Date, Delay, Distance, Airline columns

# Join flights with airport info
flights.join(airports, flights.Origin == airports.IATA)
```

**Example Data Model:**

```
┌──────────────────────────────────────────────────────────────────────┐
│  Airports Table:                                                     │
│  ┌──────────┬───────┬─────────┬──────┐                               │
│  │  City    │ State │ Country │ IATA │                               │
│  ├──────────┼───────┼─────────┼──────┤                               │
│  │ Seattle  │  WA   │  USA    │ SEA  │                               │
│  │ New York │  NY   │  USA    │ JFK  │                               │
│  │ Bangalore│  KA   │  India  │ BLR  │                               │
│  └──────────┴───────┴─────────┴──────┘                               │
│                                                                      │
│  Flights Table:                                                      │
│  ┌────────┬──────┬────────────┬───────┬──────────┬─────────┐         │
│  │ Origin │ Dest │    Date    │ Delay │ Distance │ Airline │         │
│  ├────────┼──────┼────────────┼───────┼──────────┼─────────┤         │
│  │  SEA   │ SFO  │ 01010710   │  31   │   590    │   AA    │         │
│  │  SEA   │ SFO  │ 01010955   │  104  │   590    │   UA    │         │
│  │  SEA   │ SFO  │ 01010730   │   5   │   590    │   AA    │         │
│  │  LAX   │ SFO  │ 01010600   │  15   │   400    │   DL    │         │
│  └────────┴──────┴────────────┴───────┴──────────┴─────────┘         │
└──────────────────────────────────────────────────────────────────────┘
```

### 4.5 Lazy Evaluation in DataFrames

Just like RDDs, DataFrames use **lazy evaluation**:
- Transformations (select, filter, join, groupBy) build up a **logical plan** but do **not** execute anything
- Execution only occurs when an **action** (output operation) is called, such as `show()`, `count()`, `save()`, or `collect()`
- This allows the Catalyst optimizer to see the **entire plan** and optimize across all operations

### 4.6 Immutability vs. Modifications

DataFrames themselves are **immutable** (backed by immutable RDDs). However, you can create **new, different DataFrames** from existing ones:

```python
# Add a column — creates a NEW DataFrame
df_with_status = df.withColumn("status", df.delay > 0)

# Drop columns
df_slim = df.drop("delay")

# Rename columns
df_renamed = df.withColumnRenamed("delay", "flight_delay")
```

**Important:** ACID (Atomicity, Consistency, Isolation, Durability) properties do **not** apply since DataFrames are immutable — there are no in-place updates to worry about.

---

## 5. Spark SQL

### 5.1 Overview

**Spark SQL** provides an **ANSI SQL:2003-compatible** query interface over structured data with a schema. It allows users to write standard SQL queries that Spark then optimizes and executes.

**Key Features:**
- Permits abstraction to DataFrames/Datasets
- Connects to **Apache Hive**, **JSON**, **CSV**, **Parquet**, and other data sources
- Accessible via **JDBC/ODBC** and a **SQL Shell**
- Generates **optimized query plans** via the Catalyst optimizer

```python
# Register a DataFrame as a temporary view
df.createOrReplaceTempView("flights")

# Run SQL queries directly
result = spark.sql("""
    SELECT Origin, avg(Delay) as avg_delay 
    FROM flights 
    WHERE Distance > 500 
    GROUP BY Origin
""")
```

### 5.2 Joins in Spark SQL

Spark SQL supports all standard join types:

```python
# SQL-style join
spark.sql("""
    SELECT f.*, a.City, a.Country
    FROM flights f
    JOIN airports a ON f.Origin = a.IATA
""")

# DataFrame API join
flights.join(airports, flights.Origin == airports.IATA, "inner")
```

---

## 6. Execution Model

### 6.1 From High-Level APIs to Low-Level Execution

The computation expressed in high-level DataFrame or SQL APIs is **decomposed into low-level optimized and generated RDD operations**:

```
┌──────────────────────────────────────────────────────────────────────┐
│                       EXECUTION MODEL                                │
│                                                                      │
│  User Code (DataFrame / SQL)                                        │
│         │                                                            │
│         ▼                                                            │
│  ┌─────────────────┐                                                │
│  │  Catalyst        │  Parse → Analyze → Optimize → Plan            │
│  │  Optimizer        │                                               │
│  └────────┬─────────┘                                                │
│           │                                                          │
│           ▼                                                          │
│  ┌─────────────────┐                                                │
│  │  Generated RDD   │  Optimized, compact, single-function          │
│  │  Operations      │  RDD code                                     │
│  └────────┬─────────┘                                                │
│           │                                                          │
│           ▼                                                          │
│  ┌─────────────────┐                                                │
│  │  Scala Bytecode  │  Compiled for executors' JVMs                 │
│  └─────────────────┘                                                │
│                                                                      │
│  IMPORTANT:                                                         │
│  • Generated RDD operation code is NOT accessible to users          │
│  • These RDD ops are NOT the same as the user-facing RDD APIs       │
│  • They are internal, optimized operations generated by Catalyst    │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

---

## 7. Catalyst Optimizer — Overview

The **Catalyst Optimizer** is the heart of Spark SQL's execution engine. It is what makes DataFrames and Spark SQL fast — it takes user operations and transforms them into an optimized execution plan.

### 7.1 Core Idea

```
┌──────────────────────────────────────────────────────────────────────┐
│                     CATALYST OPTIMIZER                                │
│                                                                      │
│  • DataFrames keep track of their schema                            │
│  • DataFrames support various relational operations                 │
│  • A DataFrame represents a LOGICAL PLAN to compute a dataset      │
│  • NO execution occurs until an "action" output operation is        │
│    called (e.g., save, show, collect)                               │
│  • Enables RICH OPTIMIZATION across ALL operations used to          │
│    build the DataFrame                                              │
│  • User operations are captured using an Abstract Syntax Tree       │
│    (AST) rather than opaque Python/Scala functions                  │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

### 7.2 SQL Stack and Query Planning at a Glance

The Catalyst optimizer transforms a SQL or DataFrame expression through multiple phases to arrive at optimized physical execution:

```
┌──────────────────────────────────────────────────────────────────────┐
│                   QUERY PLANNING PIPELINE                            │
│                                                                      │
│   SQL Query ─────┐                                                  │
│                  ▼                                                   │
│   DataFrame ──▶ Unresolved    ──▶  Logical   ──▶  Optimized         │
│   DSL           Logical Plan       Plan           Logical Plan      │
│                                                                      │
│                      ▲                               │               │
│                      │                               ▼               │
│                   Catalog            ┌──────────────────────────┐    │
│                                      │  Physical   Physical     │    │
│                                      │  Plan 1     Plan 2  ... │    │
│                                      └───────────┬──────────────┘   │
│                                                  │                   │
│                                           Cost Model                │
│                                                  │                   │
│                                                  ▼                   │
│                                        Selected Physical Plan       │
│                                                  │                   │
│                                           Code Generation           │
│                                                  │                   │
│                                                  ▼                   │
│                                               RDDs                  │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

The pipeline consists of **four major phases**: Analysis, Logical Optimization, Physical Planning, and Code Generation. Each phase transforms a tree of nodes — expressions, logical operators, or physical operators.

---

## 8. Catalyst Optimizer — Deep Dive (from SIGMOD 2015 Paper)

*Reference: "Spark SQL: Relational Data Processing in Spark," Michael Armbrust et al., ACM SIGMOD 2015.*

The Catalyst optimizer is an **extensible query optimizer** built using functional programming constructs in Scala. Its design had two key motivations:
1. Make it easy to **add new optimization techniques and features** to Spark SQL
2. Enable **external developers to extend** the optimizer (e.g., data-source-specific rules, new data types)

Unlike previous extensible optimizers that required a complex domain-specific language (DSL) for specifying rules and an "optimizer compiler" to translate them into executable code, Catalyst uses **standard Scala features** — particularly **pattern matching** — allowing developers to write optimization rules in a full programming language while keeping them concise and readable.

### 8.1 Trees — The Core Data Structure

At its core, Catalyst represents everything as **trees**. The main data type is a tree composed of **node objects**, where each node has a **node type** and **zero or more children**. New node types are defined as subclasses of a `TreeNode` class. These tree objects are **immutable** and can be manipulated using **functional transformations**.

**Example — A Simple Expression Language:**

Consider three node classes for a simple expression language:
- `Literal(value: Int)` — a constant value
- `Attribute(name: String)` — an attribute from an input row (e.g., `"x"`)
- `Add(left: TreeNode, right: TreeNode)` — sum of two expressions

The expression `x + (1 + 2)` is represented as a tree:

```
┌──────────────────────────────────────────────────────────────────────┐
│                  TREE FOR: x + (1 + 2)                               │
│                                                                      │
│                       Add                                           │
│                      /   \                                          │
│              Attribute    Add                                       │
│              (name="x")  /   \                                      │
│                     Literal  Literal                                │
│                     (val=1)  (val=2)                                │
│                                                                      │
│  In Scala: Add(Attribute(x), Add(Literal(1), Literal(2)))          │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

This tree representation is used for **everything** in Catalyst: expressions, logical plans, physical plans — they are all trees of typed nodes.

### 8.2 Rules — Transforming Trees

**Rules** are functions that transform one tree into another. While a rule can run arbitrary code on the tree (since it is just a Scala object), the most common approach is to use **pattern matching** functions that **find and replace subtrees** with a specific structure.

Trees offer a `transform` method that applies a pattern-matching function **recursively on all nodes** of the tree, transforming the ones that match each pattern to a result.

**Example — Constant Folding Rule:**

```scala
// Rule: If we see Add(Literal(c1), Literal(c2)), replace with Literal(c1+c2)
tree.transform {
  case Add(Literal(c1), Literal(c2)) => Literal(c1 + c2)
}

// Applied to x + (1 + 2):
//
//        Add                        Add
//       /   \          ──▶         /   \
//  Attr(x)  Add               Attr(x)  Literal(3)
//          /   \
//     Lit(1)  Lit(2)
//
// Result: x + 3
```

**Key Properties of Rules and Pattern Matching:**

1. **Partial functions**: A rule only needs to match a **subset** of all possible input trees. Catalyst automatically **skips and descends into** subtrees that do not match. This means rules are **modular** — they only reason about the cases where a given optimization applies.

2. **Multiple patterns in one transform call**: Rules can match multiple patterns simultaneously, making it concise to implement several transformations at once:

```scala
tree.transform {
  case Add(Literal(c1), Literal(c2)) => Literal(c1 + c2)   // Constant folding
  case Add(left, Literal(0))         => left                 // x + 0 = x
  case Add(Literal(0), right)        => right                // 0 + x = x
}
```

3. **Batches and fixed-point execution**: Rules are grouped into **batches**. Each batch is executed repeatedly until the tree reaches a **fixed point** — i.e., the tree stops changing after applying its rules. This means each rule can be **simple and self-contained**, yet still achieve larger **global effects** through repeated application. For example, repeated application would constant-fold `(x+0)+(3+3)` → `(x)+(6)` → `x+6`.

4. **Sanity checks**: After each batch, developers can run sanity checks on the new tree (e.g., verifying that all attributes have been assigned types), often also written via recursive matching.

5. **Arbitrary Scala code**: Rule conditions and bodies can contain **arbitrary Scala code**, giving Catalyst more power than domain-specific optimizer languages, while keeping simple rules concise.

6. **Easy to reason about and debug**: Functional transformations on immutable trees make the entire optimizer easy to understand and debug. They also enable parallelization (though this is not yet exploited).

### 8.3 The Four Phases of Query Planning

Catalyst uses its tree transformation framework in **four phases**:

```
┌──────────────────────────────────────────────────────────────────────┐
│               FOUR PHASES OF CATALYST QUERY PLANNING                 │
│                                                                      │
│   ┌──────────┐   ┌──────────┐   ┌──────────┐   ┌──────────────┐    │
│   │ Analysis │──▶│ Logical  │──▶│ Physical │──▶│    Code      │    │
│   │          │   │Optimiz.  │   │ Planning │   │ Generation   │    │
│   └──────────┘   └──────────┘   └──────────┘   └──────────────┘    │
│                                                                      │
│   Resolve       Apply rule-     Generate        Compile parts       │
│   references    based           physical        of the query        │
│   using         optimizations   plans; use      to Java             │
│   catalog                       cost model      bytecode            │
│                                 to select                           │
│                                                                      │
│   Purely        Purely          Rule-based      Code                │
│   Rule-based    Rule-based      + Cost-based    Generation          │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

Each phase uses **different types of tree nodes** — Catalyst includes libraries for expressions, data types, and logical and physical operators.

---

#### 8.3.1 Phase 1: Analysis

**Goal:** Resolve all references in the logical plan — figure out what each column name refers to and what type it has.

Spark SQL begins with a relation to be computed, either from:
- An **Abstract Syntax Tree (AST)** returned by a SQL parser, or
- A **DataFrame** object constructed using the API

In both cases, the relation may contain **unresolved attribute references or relations**. For example, in `SELECT col FROM sales`, the type of `col`, or even whether it is a valid column name, is not known until we look up the table `sales`.

An attribute is **unresolved** if we do not know its type or have not matched it to an input table (or an alias).

**What the Analyzer Does:**

The analyzer uses Catalyst rules and a **Catalog** object (which tracks all tables in all data sources) to resolve attributes. It builds an "unresolved logical plan" tree and then applies rules that:

```
┌──────────────────────────────────────────────────────────────────────┐
│                      ANALYSIS RULES                                  │
│                                                                      │
│  1. LOOK UP RELATIONS BY NAME                                       │
│     Find the actual table/data source from the catalog              │
│     e.g., "sales" → the actual sales table metadata                 │
│                                                                      │
│  2. MAP NAMED ATTRIBUTES                                            │
│     Map attributes like "col" to the actual input provided          │
│     by a given operator's children                                  │
│     e.g., "col" → column #3 of the "sales" table                   │
│                                                                      │
│  3. DETERMINE ATTRIBUTE IDENTITY                                    │
│     Determine which attributes refer to the same value and          │
│     give them a unique ID                                           │
│     e.g., "col = col" → both refer to the same column              │
│     (enables optimizations like eliminating redundant checks)       │
│                                                                      │
│  4. PROPAGATE AND COERCE TYPES                                      │
│     Propagate and coerce types through expressions                  │
│     e.g., "1 + col" — we cannot know the return type of this       │
│     until we resolve "col" and possibly cast subexpressions         │
│     to compatible types (e.g., INT + FLOAT → FLOAT)                │
│                                                                      │
│  Total: ~1000 lines of code for all analyzer rules                  │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

**Example:**

```sql
SELECT (col + 1) FROM sales
```

The analyzer must:
1. Look up `sales` in the catalog → find the table
2. Find `col` in the `sales` table → resolve it to a specific column
3. Determine the type of `col` (e.g., `INT`)
4. Infer the type of `col + 1` (e.g., `INT + INT → INT`)
5. Can also infer schema for semi-structured data (e.g., JSON)

---

#### 8.3.2 Phase 2: Logical Optimization

**Goal:** Apply standard rule-based optimizations to simplify and improve the logical plan.

This phase applies well-known database optimization techniques. These include:

```
┌──────────────────────────────────────────────────────────────────────┐
│                 LOGICAL OPTIMIZATION RULES                           │
│                                                                      │
│  1. CONSTANT FOLDING                                                │
│     Replace expressions with constants when possible                │
│     e.g., 1 + 2 → 3                                                │
│                                                                      │
│  2. PREDICATE PUSHDOWN                                              │
│     Push filter conditions as close to the data source as possible  │
│     e.g., Filter AFTER Join → Filter BEFORE Join (fewer rows to    │
│     join)                                                           │
│                                                                      │
│  3. PROJECTION PRUNING                                              │
│     Only read the columns that are actually needed                  │
│     e.g., SELECT name FROM users → only read the "name" column,    │
│     skip all other columns                                          │
│                                                                      │
│  4. NULL PROPAGATION                                                │
│     Simplify expressions involving NULL                             │
│     e.g., NULL + x → NULL, NULL AND x → NULL (in some cases)       │
│                                                                      │
│  5. BOOLEAN EXPRESSION SIMPLIFICATION                               │
│     Simplify boolean logic                                          │
│     e.g., x AND true → x, x OR false → x                          │
│                                                                      │
│  Total: ~800 lines of code for all logical optimization rules       │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

**Why Trees + Rules Make This Easy:**

Adding new optimization rules is remarkably simple. For example, when the team added the fixed-precision `DECIMAL` type to Spark SQL, they wanted to optimize aggregations (SUM, AVG) on small-precision DECIMALs. The entire rule was only **12 lines of code**:

```scala
// Simplified version: optimize SUM on small DECIMALs
// Cast to unscaled 64-bit LONG, aggregate, then convert back
object DecimalAggregates extends Rule[LogicalPlan] {
  val MAX_LONG_DIGITS = 18
  def apply(plan: LogicalPlan): LogicalPlan = {
    plan transformAllExpressions {
      case Sum(e @ DecimalType.Expression(prec, scale))
        if prec + 10 <= MAX_LONG_DIGITS =>
        MakeDecimal(Sum(LongValue(e)), prec + 10, scale)
    }
  }
}
```

This converts high-precision decimal math to fast 64-bit long integer math when possible — a significant performance win.

Similarly, a **12-line rule** optimizes `LIKE` expressions with simple regular expressions into `String.startsWith` or `String.contains` calls, which are much faster. The freedom to use arbitrary Scala code in rules makes these optimizations straightforward.

**Predicate Pushdown — Visualized:**

```
┌──────────────────────────────────────────────────────────────────────┐
│                    PREDICATE PUSHDOWN                                 │
│                                                                      │
│  BEFORE Optimization:            AFTER Optimization:                │
│                                                                      │
│     Project(name)                   Project(name)                   │
│         │                               │                           │
│     Filter(age > 21)                Join(id = id)                   │
│         │                           /          \                    │
│     Join(id = id)          Filter(age > 21)   Scan(orders)          │
│     /          \                 │                                   │
│  Scan(users)  Scan(orders)   Scan(users)                            │
│                                                                      │
│  The filter is pushed DOWN past the join to reduce the number       │
│  of rows that enter the join — much less data to process!           │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

---

#### 8.3.3 Phase 3: Physical Planning

**Goal:** Convert the optimized logical plan into one or more physical plans that can be executed on the Spark engine (using RDD operations), and select the best one.

```
┌──────────────────────────────────────────────────────────────────────┐
│                    PHYSICAL PLANNING                                 │
│                                                                      │
│  Optimized Logical Plan                                             │
│         │                                                            │
│         ▼                                                            │
│  ┌─────────────────────────────────────────────┐                    │
│  │  Physical Planner generates MULTIPLE plans  │                    │
│  │                                              │                    │
│  │  Plan A: SortMergeJoin + HashAggregate      │                    │
│  │  Plan B: BroadcastHashJoin + HashAggregate  │                    │
│  │  Plan C: ShuffledHashJoin + SortAggregate   │                    │
│  └─────────────────────────────────────────────┘                    │
│         │                                                            │
│         ▼                                                            │
│  ┌─────────────────────────────────────────────┐                    │
│  │  Cost Model evaluates each plan             │                    │
│  │  (estimates table sizes, selectivity, etc.) │                    │
│  │                                              │                    │
│  │  → Select CHEAPEST plan                     │                    │
│  └─────────────────────────────────────────────┘                    │
│         │                                                            │
│         ▼                                                            │
│  Selected Physical Plan                                             │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

**Physical planning involves two types of optimization:**

1. **Rule-based physical optimizations:**
   - **Pipelining**: Combine projections or filters into a single Spark RDD `map` operation (avoid multiple passes over the data)
   - **Push operations into data sources**: For data sources that support predicate or projection pushdown (e.g., a JDBC source can run a `WHERE` clause directly on the database)

2. **Cost-based optimization (CBO):**
   - Currently used primarily for **selecting join algorithms**
   - For relations known to be **small**, Spark SQL uses a **broadcast join** (sends the small table to all nodes via a peer-to-peer broadcast facility)
   - For large relations, it may choose **sort-merge join** or **shuffled hash join**
   - **How table sizes are estimated:**
     - From the **in-memory cache** (if the table is cached)
     - From **external file sizing** (e.g., Parquet file metadata)
     - From the **result of a subquery** with a `LIMIT`
   - Costs can be estimated **recursively** for a whole tree using a rule

**Total: ~500 lines of code for physical planning rules.**

---

#### 8.3.4 Phase 4: Code Generation

**Goal:** Compile parts of the query into **Java bytecode** for maximum execution speed.

This is where Catalyst really shines for performance. Since Spark SQL often operates on **in-memory datasets**, processing is **CPU-bound** (not I/O bound). Code generation eliminates the overhead of interpretation.

**The Problem with Interpretation:**

```
┌──────────────────────────────────────────────────────────────────────┐
│           INTERPRETED vs. CODE-GENERATED EVALUATION                  │
│                                                                      │
│  INTERPRETED (Slow):                                                │
│  For each row of data:                                              │
│    1. Walk down the AST tree                                        │
│    2. At each node, check type (if/else/switch)                     │
│    3. Dispatch to the correct evaluation function                   │
│    4. Call virtual functions at each node                            │
│    → LOTS of branches and virtual function calls                    │
│    → Costly for billions of rows!                                   │
│                                                                      │
│  CODE-GENERATED (Fast):                                             │
│  The entire expression tree is compiled into a                      │
│  single, tight loop of native bytecode                              │
│    → No tree walking, no virtual dispatch                           │
│    → Runs at near-native speed                                      │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

**How It Works — Quasiquotes:**

Catalyst leverages Scala's **quasiquotes** feature for code generation. Quasiquotes allow the **programmatic construction of Abstract Syntax Trees (ASTs)** in the Scala language, which can then be fed to the Scala compiler at runtime to generate bytecode.

```scala
// Code generation function — converts expression tree to Scala AST
def compile(node: Node): AST = node match {
  case Literal(value)     => q"$value"                           // Constant value
  case Attribute(name)    => q"row.get($name)"                   // Column access
  case Add(left, right)   => q"${compile(left)} + ${compile(right)}"  // Addition
}

// Example:
//   Expression: Add(Literal(1), Attribute("x"))
//   Generated code: 1 + row.get("x")
//
// This is then compiled to JVM bytecode and runs at native speed!
```

**Key Properties of Quasiquotes:**

1. **Type-checked at compile time**: Only appropriate ASTs or literals can be substituted, making them much safer than string concatenation
2. **Highly composable**: The code generation rule for each node does not need to know how the trees returned by its children were built
3. **Further optimized by the Scala compiler**: The generated code gets additional expression-level optimizations that Catalyst might have missed
4. **Practical**: Even new contributors to Spark SQL could quickly add rules for new expression types

**Performance Impact:**

The paper demonstrates that code-generated evaluation achieves performance very close to hand-written code, and is **dramatically faster** than interpreted evaluation:

```
┌──────────────────────────────────────────────────────────────────────┐
│  PERFORMANCE: Evaluating x + x + x (1 billion times)                │
│                                                                      │
│  Interpreted:  ████████████████████████████████████████  ~38 sec    │
│  Hand-written: ████                                      ~4 sec     │
│  Generated:    ████                                      ~4 sec     │
│                                                                      │
│  Code generation is ~10x faster than interpretation!                │
│  And nearly identical to hand-tuned code!                           │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

**Total: ~700 lines of code for the code generator.**

---

### 8.4 DataFrame DSL — How Expressions Are Captured

The DataFrame Domain Specific Language (DSL) is what makes all of this optimization possible. When you write DataFrame operations, you are NOT writing opaque functions — you are building an **Abstract Syntax Tree (AST)** that Catalyst can inspect and optimize.

**Relational Operators:**
- **Projection**: `select()`
- **Filter**: `where()`
- **Join**: `join()`
- **Aggregation**: `groupBy()`

All of these operators take **expression objects**, not arbitrary user functions. This is the key difference from the RDD API.

```python
# DataFrame API — expressions are captured as AST nodes
employees.join(dept, employees("deptId") == dept("id")) \
         .where(employees("gender") == "female") \
         .groupBy(dept("id"), dept("name")) \
         .agg(count("name"))

# employees("deptId") is NOT a Python function call —
# it creates an Expression object that Catalyst can analyze
```

**Schema evaluation is eager** (catches type errors immediately), while **execution is lazy** (waits for an action). This gives users the best of both worlds: immediate feedback on errors, but optimized execution when the computation runs.

---

### 8.5 Extension Points

Catalyst's composable rule-based design makes it inherently extensible. Two key public extension points are:

#### 8.5.1 Data Sources API

Developers can define new data sources by implementing one of several interfaces with varying degrees of optimization:

```
┌──────────────────────────────────────────────────────────────────────┐
│                   DATA SOURCE INTERFACES                             │
│                                                                      │
│  1. TableScan (simplest)                                            │
│     → Return ALL rows as RDD of Row objects                         │
│     → No pushdown, read everything                                  │
│                                                                      │
│  2. PrunedScan                                                      │
│     → Takes array of desired column names                           │
│     → Only returns those columns (column pruning)                   │
│                                                                      │
│  3. PrunedFilteredScan                                              │
│     → Takes desired columns AND filter predicates                   │
│     → Enables both column pruning AND predicate pushdown            │
│     → Filters are "advisory" — source can return false positives    │
│                                                                      │
│  4. CatalystScan (most advanced)                                    │
│     → Given complete Catalyst expression trees                      │
│     → Maximum optimization opportunity                              │
│                                                                      │
│  All data sources also expose NETWORK LOCALITY information          │
│  (which machines each partition is most efficient to read from)     │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

**Built-in data sources implemented using this API:**
- **CSV files**: Scans the whole file, user can specify schema
- **Avro**: Self-describing binary format for nested data
- **Parquet**: Columnar file format, supports column pruning and filter pushdown
- **JDBC**: Scans ranges of a table from an RDBMS in parallel, pushes filters into the RDBMS

**Example — Query Federation with JDBC:**

```sql
-- Register a MySQL table
CREATE TEMPORARY TABLE users
USING jdbc OPTIONS (driver "mysql" url "jdbc:mysql://userDB/users")

-- Register a JSON data source
CREATE TEMPORARY TABLE logs
USING json OPTIONS (path "logs.json")

-- Query across both sources!
SELECT users.id, users.name, logs.message
FROM users JOIN logs WHERE users.id = logs.userId
AND users.registrationDate > "2015-01-01"
```

Under the hood, the JDBC data source uses `PrunedFilteredScan`, which pushes the filter down to MySQL. MySQL only returns matching rows:

```sql
-- What MySQL actually executes (pushed down):
SELECT users.id, users.name FROM users
WHERE users.registrationDate > "2015-01-01"
```

This **dramatically reduces data transfer** from the remote database.

#### 8.5.2 User-Defined Types (UDTs)

Users can register custom types that map to Catalyst's built-in types. For example, to register 2D points:

```scala
class PointUDT extends UserDefinedType[Point] {
  def dataType = StructType(Seq(
    StructField("x", DoubleType),
    StructField("y", DoubleType)
  ))
  def serialize(p: Point) = Row(p.x, p.y)
  def deserialize(r: Row) = Point(r.getDouble(0), r.getDouble(1))
}
```

Once registered, Spark SQL will:
- Recognize `Point` objects in DataFrames
- Store them in **columnar format** when caching (compressing x and y as separate columns)
- Make them **writable to all data sources** (which see them as pairs of `DOUBLE`s)
- Allow **UDFs to operate directly** on the custom type

This capability is used extensively in Spark's **MLlib** machine learning library, where vector types (both sparse and dense) are registered as UDTs.

---

### 8.6 Advanced Features Built on Catalyst

#### 8.6.1 Schema Inference for Semi-Structured Data (JSON)

Spark SQL includes a **schema inference algorithm** for JSON data that works in a **single pass** over the data:

```
┌──────────────────────────────────────────────────────────────────────┐
│                  JSON SCHEMA INFERENCE                                │
│                                                                      │
│  Input JSON records:                                                │
│  {"text": "Tweet about #Spark", "tags": ["#Spark"],                 │
│   "loc": {"lat": 45.1, "long": 90}}                                │
│  {"text": "Another tweet", "tags": [],                              │
│   "loc": {"lat": 39, "long": 88.5}}                                │
│  {"text": "No location", "tags": ["#tweet", "#location"]}          │
│                                                                      │
│  Inferred Schema:                                                   │
│  text   STRING NOT NULL                                             │
│  tags   ARRAY<STRING NOT NULL> NOT NULL                             │
│  loc    STRUCT<lat FLOAT NOT NULL, long FLOAT NOT NULL>             │
│                                                                      │
│  Algorithm:                                                         │
│  • For each field (by path from root), find MOST SPECIFIC type     │
│    that matches all observed instances                              │
│  • INT → LONG → DECIMAL → FLOAT → STRING (generalization chain)   │
│  • Uses a single REDUCE operation over the data                    │
│  • Merges schemas from individual records with an associative      │
│    "most specific supertype" function                               │
│  • Single-pass AND communication-efficient (high local reduction)  │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

#### 8.6.2 Integration with MLlib

MLlib uses DataFrames as the standard data representation in its **pipeline API**:
- A **pipeline** is a graph of transformations: feature extraction → normalization → dimensionality reduction → model training
- Each pipeline stage takes and produces DataFrames
- The **vector UDT** stores both sparse and dense vectors using four primitive fields: a boolean (type), a size, an array of indices, and an array of double values
- Using DataFrames made it much easier to **expose all algorithms in all languages** (Scala, Java, Python, R)

#### 8.6.3 Query Federation

Spark SQL's data sources API enables **query federation** — querying across multiple heterogeneous data sources (JDBC databases, JSON files, Parquet, etc.) in a single program, with predicates pushed down to each source to minimize data transfer.

---

### 8.7 User-Defined Functions (UDFs)

Spark SQL supports **inline UDF registration** — you can register Scala, Java, or Python functions as UDFs and use them in SQL queries. Unlike traditional databases that require separate packaging and registration, Spark SQL UDFs can use the **full Spark API** internally.

```python
# Register a UDF in Python
from pyspark.sql.functions import udf
from pyspark.sql.types import FloatType

@udf(returnType=FloatType())
def predict(age, weight):
    return model.predict([age, weight])

# Use in SQL
spark.udf.register("predict", predict)
spark.sql("SELECT predict(age, weight) FROM users")
```

UDFs can also be accessed via **JDBC/ODBC** by business intelligence tools.

---

## 9. Query Planning: Cost-Based Optimizer

The **Cost-Based Optimizer (CBO)** in Catalyst estimates the cost of different physical plans and selects the cheapest one.

```
┌──────────────────────────────────────────────────────────────────────┐
│               COST-BASED OPTIMIZATION                                │
│                                                                      │
│  1. Represent queries as TREES                                      │
│  2. Apply RULES to manipulate them                                  │
│  3. Generate DIFFERENT PLANS based on manipulation                  │
│  4. ESTIMATE the execution cost of each plan                        │
│  5. SELECT the CHEAPEST plan for actual execution                   │
│                                                                      │
│  Cost estimation sources:                                           │
│  • In-memory cache → exact sizes known                              │
│  • External file sizing → file metadata (Parquet, etc.)             │
│  • Result of a subquery with LIMIT → estimated from subquery       │
│  • Table statistics → row count, column cardinality                │
│                                                                      │
│  Primary use: JOIN ALGORITHM SELECTION                              │
│  • Small table detected → Broadcast Hash Join                      │
│  • Large tables → Sort Merge Join or Shuffled Hash Join            │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

---

## 10. Performance Evaluation (from SIGMOD 2015 Paper)

### 10.1 SQL Performance

Compared against **Shark** and **Impala** on the AMPLab big data benchmark (web analytics workload with scans, aggregation, joins, and UDFs):

- **Spark SQL is substantially faster than Shark** in all queries (due to code generation reducing CPU overhead)
- **Spark SQL is generally competitive with Impala** (which uses C++ and LLVM) — code generation closes the gap
- Setup: 6 EC2 i2.xlarge machines, 110 GB dataset in Parquet format

### 10.2 DataFrame API vs. Native Spark Code

```
┌──────────────────────────────────────────────────────────────────────┐
│        DATAFRAME API vs. NATIVE SPARK CODE                           │
│        (Distributed aggregation: avg of b for each value of a)      │
│        (1 billion integer pairs, 100,000 distinct values of a)      │
│                                                                      │
│  Native Python API:   ████████████████████████████████  ~200 sec    │
│  Native Scala API:    ████████████                      ~100 sec    │
│  DataFrame API:       ██████                            ~50 sec     │
│                                                                      │
│  DataFrame is 12x faster than Python, 2x faster than Scala!        │
│                                                                      │
│  Why?                                                               │
│  • Python: Only logical plan in Python; physical execution          │
│    compiled to native JVM bytecode                                  │
│  • Scala: DataFrame avoids expensive key-value pair allocation      │
│    that occurs in hand-written Scala code (via code generation)     │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

### 10.3 Pipeline Performance

For a two-stage pipeline (relational filter + word count):
- **Integrated DataFrame pipeline** is **2x faster** than separate SQL + Spark jobs
- Avoids materializing intermediate results to HDFS
- SparkSQL pipelines the `map` for word count with the relational operators for filtering

---

## 11. Clash of the Titans: MapReduce vs. Spark (VLDB 2015)

*Reference: Juwei Shi et al., "Clash of the Titans: MapReduce vs. Spark for Large Scale Data Analytics," VLDB 2015.*

### 11.1 Setup

- Hadoop 2.4.0 vs. Spark 1.3.0
- 4 servers @ 32 CPU cores, 2.9 GHz
- 9 disk drives at 7.2k RPM with 1 TB each (aggregate bandwidth: ~125 GB/s reads, ~45 GB/s writes)
- 190 GB RAM per server
- 1 Gbps Ethernet switch
- 32 containers per node for MapReduce on YARN
- 8 Spark workers per node with 4 threads each

### 11.2 Key Findings — Where Spark Wins

**Spark is ~2.5x, 5x, and 5x faster than MapReduce** for Word Count, k-Means, and PageRank respectively:

| Factor | Explanation |
|--------|-------------|
| **Hash-based aggregation** | Spark's hash-based combine (map-side reduction) is ~40% more efficient than MapReduce's sort-based combine |
| **RDD caching** | Reduced CPU and disk overheads for iterative algorithms (PageRank, k-Means) — up to 90% improvement |
| **Data pipelining** | Avoids materialization of intermediate results between stages |
| **Task loading** | Spark's context switch is 10x faster than MapReduce task startup |

### 11.3 Key Findings — Where MapReduce Wins

**MapReduce is 2x faster than Spark for Sort workload:**

| Factor | Explanation |
|--------|-------------|
| **Shuffle efficiency** | MapReduce overlaps Shuffle with Map phase, hiding network overhead |
| **Open files overhead** | Map stage in Spark is slower with more Reducers due to more open files |
| **GC overhead** | Increasing JVM heap size for Spark causes garbage collection overhead |

### 11.4 Common Observations

- For one-pass jobs: **Map is CPU-bound**, **Reduce is network-bound** (disk I/O is NOT the bottleneck — network is), so spills often do not have a significant penalty
- **Input parsing** is often an overhead — RDD caching helps, OS/HDFS caching does not
- **GC overhead** becomes a bottleneck if heap size per task drops to 64 MB with 128 MB split
- **Disk caching** is a bottleneck for RDD if CPU and disk I/O capacities are unbalanced

---

## 12. Summary

```
┌──────────────────────────────────────────────────────────────────────┐
│                        KEY TAKEAWAYS                                 │
│                                                                      │
│  1. DataFrames provide a declarative, SQL-like API that lets        │
│     Spark optimize execution automatically                          │
│                                                                      │
│  2. Catalyst Optimizer uses TREES + RULES for 4-phase query         │
│     planning: Analysis → Logical Optimization → Physical            │
│     Planning → Code Generation                                      │
│                                                                      │
│  3. Code generation brings near-native performance by compiling     │
│     expressions to JVM bytecode (10x faster than interpretation)    │
│                                                                      │
│  4. Catalyst is EXTENSIBLE: new data sources, UDTs, UDFs, and      │
│     optimization rules can be added with minimal code               │
│                                                                      │
│  5. DataFrames are 2-12x faster than hand-written Spark code        │
│     thanks to Catalyst optimizations                                │
│                                                                      │
│  6. Spark excels at iterative workloads (caching, pipelining)       │
│     while MapReduce is better at shuffle-heavy workloads (Sort)     │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

---

*References:*
- *Lecture slides: L2.4 Big Data Processing with Apache Spark, DS256, IISc Bangalore*
- *Armbrust, M., et al. "Spark SQL: Relational Data Processing in Spark." SIGMOD 2015.*
- *Shi, J., et al. "Clash of the Titans: MapReduce vs. Spark for Large Scale Data Analytics." VLDB 2015.*
